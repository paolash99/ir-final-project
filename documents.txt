ID: cs.stackexchange.com:119872
Title: Analysing worst-case time complexity of quick-sort in different cases
Body: I am trying to understand worst case time complexity of quick-sort for various pivots. Here is what I came across:


When array is already sorted in either ascending order or descending order and we select either leftmost or rightmost element as pivot, then it results in worst case $O(n^2)$ time complexity.
When array is not already sorted and we select random element as pivot, then it gives worst case "expected" time complexity as $O(n \log n)$. But worst case time complexity is still $O(n^2)$. [1]
When we select median of [2] first, last and middle element as pivot, then it results in worst case time complexity of $O(n \log n)$ [1]


I have following doubts

D1. Link [2] says, if all elements in array are same then both random pivot and median pivot will lead to $O(n^2)$ time complexity. However link [1] says median pivot yields $O(n \log n)$ worst case time complexity. What is correct?  

D2. How median of first, last and middle element can be median of all elements? 

D3. What we do when random pivot is $i$th element? Do we always have to swap it with either leftmost or rightmost element before partitioning? Or is there any algorithm which does not require such swap? 


[Answer]: The usual rule here is only one question per post.

Link [2] doesn't say that random pivot and median pivot lead to $O(n^2)$ time complexity.

The median of first, middle, and last element is not the median of the whole array.  Writing "median pivot" is probably a bad idea, unless you have carefully specified what that's a median of.

Link [1] says that taking the median of all elements leads to $O(n \log n)$ time complexity.  That's different than taking the median of first, middle, and last.  Also, it's rare for practical implementations to take the median of all elements, as that link already states.


[Answer]: There is no one QuickSort. There is one original, which we only care about for historical reasons, and many variations, all different. 

In the case of n identical items, different implementations will run in O(n log n) or O (n^2). Since this is a practical case, the latter kind of algorithm should be avoided. 

Your point (3) is wrong. Worst case for n different items is always O(n^2). In case (2) it doesn’t matter if the array is already sorted. 

Taking a random element as the median makes sure that an opponent cannot create an array that takes n^2 steps to sort. Median of three random elements will usually partition the array better and therefore will be faster. 

But apart from the case of taking the first or last element with a sorted array, and problems with all or many identical items, bad behaviour won’t happen unless you have an attacker that can create the data to be sorted, or with extraordinarily bad luck. 


------------------------------------------------------------

ID: cs.stackexchange.com:102807
Title: When average , worst and best case time complexity happens in quick sort?
Body: I know recurrence relation corresponding to quick sort worst case is 

$T(n)=T(n-1)+T(0)+\Theta(n)$

and time complexity is $O(n^2)$.

This happens when we select pivot which is either largest element or smallest element in the current sub problem.

This can occur when array is sorted in either descending or ascending order and we select either largest or smallest element as pivot.

I also know that best case occurs when median is chosen as pivot, giving recurrence relation

$T(n)=T(n/2)+T(n/2)+\Theta(n)$.

and time complexity $O(n \log n)$.


  Statement 1: Any other partition should result in average case. 


I have come across problems each time choosing pivot which partition array in some fixed proportion (say $T(n)=T(n/10)+T(9n/10)+\Theta(n)$) ending in average case performance of $O(n \log n)$ (and I understand its proof). 

However I have also came across text saying:


  Statement 2: Any pivot which does not partition array in some proportion should result in worst case. If $n$th smallest or largest element is selected as pivot, it will result in worst case. 


Q1. Is statement 1 true? If no, which other pivot selection strategies could result in worst case or best case behavior?

Q2. Is statement 2 true?


[Answer]: Remember, for big-O we care about the asymptotic limit. If you choose the $k$th largest (or smallest) element, for constant $k$, then when you make $n$ big enough it's as though you chose the single largest or smallest. The $k$ elements on the other side just aren't enough to matter any more. So statement 2 is true.

Statement 1 is also true; you say you understand the proof of it, so I'm not sure how much detail to go into here. But intuitively, the amount on each side of the pivot grows proportionally to $n$. That's what makes it different from the constant $k$.


[Answer]: The time complexity for particular runs of quicksort can be anywhere between the best case and the worse case.  It can be $\Theta(n(\log n)^2)$, $\Theta(n^{\frac32})$, $\Theta(n^{\frac{19}{10}})$ or $\Theta(n^2/\log n)$. Given any $f$ such that $n\log n\le f(n)\le n^2$, we can construct runs of quicksort whose time-complexity is $\Theta(f(n))$.
Let $q$ be a quicksort algorithm. Let $c_q(A)$ be the number of comparisons used by $q$ on input array $A$. Let $m_{q,n}=\min_{\#A=n}c_q(A)$ and $M_{q,n}=\max_{\#A=n}c_q(A)$ for $n>0$. The following property holds for various versions of quicksort.
Continuum of comparisons by quicksort: Given any integer $i$ between $m_{q,n}$ and $M_{q,n}$, there is an array $A$ of size $n$ such that $c_q(A)=i$.
We can replace "comparisons" above by "swaps".


Statement 1: Any other partition should result in average case.
Q1. Is statement 1 true? If no, which other pivot selection strategies could result in worst case or best case behavior?

The "continuum of comparisons by quicksort" shows statement 1 does not make much sense. To determine the asymptotic behavior of a quicksort algorithm, we need to specify the version of quicksort with its partition scheme and which kind of input arrays might be given. It does not make much sense to say $O(n\log n)$ is the time-complexity of the average case without specifying "the average case".
Instead of statement 1, here is the clearer summary, where the average case is when the given input arrays are uniformly random, as described here.

All quicksort algorithms (that I have seen, including these variants)  takes $\Theta(n \log n)$ time in expectation in the average case.

Some quicksort algorithms choose the pivot that partition an array within some fixed proportion. Those algorithms run in $\Theta(n\log n)$-time in all cases, assuming a linear-time algorithm is used to choose the pivot. However, the constant factors hidden in the big $\Theta$-notations for these algorithms are much larger than those factors for other algorithms.




Statement 2: Any pivot which does not partition array in some proportion should result in worst case. If nth smallest or largest element is selected as pivot, it will result in worst case.
Q2. Is statement 2 true?

Although sounds reasonable, statement 2 is too ambiguous to be verified or refuted. The following is a version that could be proved.
Statement 3: The classic quicksort with Lomuto partition scheme or
Hoare partition scheme runs in $\Theta(n^2)$-time if the pivoting on every array partition the array into two parts with one part of size $O(1)$. In particular, for any constant $c$, if $c$-th smallest or $c$-th largest element is selected as pivot whenever the size is no less than $c$, it runs in $\Theta(n^2)$ time.


Exercise 1. Prove statement 3.
Exercise 2. Prove  "continuum of comparisons by quicksort" for a version of quicksort known to you.




[Answer]: One addition: Always picking the median element is not necessarily optimal. Quicksort spends time comparing items and time moving items. Picking the median element minimises the number of comparisons, but the number of moves is smaller if we don't pick exactly the median. Depending on the relative cost of comparing and moving items, picking a pivot a bit away from the median can be faster. 


[Answer]: It is true that when all partitions fulfill a constant proportion (or better), the $n\log n$ behavior occurs. This is because the size of the partitions decreases in geometric progression.
In particular, the median-of-medians strategy ensures such a ratio.
It is also true that when no partition fulfills a constant proportion, the $n^2$ behavior occurs.  This is because the size of the partitions decreases in arithmetic progression.
Statement 1 is wrong. (Unless part of the context is missing or "any other partition" has a special meaning.)
Statement 2 is right.


------------------------------------------------------------

ID: cs.stackexchange.com:80698
Title: What is the difference between Big-O and worst-case run time?
Body: Big-O describes an upper bound on run time. Is that not the definition of "worst-case"?

For example, how can we say that a hash table insertion require O(1) time on average? Constant time is the best case, n time is the worst case. How can an upper bound describe an average run time? In what sense is that bound "upper"?


[Answer]: Wikipedia says:


  Big O notation is a mathematical notation that describes the limiting behavior of a function when the argument tends towards a particular value or infinity.


So it is not the definition of the "worst case".

Wikipedia defines best, worst, and average cases as 


  In computer science, best, worst, and average cases of a given algorithm express what the resource usage is at least, at most and on average, respectively.


By resource we usually refer to time or/and space (memory) required to solve a certain problem.

It also says: 


  Big O notation is a convenient way to express the worst-case scenario for a given algorithm, although it can also be used to express the average-case - for example, the worst-case scenario for quicksort is $O(n^2)$, but the average-case run-time is $O(n\log{n})$.


To sum up: Big-O notation is a pure mathematical concept and notation, which is used in Computer science to express the worst-case scenario for a given algorithm.

Here we have some useful posts:


How does one know which notation of time complexity analysis to use?
How do O and Ω relate to worst and best case?
Sorting functions by asymptotic growth



------------------------------------------------------------

ID: cs.stackexchange.com:29145
Title: Proving Quicksort has a worst case of O(n²)
Body: I am sorting the following list of numbers which is in descending order.
I am using QuickSort to sort and it is known that the worst case running time of QuickSort is $O(n^2)$

import java.io.File;
import java.io.FileNotFoundException;
import java.util.*;



public class QuickSort 
{
    static int pivotversion;
    static int datacomparison=0;
    static int datamovement=0;

    public static void main(String args[])
    {
        Vector container = new Vector();

        String userinput = "data2.txt";
        Scanner myScanner = new Scanner("foo"); // variable used to read file
        Scanner scan = new Scanner(System.in);


        System.out.println("Enter 1 to set pivot to be first element");
        System.out.println("Enter 2 to set pivot to be median of first , middle , last element of the list");
        System.out.println("Your choice : ");
        //pivotversion = scan.nextInt();


        try
        {

            File inputfile = new File("C:\\Users\\8382c\\workspace\\AdvanceAlgorithmA3_Quicksort\\src\\" + userinput);
             myScanner = new Scanner(inputfile);

        }
        catch(FileNotFoundException e)
        {
            System.out.println("File cant be found");
        }


         String line = myScanner.nextLine(); //read 1st line which contains the number of numbers to be sorted

         while(myScanner.hasNext())
         {
             container.add(myScanner.nextInt());
         }


        System.out.println(line);



        quickSort(container,0,container.size()-1);

        for (int i =0;i container, int left, int right)
    {
          int i = left, j = right;
          int tmp;

          int pivot= 0 ;
          pivot = container.get(left);

          boolean maxarraybound = false;




          i++;

          while (i  pivot)
                {
                      j--;
                      datacomparison++;
                }
                if (i  container, int left, int right) 
    {
          int index = partition(container, left, right);
          if (left 

I am trying to prove to myself that the worst-case running time of QuickSort is indeed $O(n^2)$ by summing up the total number of data comparisons and data movements in the algorithm. 

In my current situtation, I have an input of 10000 numbers.

I would expect a total sum of data comparison and data movement to be around 100 million. 

I am only getting a total sum of data comparsion and data movement of around 26 million.


  I am sure I have miss out some "data movement" and "data comparsion"
  in my algorithm, can someone point out to me where as I have no clue?



[Answer]: Asymptotic bounds don't so much tell you how long to expect an algorithm to take, or how many operations it might perform, on a fixed input. Rather, the goal is to characterize how the time taken (or operations performed) by the algorithm grows for inputs of sufficient size. What $O(n^2)$ means is that the worst-case is bounded by some function that increases quadratically; that is, if you give your algorithm a worst-case input of size $n$ and then a worst-case input of size $2n$, you should expect your algorithm to take four times as long on the larger input (since $\frac{(2n)^2}{n^2} = \frac{4n^2}{n^2} = 4$) You might try a worst-case list of around 20,000 numbers and expect to see around 104 million operations.

Why do asymptotic notations work this way? Well, mathematics aside, it's useful for understanding how programs work. Different computers can execute at different clock frequencies, so absolute time measurements wouldn't be helpful for evaluating algorithms cross-platform (unless some reference architecture was used, which is also done, but bear with me). Code compiled for different architectures may perform more or fewer elementary operations, so an exact count of operations isn't really very useful in practice. Asymptotic growth, however, which tells you how an algorithm scales, and what to expect by way of comparison when you change the input size, is a pretty reliable indicator of how programs actually scale on any given system. You could compile your Quicksort for my x86, my friend's AMD64, my dad's ARM, and my grandpa's MIPS and you'd be able to observe the expected scaling behavior. Good luck getting the same number of operations for the same algorithm and input across those platforms!


[Answer]: To answer your question directly, In the worst case, there will be n-1 comparisons in the first recursion, n-2 comparisons in the second recursion, n-3 comparisons in the third, and so and so on, so actually, with n = 10000, I would expect the number of comparisons to be closer to 50,000,000.

As for the actual code, I think you need an additional datacomparison++ after the while ( container.get(i)  loop, as well as an additional datacomparison++ after the while ( container.get(j) > pivot) loop. If you think about how while loops work, you will realize that an additional comparison is performed for the while loop to exit, which you are not counting.

I think with the above two changes, you may get something quite close to 50 million comparisons.


[Answer]: Your question demonstrates some confusion regarding what asymptotic notations such as $O(n^2)$ and expressions such as “worst case” mean. I recommend reading the definitions and some examples. If you have a textbook, work through its exercises. Browse through our questions as well. Understanding comes from experience, so whatever you do, do work out some cases for yourself.
Asymptotic behavior

In my current situtation, I have an input of 10000 numbers.
I would expect a total sum of data comparison and data movement to be around 100 million.
I am only getting a total sum of data comparsion and data movement of around 26 million.

Asymptotic notation such as big oh counts up to a multiplicative factor. When we say that a running time is e.g. $\Theta(n^2)$, it doesn't mean that the running time is $n^2$ — that would lack a unit (is it $n^2$ clock cycles? $n^2$ seconds? …). It means that there is a multiplicative constant $C$ (which you can think of as a unit of measurement) such that the running time is about $C \, n^2$. (Furthermore, this is the case only for large enough $n$, but here the data size is large enough.)
With a single point of measurement, you can't tell anything. Asymptotic notations don't give a value, they give a shape. If the running time of an algorithm is $\Theta(n^2)$, then for large enough $n$, if you plot the running time against the data size, the shape is (approximately) a parabola. This parabola could be at any scale. When you get a number of operations $T(n) \approx 26 \times 10^6$ for an input size $n = 10,000$, if it is indeed the case that $T(n) \in \Theta(n^2)$ and the input size is into the “asymptotic zone”, this tells you that the constant $C$ such that $T(n) \approx C \, n^2$ satisfies $26 \times 10^6 \approx C \times (10^4)^2$. Note that the constant $C$ has a unit: it's a number of elementary operations.
If you want to test experimentally whether the running time is indeed $\Theta(n^2)$, you need to test with multiple input data sizes. Calculate the constant $C$ for each data size; if the hypothesis of $\Theta(n)$ behavior is correct, then you should get approximately the same value for large enough inputs. In other words, plot the running time against the data size, and check whether the shape does look like the expected parabola.
Worst case vs upper bound
The notation $T(n) \in O(n^2)$ means that $T(n)$ is at most $n^2$, up to a scaling factor (multiplicative constant), for large $n$. The big oh gives an upper bound; it's possible that the function is in fact less than this upper bound (i.e. the upper bound is not optimal). For example, $n \in O(n^2)$ (because for all $n \ge 1$, $n \le 1 \times n^2$). The time complexity of quicksort is $O(n^2)$; it's also $O(n^3)$ and $o(42^n)$, but those statements are weaker and thus less useful.
The worst case of quicksort is $O(n^2)$ (and also $O(n^3)$, etc.). It is true that for most choices of pivot, the worst case is $\Theta(n^2)$. What this means, using $T(V)$ to denote the number of operations for the input $A$, is that
    there exists $N$, $C_1$ and $C_2$ such that
    for every $n \ge N$,
    there exists an input $A$ of size $n$
    such that $C_1 n^2 \le T(A) \le C_2 n^2$
            and for all inputs $A'$ of size $n$, $T(A') \le T(A)$.
If you show that quicksort is $O(n^2)$ (i.e. at most $n^2$ for large $n$, up to a scaling factor), that obviously tells you that the worst case is $O(n^2)$, but there may be a better bound. If you want to prove that the worst case is $\Theta(n^2)$, i.e. that quicksort for large enough $n$ is sometimes approximately $C \, n^2$ but never more for some $C$, it's enough to know that quicksort in general is $O(n^2)$ and that there is some input distribution (for every $n$) that's $\Omega(n^2)$, i.e. at least $C_1 n^2$ for large enough $n$.

I am sorting a list of numbers which are in descending order , should't it be a worst case scenario already???

There is no intrinsic relationship between the original order of the elements and the difficulty of sorting them. Sorting numbers that start out in descending order is not intrinsically harder than any other order. With quicksort, what input distribution constitutes the worst case depends on how the pivot is chosen. For example, if you always pick the first element as the pivot, input that is in ascending order and input that is in descending order both reach the $\Theta(n^2)$ worst case. If you always pick the middle element as the pivot, these two input distributions do not reach the worst case — an example worst case would be with the smallest element in the middle (position $\frac{1}{2} n$ rounded to an integer), the next smallest at position $\frac{1}{4} n$, the next at $\frac{3}{4} n$, the next four at $\frac{1}{8} n$, $\frac{3}{8} n$, $\frac{5}{8} n$, $\frac{7}{8} n$, and so on.


[Answer]: here is a writeup for an NYU CS lecture containing info/reasoning on why quicksort is $O(n^2)$ in worst case even with any choice of pivot algorithm, skip to section "Worst case of quicksort". others have pointed out why $O(f(n))$ notation is not really a formula for individual datapoints. as for finding $O(n^2)$ in practice, ie via empirical experiment/ exercise as you seem interested in, try writing code that repeatedly runs Quicksort on worst case inputs (previously sorted ascending or descending) of size $n..m$ and graph the results, and fit it to a function.


------------------------------------------------------------

ID: cs.stackexchange.com:145748
Title: How can you have an upper and lower bound on the worst-case complexity of an algorithm?
Body: Yesterday I started reading "An Introduction To The Analysis of Algorithms" by Sedgewick/Flajolet. For me it was not clear what he meant with "theory of algorithms" and the "scientific approach". I googled a bit and found someone who had the same question:
Sedgewick analysis of algorithm, difference between theory of algorithm and scientific approach
Until then I thought that I had a good understanding of what worst-case complexity of an algorithm is and how it relates to $O()$ but I think that I was a bit wrong.
In the upper thread someone commented:  "(BTW O(), Ω(), Θ() as used in CLRS are about upper, lower and tight bounds all on the worst-case running time, they are not about worst-case, best-case, average-case.)"
And then I was confused. Let split this up in worst-case analysis of a problem and worst-case analysis of a specific algorithm.
Worst-case analysis of a problem
When you have a problem B you want to solve, than it is absolutely understandable that you can have an upper and lower bound on the worst-case complexity since there are numerous algorithms for problem B which all can have different worst-case complexities.
In this case one could say:
The worst-case complexity of problem B lies in O(f(n)) if there is an algorithm for problem B whose worst-case runtine complexity lies in O(f(n)).
The worst-case complexity of a problem B lies in Ω(f(n)) if there can't be (yeah this has to be rigorously proved) an algorithm for the problem B whose worst case complexity lies not! in Ω(f(n)).
A good example is the proof in CLRS that the worst-case complexity of sorting(via comparing)  lies in Θ(n log(n)).
So now to the worst-case complexity of an algorithm.
Worst-case analysis of an algorithm
Lets say we have an algorithm A for some problem and want to find out something about the worst-case complexity of A. For the upper bound I just have to find out for which problem instances the algorithm A has to do the most steps or the most "work".
So here is my problem: What should the lower bound on the worst-case complexity of an algorithm A be?
There is only one way which for me could explain that. If every algorithm yields several different implementations than one could really define a lower bound on the "best" worst-case complexity on all possible implementations of this algorithm A.
If everything what I wrote here is right for the most part than I have to say, that a lot! of books deal extremely sloppy with the relation between the asymptotic analysis and the best-,average and worst-case runtime.


[Answer]: There is no real difference between the definition of upper and lower bounds.
You say

For the upper bound I just have to find out for which problem instances the algorithm A has to do the most steps or the most "work"

Well, that is the same thing with the lower bound, but instead of saying "this particular worst instance of size $n$ takes at most $f(n)$ time to compute", you will say "this particular worst instance of size $n$ takes at least $g(n)$ time to compute".
Obviously, since we are talking about the same worst instance, the computing time will be the same, so it would be more pertinent to give a tight bound instead of a lower or an upper. However, it is not always as easy to find a lower bound than an upper bound (and vice versa).
For example, considering the quicksort algorithm, the worst-case occurs when the array is already sorted. Since a partition is needed, it is easy to see that the worst-case is $\Omega(n)$. Since there is at most $n$ calls to the partition function, we can say that the worst-case is $\mathcal{O}(n^2)$. However, it needs a bit more of analysis to conclude that the worst-case is $\Theta(n^2)$.


[Answer]: Just because an algorithm has a worst case, doesn’t mean you can find it out.
You may find a set of cases that seem quite bad to you, but can all be solved in O(n^2). But you can’t prove that every instance can be solved in O(n^2). So the lower bound for worst case is O(n^2). On the other hand maybe you can prove that every instance can be solved in O(n^3). But you don’t actually have a set of instances that take that long. The worst you have is your set of O(n^2) instances. So you have an upper bound for the worst case, which is O(n^3). The actual worst case is somewhere in between.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:269493
Title: Big-O notation for other cases
Body: I was just reading answers to a question Plain English explanation of Big O 
From that i came to know that Big-O notation is just an "upper bound" of the complexity of an algorithm?

But can we apply it to other cases(i.e best and average case) of an algorithm?


[Answer]: Big-Oh describes the growth rate of a set of functions by comparing it to the growth rate of another function. What those functions mean is totally irrelevant to Big-Oh. It could be a function describing the worst-case time complexity of an algorithm. It could be a function describing the best-case time complexity of an algorithm. It could be a function describing the average case time complexity of an algorithm. It could be a function describing the amortized worst-case time complexity of an algorithm. It could be a function describing the worst-case step complexity of an algorithm. It could be a function describing the worst-case space complexity of an algorithm. It could be a function describing the amount of humans in the world. It could be a function describing the amount of money a movie makes in relation to its production cost.


[Answer]: Yes, you can apply it to other cases.

Big-Oh basically says "no matter how you stack the deck against this algorithm, at worst its performance will scale this way compared to the input."

Omega is similar, but means "no matter how good you make its inputs, at best its performance cannot scale any better than this compared to the input."

For example, quicksort is a popular sorting algorithm. It is actually O(n2) because at worst it has quadratic performance (bad pivot choice). At best it is Ω(n log2 n) which is actually the best that any general-purpose sorting algorithm can possibly achieve.


[Answer]: Sure the three main notations are:

"Big-Oh" meaning your function is always 

"Big-Omega" meaning your function is always >= c*f(x) for some constant c and values greater than some x

"Big-Theta" which is used to describe a tight upper and lower bound, meaning it is both Big-Oh and Big-Omega.  To show Big-Theta you should show both Big-Oh and Big-Omega are the same.

And then on top of all this sometimes you will hear about the "worst case of the best case" or "best case of the worst case" etc.  

And to the person above who talks about quick sort, what about smooth sort?


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:77772
Title: Which sorting algorithms have a different worst case complexity than their average case?
Body: I'v been working on sorting for a while now but I can't figure these two questions apart,  I'm kind of getting mixed up somewhere ... Somebody help

a) Which sorting algorithms have a different worst case complexity than their average case? 

b) Which sorting algorithms have a different best case complexity than their average case? 


[Answer]: The most obvious example would be Quicksort -- its average case is O(N log N), and worst case is O(N2). As its normally written, the best case is O(N log N), but some versions have included a pre-scan to exit early when data was sorted, which gives a best-base of O(N), though I suppose it's open to argument that it's no longer purely a Quicksort at that point.

I don't know whether it does any more, but the implementation of qsort in Microsoft's C standard library used to do that -- mostly to make up for a poor implementation (always used the first element as the pivot) that would otherwise have been O(N2) for sorted data.


[Answer]: Insertion sort is an example of a sort algorithm with a better best case than average case. Best case is O(n), average case and worst case are O(n²)


[Answer]: Here's a complete overview of the complexities.

Some of the most popular ones:


Quick sort: O(n²) in the worst case, O(n lg(n)) on average and in the best case.
Insertion sort: O(n²) in the worst case & average case, O(n) in the best case.



[Answer]: The crucial thing here is recognizing that the algorithm needs to take shortcuts to avoid having to compare each element to every other element (which would result in a O(N^2) algorithm) but only to some of the other elements and to do it again when sorting those elements.  This is the reason why "divide work in piles and handle each of those piles by dividing it in piles and handle each of those piles etc" ends up with O(N log N) complexity.

The hard part is picking those elements, and to handle working with those elements.  Picking is hard because you must choose well to get the subpiles equally sized.  Managing is hard because you cannot spend too much time moving things around if you want a fast algorithm.

For instance, QuickSort works by choosing an element (traditionally called "pivot") and divide the work in two piles.  One having elements smaller than the pivot, and the other having larger.  Then quicksort each pile, and merge the two sorted piles back.  If the pivot is the smallest element each time, you end up with an empty sub-pile and a big subpile with all the elements but the pivot.  This will result in a worst case of O(N^2).  If you choose well, you get half-sized sub-piles and a running time of O(N log N).

So a typical way to choose the pivot is to look at three randomly chosen values from the pile and choose the one in the middle.  This at least avoids the empty sub-pile.


[Answer]: Bubble Sort, O(N) in the best case, O(N ** 2) in the average and worst cases.

To get the O(N) behaviour, set a swapped boolean to false at the beginning of each pass, set it to true whenever you swap two elements. If you have swapped elements, you need to do another pass. This also works as a general termination criterion, as long as comparisons are deterministic, which means you can skip an O(N) counting pass before starting.


[Answer]: There are implementations that first check if the array starts or ends with a sequence of elements in ascending or descending order, and takes advantage if many such elements are found. So best case is linear (if the array is mostly sorted in ascending or descending order). What the average case would be can be highly debatable. 


------------------------------------------------------------

ID: cs.stackexchange.com:154214
Title: How to design a faster sort algorithm? Is there sort of meta-algoritm for it? Or we do not understand how better sort algorithms were discovered?
Body: I know that Quicksort or MergeSort are faster than, say, Bubblesort or Selection sort. And I know why (complexity metrics) but I never been able to find out how could someone start with, for example Bubblesort, and then optimize parts of the code to end up with Quicksort or MergeSort… Is it even possible?
Or it’s the case that going from Bubblesort to Quicksort such a large conceptual leap that there isn’t really a way to “meta-explain” what is the thinking process to go from one to the other?
Another way to put this is: Is there an meta-algorithm to make an algorithm like Bubblesort become an algorithm like Quicksort? Or this still understood so poorly that all we can say is: give the problem to a planet full of human neural networks and wait some years?
I wonder if the way to find out would be to start with the slowest possible algorithm: reorder the list in all possible arrangements, select the one that is ordered and from there look for ways to add constraints to be more efficient… perhaps using something like miniKanren relational programming…


[Answer]: Algorithm design is known to be an art. There is no magical recipe. It all depends on the mathematical properties of the problems addressed.
In the case of sorting, the designers were helped by a few theoretical considerations:

there are $n!$ ways to permute an array, so any sort that relies on comparisons (each time one bit of information) must perform at least $\lg(n!)\sim n\lg(n)$ comparisons. So there is no point searching for faster algorithms.

finding the rank (sorted position) of every element allows you to identify the permutation; this can be done by comparing every element to every other, which takes $\sim n^2$ comparisons.

among the $n^2$ comparisons, many are redundant due to the transitivity property ($a). This gives hope for $o(n^2)$ solutions.


These a priori complexity results serve as guides for algorithm design and tell you how close you are from a good solution.
Now two general techniques can be tried:

incremental processing: assuming you sorted the $m$ first elements, what does it take to sort the $m+1$ first elements ? (insert the new element at the right place $\to$ StraightInsertionSort); or assuming you sorted the $m$ smallest elements, which comes next ? (take the smallest of the remaining elements $\to$ StraightSelectionSort).

divide & conquer: assuming you can sort independently the first halve and second halve of the array, what does it take to obtain the fully sorted array ? (perform a merge operation $\to$ MergeSort); or, assuming that you can presort the array in such a way that the first elements are smaller than the last elements, what does it take to get the fully sorted array ? (the first operation is called a partition; after the partition, it suffices to concatene the sorted subsequences).


It turns out that incremental processing yields simple but $O(n^2)$ algorithms, while D&C results in $O(n\log(n))$ for MergeSort (but requires an extra array), and $\Omega(n\log(n))/O(n^2)$ for Quicksort (so is not optimal, but fast in practice anyway).
None of these algorithms are perfect. There is another, which does not fall in the above categories and relies on an unexpected concept: the heap. This is a special data structure which consists in an implicit binary tree with an order relation between the keys. And it turns out that building a heap takes $O(n)$ time, and extracting the smallest element $\lg(n)$ time at worst. This gives birth to HeapSort, a good algorithm with guaranteed $O(n\lg(n))$ behavior.
There is much more to say about sorting and algorithm design. The morale is that there are general principles, but creativity is still required.


[Answer]: No matter how much one optimizes its code, a bad algorithm will always be bad.
The key to great software is starting with a great algorithm.
What distinguishes an algorithm is something that makes it fundamentally different from other algorithms.
Iteratively improving an algorithm will never result in something that is fundamentally different.
Consider an extreme example.
The best theoretically possible algorithm will require n×log(n) comparisons.
But if the comparison keys are dense enough (or if they can hash to numbers that are dense enough),
they can be sorted with a linear order algorithm (radix sort), which is significantly better than n×log(n).
There is no way anyone can start with a comparison sort and gradually improve it into a radix sort;
the two algorithms are so fundamentally different.


[Answer]: Bubblesort to Quicksort requires some cleverness, but is a quite natural progression.
First, you implement Bubblesort, and you find that for 100,000 random items it is indeed very slow. 100,000^2 operations. You'll find easily that if you split the array into two groups of 50,000, sort both and merge the results, you only need 50,000^2 * 2 + 100,000 or so operations which is almost twice as fast. And then obviously you sort a 50,000 item array by splitting it into two parts of 25,000. And if you follow this down and measure, you'll find that for some rather small n the bubble sort is actually faster. Now we've got merge sort.
Mergesort is actually already optimal in Big-O notation, but making an algorithm twice or three times as fast is still worthwhile. So we would analyse what's bad about merge sort, and it is mostly the fact that we need additional memory. Swapping elements doesn't require more than constant additional memory. Figuring out the Quicksort partition algorithm is difficult, but possible enough.
When you examine Quicksort more closely, two annoying things are the worst case which you fix by randomising the pivot, and the fact that it doesn't take advantage if the data is already sorted or mostly sorted.
Some implementations now check how many initial elements are in either ascending or descending order, and how many elements at the end are in either ascending or descending order. And if the numbers are significantly high compared to the total number of items (say large compared to n / log n) we can sort the unsorted items and then do one or two merges. This will be linear if we combine two sorted arrays, or if we take a sorted array with just one change, or a sorted array with O (n / log n) items appended, and usually be an improvement if we take a sorted array with two or three changes. (And the check is very fast if it doesn't gain anything).
Another implementation assumes that your array was created by starting with a sorted array, and adding / removing / appending / changing a few values, fewer than O (n / log n). Here we can remove all the items that are not in sorted order, sort them separately, then merge. This also works well if you take an array containing a million names, sorted correctly according to one of the official German sorting orders, and sorting it according to the official Swedish sorting order.
The last two changes are both quite natural.


[Answer]: The other answers here explain very well the mathematical idea and thought process to come up with a sorting algorithm, there might be cases where even the "most optimal" algorithm you have, may not deliver the performance you expect. Of course they all sort so they are correct, but in practical scenarios, lower execution times(time and space complexities)are desirable. Often, algorithms are "combined" resulting in a hybrid implementation, which performs better than any algorithm alone. The choice of algorithms to pick depends on the nature of data and empirical results.
An example: Combining merge sort and insertion sort is helpful as for fewer values and completely/nearly sorted data, insertion sort performs better than merge sort. So you can perform merge sort, and once the sub problem size becomes reasonably small, you can use insertion sort.
Notable examples implementing these ideas are Timsort and Introsort.


[Answer]: And here is my meta-algorithm for creating fast algorithms and fast implementations of algorithms:
Step 1: Create a working algorithm.
Step 2: Run the algorithm and figure out what bits run slower than you think they should. At this point you also understand the problem better.
Step 3: With your better understanding of the problem, and your knowledge what makes your current algorithm slower than it should be, create a new algorithm and go back to Step 3.
You stop when you think your benefits from a faster algorithm don't outweigh the cost of creating a faster algorithm anymore. You may continue if you find common situations that you might be able to solve much faster than the general case. It's pointless to optimise for the worst case if the worst case never happens for problems that people actually try to solve.


[Answer]: I'm going to pull my answer from the book Introduction to the Design and Analysis of Algorithms by Anany Levitin. Anything in blockquotes is directly from the book. Here's the steps he gives in the book:

1. Understanding the Problem

Read the problem’s
description carefully and ask questions if you have any doubts about the problem,
do a few small examples by hand, think about special cases, and ask questions again if needed.

2. Choosing between Exact and Approximate Problem Solving

The next principal decision is to choose between solving the problem exactly or
solving it approximately. In the former case, an algorithm is called an exact algorithm; in the latter case, an algorithm is called an approximation algorithm. Why would one opt for an approximation algorithm? First, there are important problems that simply cannot be solved exactly for most of their instances; examples
include extracting square roots, solving nonlinear equations, and evaluating definite integrals. Second, available algorithms for solving a problem exactly can be unacceptably slow because of the problem’s intrinsic complexity. This happens, in
particular, for many problems involving a very large number of choices...

3. Design an algorithm
Describe the algorithm, usually using pseudocode.
4. Proving an Algorithm's Correctness

Once an algorithm has been speciﬁed, you have to prove its correctness. That is,
you have to prove that the algorithm yields a required result for every legitimate
input in a ﬁnite amount of time. For example, the correctness of Euclid’s algorithm
for computing the greatest common divisor stems from the correctness of the
equality gcd(m, n) = gcd(n, m mod n)

5. Analyze the Algorithm
Measure how well the algorithm performs. There are several qualities to keep in mind:

Time efficiency (how fast the algorithm runs)
Space efficiency (how much extra memory it uses)
Simplicity (how simple it is; simple algorithms tend to have less errors)
Generality (how general the algorithm is and the set of inputs it accepts)


If you are not satisﬁed with the algorithm’s efﬁciency, simplicity, or generality,
you must return to the drawing board and redesign the algorithm. In fact, even if
your evaluation is positive, it is still worth searching for other algorithmic solutions.

6. Code the algorithm
Finally code the algorithm. Make sure to test to make sure it's working properly.
"Okay this is fine and dandy, but that still doesn't answer the question about how we got from Bubble Sort to something like Quicksort"
It's a fact that an array grows, the time to sort doesn't grow linearly, but exponentially. What we realized was that partitioning the array into smaller sub-arrays, sorting those sub-arrays, then recombining them resulted in a massive speedup. This is basically what algorithms like Merge Sort and Quicksort is doing.


[Answer]: I agree that there is no recipe and others provided insight on how matters evolved or can evolve. A key aspect hindering the design of your "meta-algorithm" is a lack of "modularity" in Computer Science. The theory of algorithm design (and analysis) is not on par with say civil engineering, for which automation is more advanced. Newton's calculus supports the prediction of properties of a bridge from the blue-prints. Hence such projects can be designed and analysed in a modular fashion (properties of the materials lead to a prediction of properties of the end product).
In CS, design and analysis techniques are tailored to particular algorithms/problems as opposed to being based on a single foundational theory. In many cases there are infinitely many inputs, each of which could change the execution behaviour in a different way. Modularity is not guaranteed (i.e. the design and analysis of parts of the code does not guarantee control/predictability of the design and analysis of the whole).
Computer science faces a hurdle on that front since all parts interact in a multitude of ways (input-dependency) and is not yet at the stage where your question can be fully answered. The field is less than 100 years old (as a science). Other fields, in comparison, have been developed for millennia (though of course computation has been around for a lot longer than the advent of computers).


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:146173
Title: Merge sort versus quick sort performance
Body: I have implemented merge sort and quick sort using C (GCC 4.4.3 on Ubuntu 10.04 running on a 4 GB RAM laptop with an Intel DUO CPU at 2GHz) and I wanted to compare the performance of the two algorithms.

The prototypes of the sorting functions are:

void merge_sort(const char **lines, int start, int end);

void quick_sort(const char **lines, int start, int end);


i.e. both take an array of pointers to strings and sort the elements with index i : start 

I have produced some files containing random strings with length on average 4.5 characters. The test files range from 100 lines to 10000000 lines.

I was a bit surprised by the results because, even though I know that merge sort has complexity O(n log(n)) while quick sort is O(n^2), I have often read that on average quick sort should be as fast as merge sort. However, my results are the following.


Up to 10000 strings, both algorithms perform equally well. For 10000 strings, both require about 0.007 seconds.
For 100000 strings, merge sort is slightly faster with 0.095 s against 0.121 s.
For 1000000 strings merge sort takes 1.287 s against 5.233 s of quick sort.
For 5000000 strings merge sort takes 7.582 s against 118.240 s of quick sort.
For 10000000 strings merge sort takes 16.305 s against 1202.918 s of quick sort.


So my question is: are my results as expected, meaning that quick sort is comparable in speed to merge sort for small inputs but, as the size of the input data grows, the fact that its complexity is quadratic will become evident?

Here is a sketch of what I did.
In the merge sort implementation, the partitioning consists in calling merge sort recursively, i.e.

merge_sort(lines, start, (start + end) / 2);
merge_sort(lines, 1 + (start + end) / 2, end);


Merging of the two sorted sub-array is performed by reading the data from the array lines and writing it to a global temporary array of pointers (this global array is allocate only once). After each merge the pointers are copied back to the original array. So the strings are stored once but I need twice as much memory for the pointers.

For quick sort, the partition function chooses the last element of the array to sort as the pivot and scans the previous elements in one loop. After it has produced a partition of the type

start ... {elements  pivot} ... end


it calls itself recursively:

quick_sort(lines, start,          pivotIndex - 1);
quick_sort(lines, pivotIndex + 1, end);


Note that this quick sort implementation sorts the array in-place and does not require additional memory, therefore it is more memory efficient than the merge sort implementation.

So my question is: is there a better way to implement quick sort that is worthwhile trying out? If I improve the quick sort implementation and
perform more tests on different data sets (computing the average of the running times on different data sets) can I expect a better performance
of quick sort wrt merge sort?

EDIT

Thank you for your answers.

My implementation is in-place and is based on the pseudo-code I have found
on wikipedia in Section In-place version:

function partition(array, 'left', 'right', 'pivotIndex')


where I choose the last element in the range to be sorted as a pivot, i.e. pivotIndex := right.
I have checked the code over and over again and it seems correct to me.
In order to rule out the case that I am using the wrong implementation
I have uploaded the source code on github (in case you would like
to take a look at it).

Your answers seem to suggest that I am using the wrong test data. I will look into it and try out different test data sets. I will report as soon as I have some results. 


[Answer]: Your result is most definitely not expected. In fact, quicksort is used because it tends to be quite a bit faster than mergesort in the average case, i.e. if quicksort doesn't degenerate due to badly chosen pivot elements.

This caveat also hints towards the first thing you should try: choose the pivot element for quicksort randomly, thereby eliminating problems with (partially) presorted data. "tuned" quicksorts will even choose 3 or 5 random elements and take the median for the early runs, since the choice of pivot has a disproportional impact there.

And of course it could be that your implementation of quicksort is simply flawed (it's more difficult to implement really correctly than it sounds).


[Answer]: To try to put this in perspective, let's consider what you can expect from the standard library. To get an idea, I wrote this in C++:

#include 
#include 
#include 
#include 
#include 
#include 

std::string gen_random() {
    size_t len = rand() % 25 + 5;

    std::string x;

    std::generate_n(std::back_inserter(x), len, rand);
    return x;
}

static const int num = 10000000;

int main(){
    std::vector strings;

    std::generate_n(std::back_inserter(strings), num, gen_random);

    clock_t start = clock();
    std::sort(strings.begin(), strings.end());
    clock_t ticks = clock() - start;

    std::cout 

This generates and then sorts the specified number of strings (each between 5 and 30 characters long). On my machine (which is probably somewhat slower than yours) I'm getting a time of ~14 seconds for the sort, which I'd guess is implemented as an Introsort. In the normal case, I'd expect pretty much the same performance from Introsort as Quicksort.

Bottom line: the result you're getting for merge sort is fairly reasonable, but the result you're getting from Quicksort indicates that your implementation has a serious problem.


[Answer]: I will agree with Michael; Quicksort is difficult to implement correctly. Back when I was in college writing my final exam (a comparison of sorting algorithms) my QuickSort implementation managed to blue-screen a Windows NT computer (it didn't cause a GPF; it BSODed).

The biggest problem I generally encounter when quicksorting is pivot selection. Remember that QuickSort's Achilles' heel is pretty common; a near-sorted list. For that reason, proper pivot selection is crucial despite the extra complexity. Even something so simple as picking the middle element of the subarray is generally better than picking at either end; for the two most common cases of a near-sorted list and a truly random list, this will generally produce a good result. Median-of-3 is even better.

Another thing to check is that you have an intelligent "base case". A partition of 2 elements can be sorted trivially (are they in order? if not, swap em), so pivoting a 2-element array in order to reach the base case of zero or one elements is wasteful.

One more thing is to ensure your algorithm is "in-place" (a major advantage of the "intuitive" QuickSort over the "intuitive" MergeSort; allocating memory is expensive), and that you aren't trying to keep the pivot in between the two halves while you're pivoting. Choose your pivot, swap it with the last element, then work in from the left looking for a "big" element, and then from the right looking for a "small" element; swap and continue until a "big" value is found without a "small" value on its right to swap it with (swap it with the pivot at the last element and recurse). This prevents unneeded swapping to keep the pivot in the "middle" of the two halves.


[Answer]: If you look at your code for swapping you:

// If current element is lower than pivot
// then swap it with the element at store_index
// and move the store_index to the right.


But, ~50% of the time that string you just swapped needs to be moved back, which is why faster merge sorts work from both ends at the same time.  

Next if you check to see if the first and last elements are the same before doing each of the recursive call you avoid wasting time calling a function only to quickly exit it.  This happens 10000000 in your final test which does add noticeable amounts of time.

Use,

if (pivot_index -1 > start)
  quick_sort(lines, start, pivot_index - 1);

if (pivot_index + 1 

You still want an outer function to do an initial   if (start 

Also, picking a random pivot tends to avoid N^2 worst case results, but it's probably not a big deal with your random data set.

Finally, the hidden problem is QuickSort is comparing strings in ever smaller buckets that are ever closer together, 

(Edit: So, AAAAA, AAAAB, AAAAC, AAAAD then AAAAA, AAAAB.  So, strcmp needs to step though a lot of A's before looking the useful parts of the strings.)

but with Merge sort you look at the smallest buckets first while they are vary random.  Mergsorts final passes do compare a lot of strings close to each other, but it's less of an issue then.  One way to make Quick sorts faster for strings is to compare the first digits of the outer strings and if there the same ignore them when doing the inner comparisons, but you have to be careful that all strings have enough digits that your not skipping past the null terminator.


[Answer]: 
  are my results as expected?


Merge sort has the following performance characteristics:


Best case: O(n log n)
Average case: O(n log n)
Worst case: O(n log n)


Quicksort has the following performance characteristics:


Best case: O(n log n)
Average case: O(n log n)
Worst case: O(n^2)


Remember: Big-O Notation states the asymptotic bounds ignoring constant factors. 

Quicksort has best-case performance when the pivot elements it chooses tend evenly to partition sub-ranges. It has a worst-case quadratic performance when the the opposite holds, such as when the input is sorted in reverse, or nearly sorted in reverse. There are many varieties of quicksort and they vary, in part, in how they choose pivot elements.

Merge sort performance is much more constrained and predictable than the performance of quicksort. The price for that reliability is that the average case of merge sort is slower than the average case of quicksort because the constant factor of merge sort is larger. However, this constant factor greatly depends on the particular details of the implementation. A good merge sort implementation will have better average performance than a poor quicksort implementation.


[Answer]: Where I think your analysis is also failing is your not considering your data.

What happens to Merge sort and Quick sort when every element in the array is exactly the same? Can you solve this problem or this 'edge' case keep their running times the same? Who's performance is better?

What if the data is completely random? Who's performance is better?


  So my question is: is there a better way to implement quick sort that
  is worthwhile trying out?


Have you considered running Insertion Sort within Quick sort when your arrays are 

How the pivot is chosen is key to Quick Sorts run time. How are you choosing the pivot? Which way is better?

There are actually two ways to implement Quick sort, have you discovered both? 

I think you will be well served to think about these questions.


[Answer]: Note that the pseudo-code in wiki is not practical. It is written to undestand.
If an array is [0,0,0,1] pivot is 1 and other elements are less than the pivot,
then swapping(t 
In this case swapping works nothing.
If an array is [4,2,1,3] then [4,2,1,3] --> [2,4,1,3] --> [2,1,4,3] --> [2,1,3,4].
In this case '4' moves like in bubble sort.

I suggest a new pseudo-code.

quicksort(A, i, k)
    if i = pivot
            array[hole] := array[left]
            hole := left
            while right > hole
                if array[right] 

Performance comparison is difficult, because it depends on some factors.


Implementation
Good code run fast, ugly code run slow, you know.
Data size
a. Size of array element.
b. Whole array size exceed a cache memory or not.
c. Whole array size exceed a main memory or not.
Data structure
a. An array is stored in a continuous memory as C language.
b. An array consist of pointers to a continuous memory.
c. Pointer is gotten by malloc(3) or "new" operator.
Data spread
If random number is used then ESD(estimated standard deviation) of processing time is 5%. Not used then 2%. I researched.
OS, H/W
Linux/Unix, microsoft windows is multi-task OS. Program is often interrupted.
Many core CPU is better. Test before GUI login is better. Single-task OS is better.


Example: N=100000, 32 byte/element, pivot is a middle element, strcmp(3), continuous memory

qsort_middle()  usec = 522870   call = 999999   compare = 28048465  copy = 15404514
merge_sort()    usec = 533722   call = 999999   compare = 18673585  copy = 19673584


Source C programs and scripts are posted in github. Thre is various quicksort and merge sort.


[Answer]: You are not calling the standard library qsort (or the standard C++ sort) function. What you are calling are two random functions written by some random person who is more or less clever, and that will be reflected in the performance. You are not comparing the performance of two algorithms, you are comparing the performance of two random implementations. 

Usually the best thing to do is to use what most people are using, in this case calling qsort. Which you can assume will sort things as fast as it can. Since sorting is so essential, the qsort implementation is unlikely to be plain quicksort, and even less likely to be an implementation of plain quicksort with obvious performance problems (like the quick_sort function that you used).

If you are lucky, qsort will for example sort sorted arrays in linear time, and almost sorted arrays in close to linear time. 


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:296738
Title: Difference between O(N^2) and Θ(N^2)
Body: What is the difference between O(N^2) and Θ(N^2)? I know that O is the upper bound and Θ is tight bound. Could someone explain me this concept with an example.


[Answer]: Compare Selection Sort and QuickSort.

Selection sort has a n^2 behavious in both the best and worst case. It will always perform bad. 

Quicksort on the other hand, might perform with n^2 complexity if the odds are against you, but it might (and in most cases will) perform with n*log(n) complexity.

Note how both algorithms are O(n^2) in the worst case, however only Selection Sort is Θ(n^2). This is essentially saying the above - we can bound Selection Sort running time from below by the same function, whereas Quicksort may perform asymptotically better in the best case vs worst case.


[Answer]: This notation has nothing to do with the difference between best/average/worst case complexity. You can apply both notations to each of them.

An algorithm which has linear runtime is in both O(n) and O(n^2), since O only denotes an upper bound. Any algorithm that's asymptotically faster than n^2 is also in O(n^2).  Mathematically O(n^2) is a set of functions which grows at most as fast as c * n^2. For example this set contains c, c * x, c * x^1.5, c x^2, c1 * x^2 + c2 * x for any c.

Θ on the other hand is both a lower and an upper bound. So a linear runtime algorithm is in Θ(n) but not in Θ(n^2). Mathematically Θ(n^2) is a set of functions which don't grow faster that c*n^2 for some c but also doesn't grow slower than c*n^2 for another smaller c. For example this set contains c * x^2, c1 * x^2 + c2 * x, arctan(x) * x^2 for any positive c and other functions where the fastest growing term grows like c * x^2 but not c, c x, c x^1.5 because all the terms grow slower than c * x^2.

See Big O notation on wikipedia


------------------------------------------------------------

ID: cs.stackexchange.com:138335
Title: What is the space complexity of quicksort?
Body: What is the space complexity of quicksort?
I was doing some research and found some saying it is $O(1)$, some saying it's $O(\log n)$, and some saying $O(n)$. Not sure what to believe, even though $O(\log n)$ seems to make the most sense for me. Does it all depend on the pivot point that is chosen?


[Answer]: Here is quicksort in a nutshell:

Choose a pivot somehow.
Partition the array into two parts (smaller than the pivot, larger than the pivot).
Recursively sort the first part, then recursively sort the second part.

Each recursive call uses $O(1)$ words in local variables, hence the total space complexity is proportional to the height of the recursion tree.
The height of the recursion tree is always at least $\Omega(\log n)$, hence this is a lower bound on the space complexity. If you choose the pivot at random or using a good heuristic, then the recursion tree will have height $O(\log n)$, and so the space complexity is $\Theta(\log n)$. If the pivot can be chosen adversarially, you can cause the recursion tree to have height $\Theta(n)$, causing the worst-case space complexity to be $\Theta(n)$.


[Answer]: Since worst case space complexity of $\Theta(n)$ could be a problem, you can make a slight modification to the Qicksort algorithm: Partition the array, then sort the smaller half recursively, and sort the larger half iteratively. Roughly:
Sort (range r)
    While r contains two or more elements
        Partition range r
        Sort (smaller sub partition)
        r = larger sub partition 

This reduces the worst case space required to $\Theta(\log n))$. It does not help with the worst case execution time.


------------------------------------------------------------

ID: cs.stackexchange.com:35994
Title: Why does Randomized Quicksort have O(n log n) worst-case runtime cost
Body: Randomized Quick Sort is an extension of Quick Sort in which the pivot element is chosen randomly. What can be the worst case time complexity of this algorithm. According to me, it should be $O(n^2)$, as the worst case happens when randomly chosen pivot is selected in sorted or reverse sorted order. But in some texts [1] [2] its worst case time complexity is written as $O(n\log{n})$

What's correct?


[Answer]: Both of your sources refer to the "worst-case expected running time" of $O(n \log n).$ I'm guessing this refers to the expected time requirement, which differs from the absolute worst case.

Quicksort usually has an absolute worst-case time requirement of $O(n^2)$. The worst case occurs when, at every step, the partition procedure splits an $n$-length array into arrays of size $1$ and $n-1$. This "unlucky" selection of pivot elements requires $O(n)$ recursive calls, leading to a $O(n^2)$ worst-case.

Choosing the pivot randomly or randomly shuffling the array prior to sorting has the effect of rendering the worst-case very unlikely, particularly for large arrays. See Wikipedia for a proof that the expected time requirement is $O(n\log n)$. According to another source, "the probability that quicksort will use a quadratic number of compares when sorting a large array on your computer is much less than the probability that your computer will be struck by lightning."

Edit:

Per Bangye's comment, you can eliminate the worst-case pivot selection sequence by always selecting the median element as the pivot. Since finding the median takes $O(n)$ time, this gives $\Theta(n \log n)$ worst-case performance. However, since randomized quicksort is very unlikely to stumble upon the worst case, the deterministic median-finding variant of quicksort is rarely used. 


[Answer]: Note that there are two things to take expectation/average over: the input permutation and the pivots (one per partitioning). 

For some inputs and implementations of Quicksort all pivots are bad ($n$ times the same number sometimes works) so randomisation does not help. In such a case the expected time (averaging over pivot choices) can be quadratic in the worst case (a bad input). Still, the "overall" expected time (averaging over both inputs and pivot choices) is still $\Theta(n \log n)$ for reasonable implementations.

Other implementations have true worst-case runtime in $\Theta(n \log n)$, namely those that pick the exact median as pivot and deal with duplicates in a nice way.

Bottom line, check your source(s) for which implementation they use and which quantity they consider random resp. fixed in their analysis.


[Answer]: Yes you are corect, it will be $O(n^2)$.

The worst case for randomized quicksort is same elements as input. Ex: 2,2,2,2,2,2

Here the algorithm whatever it picks will be $T(n) = T(n-1) + n$ and hence $O(n^2)$.


[Answer]: You were missing that these texts talk about "worst case expected run time", not "worst case runtime". 

They are discussing a Quicksort implementation that involves a random element. Normally you have a deterministic algorithm, that is an algorithm which for a given input will always produce the exact same steps. To determine the "worst case runtime", you examine all possible inputs, and pick the one that produces the worst runtime. 

But here we have a random factor. Given some input, the algorithm will not always do the same steps because some randomness is involved. Instead of having a runtime for each fixed input, we have an "expected runtime" - we check each possible value of the random decisions and their probability, and the "expected runtime" is the weighted average of the runtime for each combination of random decisions, but still for a fixed input. 

So we calculate the "expected runtime" for each possible input, and to get the "worst case expected runtime", we find the one possible input where the expected runtime is worst. And apparently they showed that the worst case for the "expected runtime" is just O (n log n). I wouldn't be surprised if just picking the first pivot at random would change the worst case expected runtime to o (n^2) (little o instead of Big O), because only a few out of n pivots will lead to worst case behaviour. 


------------------------------------------------------------

ID: cs.stackexchange.com:132609
Title: Time complexity of a machine which combines Insertion Sort and Quicksort
Body: Given a machine that sorts an array of length $n$ with the following algorithm:

Sort first $2\sqrt{n} + 1$ elements of array with Insertion Sort.(Check Insertion Sort)
Select the median of the whole array (after its first $2\sqrt{n} + 1$ length part is sorted from step 1) as the pivot then partition the array.(Check Quicksort)
Do both above steps for both partitions resulted from step 2 in a recursive way.

Suggest a recursive function that computes the time complexity of the above machine in its worst case.
$\rule{17.5cm}{0.4pt}$
As I see it's somehow a mixture of Quicksort and Insertion sort and every time the machine wants to quick sort a partition of length n we are sure that first $2\sqrt{n} + 1$ elements are sorted.
we know:

Quicksorting a sorted array of length n is from $O(n\log n$) and Insertion sorting it is from $O(n)$.
worst case of both Quicksort and Insertion sort is $O(n^2)$.



[Answer]: The worst case for Quicksort happens when each partitioning divides an array of size n into sub arrays of size 1 and n-1. In your case, is that possible? Why isn’t it possible? What is the worst outcome of partitioning if the pivot is the median of 2k numbers? So what is the worst case in your algorithm?


------------------------------------------------------------

ID: cs.stackexchange.com:66573
Title: Why quicksort instead of a B-tree construction?
Body: As far as I know (despite some variations which provide empirical average-case improvements) quick-sort is worst case $O(n^2)$ with the original Hoare partition scheme having a particularly bad behavior for already sorted lists, reverse-sorted, repeated element lists. 

On the other hand a B-Tree has $O(\log n)$ insertions, meaning worst-case $O(n\log n)$ to process an array of $n$ elements. Also an easy optimization to memoize the memory-address of the lowest and highest nodes (would make it possible to process sorted / reverse-sorted / repeated-element lists in $O(n)$). 

While there are more favored sorting algorithms than quicksort now (e.g. timsort) what originally favored its use? Is it the susceptibility to parallelization (also in place swaps, lower memory complexity)? Otherwise why not just use a B-tree? 


[Answer]: Consider the following general question:


  Here is an $O(n\log n)$ sorting algorithm. Why use quicksort (or timsort, or whatever is used in some library) rather than my algorithm?


There are several possible types of answers:


Your algorithm is worse than quicksort in some sense, say it is slower on average.
Your algorithm has similar performance to quicksort. Libraries need only one algorithm, so they chose quicksort for historical reasons.
Your algorithm is better than quicksort in some sense (say worst case complexity), but worse in some other sense (say average case complexity). Libraries prefer the tradeoffs of quicksort.
Your algorithm is better than quicksort, but is not widely known and so not implemented in libraries.
Your algorithm is implemented in some libraries, but they are too obscure for you to realize.


But the real answer is:


  Library designers implement some sorting algorithm they know. Later on, other people might implement a different algorithm. Nobody is claiming that whatever algorithm is implemented is the best algorithm in all (or even some) applications. People for whom this is important implement their own algorithms.



[Answer]: A B-tree has one significant disadvantage on the fastest deep cache machines, it depends on pointers. So as the size grows each access have a greater and greater risk of causing a cache-miss/TLB-miss. 
Effectively getting a K value of z*x, x=sum(cache/TLB-miss per access, L1-TLB misses are typically size of tree / total cache size), z ~= access time of least cache or main memory that can hold the entire tree.

On the other hand the "average case" quicksort streams memory at maximum pre-fetcher speed. Only drawback here is the average case also cause a stream to be written back. And after some partitions the entire active set sit in caches and will get streamed quicker.

Both algorithms suffers heavily from branch mis-predictions but quicksort, just need to backup a bit, B-Tree additionally needs to read in a new address to fetch from as it has a data dependency which quicksort doesn't.

Few algoritmes are implemented as pure theoretically functions. Nearly all have some heuristics to fix their worst problems, Tim-sort excepted as its build of heuristics.

merge-sort and quick-sort are often checked for already sorted ranges, just like Tim-sort. Both also have an insertion sort for small sets, typically less than 16 elements, Tim-sort is build up of these smaller sets.
The C++ std::sort is a quicksort hybrid with insertion sort, with the additional fallback for the worst case behaviour, if the dividing exceed twice the expected depth it changes to a heap-sort algorithm.

The original quicksort used the first element of the array as pivot, this was quickly abandoned for a (pseudo)random element, typically the middle. Some implementations changed to median-of-three (random elements) to get a better pivot, recently a median-of-5-median (of all elements) was used, and last I saw in some presentation from Alexandrescu was a median-of-3-medians (of all elements) to get the a pivot that was close to the actual median (1/3 or a 1/5 of the span).


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:150615
Title: Why is quicksort better than other sorting algorithms in practice?
Body: This is a repost of a question on cs.SE by Janoma. Full credits and spoils to him or cs.SE.

In a standard algorithms course we are taught that quicksort is O(n log n) on average and O(n²) in the worst case. At the same time, other sorting algorithms are studied which are O(n log n) in the worst case (like mergesort and heapsort), and even linear time in the best case (like bubblesort) but with some additional needs of memory.

After a quick glance at some more running times it is natural to say that quicksort should not be as efficient as others.

Also, consider that students learn in basic programming courses that recursion is not really good in general because it could use too much memory, etc. Therefore (and even though this is not a real argument), this gives the idea that quicksort might not be really good because it is a recursive algorithm.

Why, then, does quicksort outperform other sorting algorithms in practice? Does it have to do with the structure of real-world data? Does it have to do with the way memory works in computers? I know that some memories are way faster than others, but I don't know if that's the real reason for this counter-intuitive performance (when compared to theoretical estimates).


[Answer]: Quicksort is often a good choice as it is reasonably fast and reasonably quick and easy to implement.

If you are serious about sorting large amounts of data very quickly then you are probably better of with some variation on MergeSort. This can be made to take advantage of external storage, can make use of multiple threads or even processes but they are not trivial to code.   


[Answer]: Quick sort is considered to be quicker because the coefficient is smaller that any other known algorithm. There is no reason or proof for that, just no algorithm with a smaller coefficient has been found. Its true that other algorithms also have O(n log n) time, but in the real world the coefficient is important also.

Note that for small data insertion sort (the one that is considered O(n2) ) is quicker because of the nature of the mathematical functions. This depends on the specific coefficients that vary from machine to machine. (At the end, only assembly is really running.) 
So sometimes a hybrid of quick sort and insertion sort is the quickest in practice I think.


[Answer]: You shouldn't center only on worst case and only on time complexity. It's more about average than worst, and it's about time and space.

Quicksort:


has an average time complexity of Θ(n log n);
can be implemented with space complexity of Θ(log n);


Also have in account that big O notation doesn't take in account any constants, but in practice it does make difference if the algorithm is few times faster. Θ(n log n) means, that algorithm executes in K n log(n), where K is constant. Quicksort is the comparison-sort algorithm with the lowest K.


[Answer]: Quicksort does not outperform all other sorting algorithms. For example, bottom-up heap sort (Wegener 2002) outperforms quicksort for reasonable amounts of data and is also an in-place algorithm. It is also easy to implement (at least, not harder than some optimized quicksort variant). 

It is just not so well-known and you don't find it in many textbooks, that may explain why it is not as popular as quicksort.


[Answer]: The actual performance of algorithms depends on the platform, as well as the language, the compiler, programmer attention to implementation detail, specific optimization effort, et cetera.  So, the "constant factor advantage" of quicksort isn't very well-defined -- it's a subjective judgement based on currently-available tools, and a rough estimation of "equivalent implementation effort" by whoever actually does the comparative performance study...

That said, I believe quicksort performs well (for randomized input) because it is simple, and because its recursive structure is relatively cache-friendly.  On the other hand, because its worst case is easy to trigger, any practical use of a quicksort will need to be more complex than its textbook description would indicate: thus, modified versions such as introsort.

Over time, as the dominant platform changes, different algorithms may gain or lose their (ill-defined) relative advantage.  Conventional wisdom on relative performance may well lag behind this shift, so if you're really unsure which algorithm is best for your application, you should implement both, and test them.


[Answer]: I wouldn't agree that quicksort is better than other sorting algorithms in practice.

For most purposes, Timsort - the hybrid between mergesort/insertion sort which exploits the fact that the data you sort often starts out nearly sorted or reverse sorted.

The simplest quicksort (no random pivot) treats this potentially common case as O(N^2) (reducing to O(N lg N) with random pivots), while TimSort can handle these cases in O(N).  

According to these benchmarks in C# comparing the built-in quicksort to TimSort, Timsort is significantly faster in the mostly sorted cases, and slightly faster in the random data case and TimSort gets better if the comparison function is particularly slow.  I haven't repeated these benchmarks and would not be surprised if quicksort slightly beat TimSort for some combination of random data or if there is something quirky in C#'s builtin sort (based on quicksort) that is slowing it down.  However, TimSort has distinct advantages when data may be partially sorted, and is roughly equal to quicksort in terms of speed when the data is not partially sorted.

TimSort also has an added bonus of being a stable sort, unlike quicksort.  The only disadvantage of TimSort uses O(N) versus O(lg N) memory in the usual (fast) implementation.


[Answer]: Quicksort is fast, but only if you implement it very carefully.
Make sure that an array that is already sorted in correct or reverse order will be sorted quickly. This is a very common case, when an array is sorted twice. There are implementations that check first whether your array starts or ends in a subarray that is sorted in ascending or descending order. If the elements are random, then this will take very little time. If an array is sorted or the concatenation of two sorted arrays, then you can sort it in linear time. If you have n sorted items followed by fewer than O (n / log n) random items, you can sort it in linear time.
Really bad implementations will always choose the first or last element as the pivot, which turns Quicksort into Big-Theta(n^2) if the array is sorted; picking a random element as the pivot will help enormously.
Make sure that the implementation is fast if all or many elements are equal. With k equal elements, bad implementations will take k^2 operations for those elements, so if more than O (sqrt (n log n)) elements are equal, you won't be sorting in O (n log n) anymore.
Fine tuning: Consider the cost of a comparison and of moving elements. You can't avoid n log n comparisons, but you can sort with O (n) moves. If moving is substantially slower than comparing, you can sort an array of array indexes, get the exact order of indices for the correctly sorted array, and permute the elements. This may not be cache friendly, and uses extra storage for indices, so you may only want to do it if a subarray that you are partitioning fits into a cache.
Easier fine tuning: If moving elements is a lot more expensive than comparing, but not excessively: You reduce the number of moves by not picking the median as the pivot, but a bit of the median. So what you can do is pick 4 random elements and use the 2nd smallest or 2nd largest as the pivot. The number of comparisons grows, but the number of moves goes down.
Partial sorting: Sorting may be needed to display items in sorted order. But if you have tons of items, you can't display them all. You can put a million items into buckets according to the first letter, and if you want to display say items 330,000 to 330,025 you check which bucket they are in, and sort the one bucket containing the items you want. This can make things ten times faster.
Two cases where Quicksort is not the fastest by far: 1. Your array was sorted, but its values change slowly over time. Say weather stations sorted by temperature. One minute later the temperatures have changed slightly. Bubblesort might be fastest. 2. Your array was sorted, but a small number of random items has been changed, sometimes massively changed. If you have n items of which k are changed, you can sort them in O (n + k log k) which is linear if k = O (n / log n).


------------------------------------------------------------

ID: cs.stackexchange.com:3
Title: Why is quicksort better than other sorting algorithms in practice?
Body: In a standard algorithms course we are taught that quicksort is $O(n \log n)$ on average and $O(n^2)$ in the worst case. At the same time, other sorting algorithms are studied which are $O(n \log n)$ in the worst case (like mergesort and heapsort), and even linear time in the best case (like bubblesort) but with some additional needs of memory.
After a quick glance at some more running times it is natural to say that quicksort should not be as efficient as others.
Also, consider that students learn in basic programming courses that recursion is not really good in general because it could use too much memory, etc. Therefore (and even though this is not a real argument), this gives the idea that quicksort might not be really good because it is a recursive algorithm.
Why, then, does quicksort outperform other sorting algorithms in practice? Does it have to do with the structure of real-world data? Does it have to do with the way memory works in computers? I know that some memories are way faster than others, but I don't know if that's the real reason for this counter-intuitive performance (when compared to theoretical estimates).

Update 1: a canonical answer is saying that the constants involved in the $O(n\log n)$ of the average case are smaller than the constants involved in other $O(n\log n)$ algorithms. However, I have yet to see a proper justification of this, with precise calculations instead of intuitive ideas only.
In any case, it seems like the real difference occurs, as some answers suggest, at memory level, where implementations take advantage of the internal structure of computers, using, for example, that cache memory is faster than RAM. The discussion is already interesting, but I'd still like to see more detail with respect to memory-management, since it appears that the answer has to do with it.

Update 2: There are several web pages offering a comparison of sorting algorithms, some fancier than others (most notably sorting-algorithms.com). Other than presenting a nice visual aid, this approach does not answer my question.


[Answer]: In comparison to other comparison-based sorting algorithms with $O(n \lg n)$ time complexity, quick-sort is often considered to better than other algorithms like merge-sort because it is an in-place sorting algorithm. In other words, we don't need (much more) memory to store the members of the array.

ps: to be precise, being better than other algorithms is task dependent. For some tasks it might be better to use other sorting algorithms.

See also:


Comparison of quick-sort with other sorting algorithms
Comparison of heap-sort with other sorting algorithms



[Answer]: I think one of the main reasons why QuickSort is so fast compared with other sorting algorithms is because it's cache-friendly. When QS processes a segment of an array, it accesses elements at the beginning and end of the segment, and moves towards the center of the segment.

So, when you start, you access the first element in the array and a piece of memory (“location”) is loaded into the cache. And when you try to access the second element, it's (most likely) already in the cache, so it's very fast.

Other algorithms like heapsort don't work like this, they jump in the array a lot, which makes them slower.


[Answer]: Even though quick-sort has a worst case run time of $\Theta(n^2)$, quicksort is considered the best sorting because it is VERY efficient on the average: its expected running time is $\Theta(n\log n)$ where the constants are VERY SMALL compared to other sorting algorithms. This is the main reason for using quick sort over other sorting algorithms.

The second reason is that it performs in-place sorting and works very well with virtual-memory environments.

UPDATE: : (After Janoma's and Svick's comments)

To illustrate this better let me give an example using Merge Sort (because Merge sort is the next widely adopted sort algorithm after quick sort, I think) and tell you where the extra constants come from (to the best of my knowledge and why I think Quick sort is better):

Consider the following seqence:

12,30,21,8,6,9,1,7. The merge sort algorithm works as follows:

(a) 12,30,21,8    6,9,1,7  //divide stage
(b) 12,30   21,8   6,9   1,7   //divide stage
(c) 12   30   21   8   6   9   1   7   //Final divide stage
(d) 12,30   8,21   6,9   1,7   //Merge Stage
(e) 8,12,21,30   .....     // Analyze this stage


If you care fully look how the last stage is happening, first 12 is compared with 8 and 8 is smaller so it goes first. Now 12 is AGAIN compared with 21 and 12 goes next and so on and so forth. If you take the final merge i.e. 4 elements with 4 other elements, it incurs a lot of EXTRA comparisons as constants which is NOT incurred in Quick Sort. This is the reason why quick sort is preferred.


[Answer]: Others have already said that the asymptotic average runtime of Quicksort is better (in the constant) than that of other sorting algorithms (in certain settings).

What does that mean? Assume any permutation is chosen at random (assuming uniform distribution). In this case, typical pivot selection methods provide pivots that in expectation divide the list/array roughly in half; that is what brings us down to $\cal{O}(n \log n)$. But, additionally, merging partial solutions obtained by recursing takes only constant time (as opposed to linear time in case of Mergesort). Of course, separating the input in two lists according to the pivot is in linear time, but it often requires few actual swaps.

Note that there are many variants of Quicksort (see e.g. Sedgewick's dissertation). They perform differently on different input distributions (uniform, almost sorted, almost inversely sorted, many duplicates, ...), and other algorithms might be better for some.

Another fact worth noting is that Quicksort is slow on short inputs compared to simper algorithms with less overhead. Therefore, good libraries do not recurse down to lists of length one but will use (for example) insertion sort if input length is smaller than some $k \approx 10$.


[Answer]: There are multiple points that can be made regarding this question.

Quicksort is usually fast

Although Quicksort has worst-case $O(n^2)$ behaviour, it is usually fast: assuming random pivot selection, there's a very large chance we pick some number that separates the input into two similarly sized subsets, which is exactly what we want to have.

In particular, even if we pick a pivot that creates a 10%-90% split every 10 splits (which is a meh split), and a 1 element - $n-1$ element split otherwise (which is the worst split you can get), our running time is still $O(n \log n)$ (note that this would blow up the constants to a point that Merge sort is probably faster though).

Quicksort is usually faster than most sorts

Quicksort is usually faster than sorts that are slower than $O(n \log n)$ (say, Insertion sort with its $O(n^2)$ running time), simply because for large $n$ their running times explode.

A good reason why Quicksort is so fast in practice compared to most other $O(n \log n)$ algorithms such as Heapsort, is because it is relatively cache-efficient. Its running time is actually $O(\frac{n}{B} \log (\frac{n}{B}))$, where $B$ is the block size. Heapsort, on the other hand, doesn't have any such speedup: it's not at all accessing memory cache-efficiently.

The reason for this cache efficiency is that it linearly scans the input and linearly partitions the input. This means we can make the most of every cache load we do as we read every number we load into the cache before swapping that cache for another. In particular, the algorithm is cache-oblivious, which gives good cache performance for every cache level, which is another win.

Cache efficiency could be further improved to $O(\frac{n}{B} \log_{\frac{M}{B}} (\frac{n}{B}))$, where $M$ is the size of our main memory, if we use $k$-way Quicksort. Note that Mergesort also has the same cache-efficiency as Quicksort, and its k-way version in fact has better performance (through lower constant factors) if memory is a severe constrain. This gives rise to the next point: we'll need to compare Quicksort to Mergesort on other factors.

Quicksort is usually faster than Mergesort

This comparison is completely about constant factors (if we consider the typical case). In particular, the choice is between a suboptimal choice of the pivot for Quicksort versus the copy of the entire input for Mergesort (or the complexity of the algorithm needed to avoid this copying). It turns out that the former is more efficient: there's no theory behind this, it just happens to be faster.

Note that Quicksort will make more recursive calls, but allocating stack space is cheap (almost free in fact, as long as you don't blow the stack) and you re-use it. Allocating a giant block on the heap (or your hard drive, if $n$ is really large) is quite a bit more expensive, but both are $O(\log n)$ overheads that pale in comparison to the $O(n)$ work mentioned above.

Lastly, note that Quicksort is slightly sensitive to input that happens to be in the right order, in which case it can skip some swaps. Mergesort doesn't have any such optimizations, which also makes Quicksort a bit faster compared to Mergesort.

Use the sort that suits your needs

In conclusion: no sorting algorithm is always optimal. Choose whichever one suits your needs. If you need an algorithm that is the quickest for most cases, and you don't mind it might end up being a bit slow in rare cases, and you don't need a stable sort, use Quicksort. Otherwise, use the algorithm that suits your needs better.


[Answer]: Short Answer

The cache efficiency argument has already been explained in detail. In addition, there is an intrinsic argument, why Quicksort is fast. If implemented like with two “crossing pointers”, e.g. here, the inner loops have a very small body. As this is the code executed most often, this pays off.

Long Answer

First of all, 

The Average Case does not exist!

As best and worst case often are extremes rarely occurring in practice, average case analysis is done. But any average case analysis assume some distribution of inputs! For sorting, the typical choice is the random permutation model (tacitly assumed on Wikipedia).

Why $O$-Notation?

Discarding constants in analysis of algorithms is done for one main reason: If I am interested in exact running times, I need (relative) costs of all involved basic operations (even still ignoring caching issues, pipelining in modern processors ...). 
Mathematical analysis can count how often each instruction is executed, but running times of single instructions depend on processor details, e.g. whether a 32-bit integer multiplication takes as much time as addition.

There are two ways out:


Fix some machine model. 

This is done in Don Knuth's book series “The Art of Computer Programming” for an artificial “typical” computer invented by the author. In volume 3 you find exact average case results for many sorting algorithms, e.g. 


Quicksort: $ 11.667(n+1)\ln(n)-1.74n-18.74 $
Mergesort: $ 12.5 n \ln(n) $
Heapsort:  $ 16 n \ln(n) +0.01n $
Insertionsort: $2.25n^2+7.75n-3ln(n)$

[source]  


These results indicate that Quicksort is fastest. But, it is only proved on Knuth's artificial machine, it does not necessarily imply anything for say your x86 PC. Note also that the algorithms relate differently for small inputs:

[source]
Analyse abstract basic operations.

For comparison based sorting, this typically is swaps and key comparisons. In Robert Sedgewick's books, e.g. “Algorithms”, this approach is pursued. You find there


Quicksort: $2n\ln(n)$ comparisons and $\frac13n\ln(n)$ swaps on average
Mergesort: $1.44n\ln(n)$ comparisons, but up to $8.66n\ln(n)$ array accesses (mergesort is not swap based, so we cannot count that).
Insertionsort: $\frac14n^2$ comparisons and $\frac14n^2$ swaps on average.


As you see, this does not readily allow comparisons of algorithms as the exact runtime analysis, but results are independent from machine details.


Other input distributions

As noted above, average cases are always with respect to some input distribution, so one might consider ones other than random permutations. E.g. research has been done for Quicksort with equal elements and there is nice article on the standard sort function in Java


[Answer]: 1 - Quick sort is inplace (doesn't need extra memmory, other than a constant amount.)

2 - Quick sort is easier to implement than other efficient sorting algorithms.

3 - Quick sort has smaller constant factors in it's running time than other efficient sorting algorithms.

Update: 
    For merge sort, you need to do some "merging," which needs extra array(s) to store the data before merging; but in quick sort, you don't. That's why quick sort is in-place. There are also some extra comparisons made for merging which increase constant factors in merge sort.


[Answer]: In one of the programming tutorials at my university, we asked students to compare the performance of quicksort, mergesort, insertion sort vs. Python's built-in list.sort (called Timsort). The experimental results surprised me deeply since the built-in list.sort performed so much better than other sorting algorithms, even with instances that easily made quicksort, mergesort crash. So it's premature to conclude that the usual quicksort implementation is the best in practice. But I'm sure there much better implementation of quicksort, or some hybrid version of it out there.

This is a nice blog article by David R. MacIver explaining Timsort as a form of adaptive mergesort.


[Answer]: Most of the sortings methods have to move data around in short steps (for example, merge sort makes changes locally, then merges this small piece of data, then merges a bigger one. ..). In consequence, you need many data movements if data is far from its destination.

Quicksort, on the other side tries to interchange numbers that are in the first part of the memory and are big, with numbers that are in the second part of the array and are small (if you are sorting $a \le b$, the argument is the same in the other sense), so they get quickly allocated near their final destination. 


[Answer]: My experience working with real world data is that quicksort is a poor choice. Quicksort works well with random data, but real world data is most often not random.

Back in 2008 I tracked a hanging software bug down to the use of quicksort. A while later I wrote simple implentations of insertion sort, quicksort, heap sort and merge sort and tested these. My merge sort outperformed all the others while working on large data sets.

Since then, merge sort is my sorting algorithm of choice. It is elegant. It is simple to implement. It is a stable sort. It does not degenerate to quadratic behaviour like quicksort does. I switch to insertion sort to sort small arrays.

On many occasions I have found my self thinking that a given implementation works surprisingly well for quicksort only to find out that it actually isn't quicksort. Sometimes the implementation switches between quicksort and another algorithm and sometimes it does not use quicksort at all. As an example, GLibc's qsort() functions actually uses merge sort. Only if allocating the working space fails does it fall back to in-place quicksort which a code comment calls "the slower algorithm".

Edit: Programming languages such as Java, Python and Perl also use merge sort, or more precisely a derivative, such as Timsort or merge sort for large sets and insertion sort for small sets. (Java also uses dual-pivot quicksort which is faster than plain quicksort.)


[Answer]: Under what conditions is a specific sorting algorithm actually the fastest one?


When implemented in a parallel way in hardware, does it need to have a reasonably low latency while requiring as few gates as possible?

Yes, use a bitonic sorter or Batcher odd-even mergesort, latency is $\Theta(\log(n)^2)$ and the number of comparators and multiplexers is $\Theta(n \cdot \log(n)^2)$.
How many different values can each element have? Can every possible value have assigned a unique place in memory or cache?

Yes, use count sort or radix sort, those usually have a linear runtime of $\Theta(n \cdot k)$ (count sort) or $\Theta(n \cdot m)$ (bucket sort) but slow down for a large number of different values, as $k=2^{\#number\_of\_Possible\_values}$ and $m = \#maximum\_length\_of\_keys$.
Does the underlying data structure consist of linked elements?

Yes, always use in-place merge sort. There are both easy to implement fixed size or adaptive (a.k.a. natural) bottom-up in place merge sorts of different arities for linked data structures, and since they never require copying the entire data in each step and they never require recursions either, they are faster than any other general comparison-based sorts, even faster than quick sort.
Does the sorting need to be stable?

Yes, use mergesort, either in place or not, fixed-size or adaptive, depending on the underlying data structure and the kind of data to be expected, even in cases where quick sort would otherwise be preferred, as stabilizing an arbitrary sorting algorithm requires $\Theta(n)$ additional memory in the worst case consisting of original indexes, which also needs to be kept in sync with each swap that is to be performed on the input data, so that every performance gain that quick sort might have over merge sort is probably thwarted.
Can the size of the underlying data be bound to a small to medium size? e.g. Is n  (depending on the underlying architecture and data structure)?

use bitonic sort or Batcher odd-even mergesort. Go to #1
Can you spare another $\Theta(n)$ memory?

Yes


Does the input data consist of large pieces of already sorted sequential data?
use adaptive (aka natural) merge sort or timsort
Does the input data mostly consist of elements that are almost in the correct place?
Use bubble sort or insertion sort. If you fear their $\Theta(n^2)$ time complexity (which is pathological for almost sorted data), maybe consider switching to shell sort with an (almost) asymptotically optimal sequence of gaps, some sequences that yield $\Theta(n \cdot \log(n)^2)$ worst case run time are known, or maybe try comb sort. I'm not sure either shell sort or comb sort would perform reasonably good in practice.


No


Can you spare another $\Theta(\log(n))$ memory?
Yes
Does the underlying data structure allow for directed sequential access or better?
Yes


Does it allow only a single sequence of read/write accesses at a time up till the end of the data has been reached (e.g. directed tape access)?
Yes, use merge sort, but there is no obvious way to make this case in place, so it may require additional $\Theta(n)$ memory. But if you have time and the balls to do it, there is a way to merge 2 arrays in $\Theta(n)$ time using only $\Theta(\log(n))$ space in a stable way, according to Donald E. Knuth "The Art of Computer Programming, Volume 3: Sorting and Searching", exercise 5.5.3. states that there is an algorithm by L. Trabb-Pardo that does so. However, I doubt this would be any faster than the naive mergesort version or the quicksort from the case above.
No, it allows multiple simultaneous accesses to a sequence of data (e.g. is not a tape drive) use quicksort, for practical purposes I would recommend either a randomized or an approximated median one. If you are wary of pathological $\Theta(n^2)$ cases, consider using intro sort. If you are hell-bent on deterministic behavior, consider using the median-of-median algorithm to select the pivot element, it requires $\Theta(n)$ time and its naive implementation requires $\Theta(n)$ space (parallelizable), whereas it may be implemented to only require $\Theta(\log(n))$ space (not parallelizable). However, the median-of-median algorithm gives you a deterministic quicksort which has worst-case $\Theta(n \cdot \log(n))$ run-time.

No, you're screwed (sorry, we need at least 1 way of accessing each data element once)


No, Can you spare a small constant amount of memory?
Yes, Does the underlying data structure allow for random access?


Yes, use heapsort, it has an asymptotic optimal run-time of $\Theta(n \cdot \log(n))$, but dismal cache coherency and doesn't parallelize well.
No, you are screwed

No, you are screwed




Implementation hints for quicksort


Naive binary quicksort requires $\Theta(n)$ additional memory, however, it is relatively easy to reduce that down to $\Theta(\log(n))$ by rewriting the last recursion call into a loop. Doing the same for k-ary quicksorts for k > 2 requires $\Theta(n^{\log_k(k-1)})$ space (according to the master theorem), so binary quicksort requires the least amount of memory, but I would be delighted to hear if anyone knows whether k-ary quicksort for k > 2 might be faster than binary quicksort on some real world setup.
There exist bottom-up, iterative variants of quicksort, but AFAIK, they have the same asymptotic space and time boundaries as the top-down ones, with the additional down sides of being difficult to implement (e.g. explicitly managing a queue). My experience is that for any practical purposes, those are never worth considering.


Implementation hints for mergesort


bottom-up mergesort is always faster than top-down mergesort, as it requires no recursion calls.
the very naive mergesort may be sped up by using a double buffer and switch the buffer instead of copying the data back from the temporal array after each step.
For many real-world data, adaptive mergesort is much faster than a fixed-size mergesort.
the merge algorithm can easily be parallelized by splitting the input data into k approximately same-sized parts. This will require k references into data, and it is a good thing to choose k such that all of k (or c*k for a small constant c >= 1) fit into the nearest memory hierarchy(usually L1 data cache). Choosing the smallest out of k elements the naive way(linear search) takes $\Theta(k)$ time, whereas building up a min-heap within those k elements and choosing the smallest one requires only amortized $\Theta(\log(k))$ time (picking the minimum is $\Theta(1)$ of course, but we need to do a little maintenance as one element is removed and replaced by another one in each step).
The parallelized merge always requires $\Theta(n)$ memory regardless of k.
From what I have written, it is clear that quicksort often isn't the fastest algorithm, except when the following conditions all apply:
there are more than a "few" possible values
the underlying data structure is not linked
we do not need a stable order
data is big enough that the slight sub-optimal asymptotic run-time of a bitonic sorter or Batcher odd-even mergesort kicks in
the data isn't almost sorted and doesn't consist of bigger already sorted parts
we can access the data sequence simultaneously from multiple places
memory writes are particularly expensive (because that's mergesort's main disadvantage), so far as it slows down the algorithm beyond a quicksort's probable sub-optimal split. or we can only have $\Theta(\log(n))$ additional memory, $\Theta(n)$ is too much (e.g. external storage)



[Answer]: You switched in your question from "is better" to "has better runtime". These are not the same. If you look at the other answers, you will find that Mergesort might run faster - but Quicksort has the advantage that it uses negligible space for things other than the array to be sorted. That means I can call it without fear of running out of memory, no matter how large the array is. That is not quite true for Mergesort, if the data can be close in size to the available data. And if virtual memory is available, Mergesort can take a lot longer due to paging.
In practice you wouldn't just use a O (n log n) algorithm, you would check if you can do things faster in special cases. If your array is mostly ascending or mostly descending with few exceptions, it can be sorted in linear time. (If let's say all but 1/8th of the values are in ascending order, it can be sorted in O (n log n), but much faster than using Quicksort or Mergesort).
If you go a bit further: If your array consists of an initial part that is mostly sorted, a middle part, and an end part that is mostly sorted, and the middle part is substantially smaller than the whole array, then we can sort the middle part with Quicksort or Mergesort, and combine the result with the sorted initial or end parts. However, this doesn't affect the question, because the middle part can still be sorted with Quicksort or Mergesort.
Another question is: Can the work be multi-threaded? The partitioning is hard to divide between two threads. Once the array is partitioned, each half can easily be handled by a separate thread. If you have many threads, being able to use only one for the initial partitioning is painful. With Mergesort, we can also do many smaller merges using many threads. We have the advantage that a single merge can also be run in parallel by one thread producing the lower half of the sorted array, and another thread working from the high values down, producing the higher half of the sorted array. I think this gives a large advantage to Mergesort if many threads are available.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:203059
Title: Why is Quicksort called "Quicksort"?
Body: The point of this question is not to debate the merits of this over any other sorting algorithm - certainly there are many other questions that do this.  This question is about the name.  Why is Quicksort called "Quicksort"?  Sure, it's "quick", most of the time, but not always.  The possibility of degenerating to O(N^2) is well known.  There are various modifications to Quicksort that mitigate this problem, but the ones which bring the worst case down to a guaranteed O(n log n) aren't generally called Quicksort anymore.  (e.g. Introsort).

I just wonder why of all the well-known sorting algorithms, this is the only one deserving of the name "quick", which describes not how the algorithm works, but how fast it (usually) is.  Mergesort is called that because it merges the data.  Heapsort is called that because it uses a heap.  Introsort gets its name from "Introspective", since it monitors its own performance to decide when to switch from Quicksort to Heapsort.  Similarly for all the slower ones - Bubblesort, Insertion sort, Selection sort, etc.  They're all named for how they work.  The only other exception I can think of is "Bogosort", which is really just a joke that nobody ever actually uses in practice.  Why isn't Quicksort called something more descriptive, like "Partition sort" or "Pivot sort", which describe what it actually does?  It's not even a case of "got here first".  Mergesort was developed 15 years before Quicksort.  (1945 and 1960 respectively according to Wikipedia)

I guess this is really more of a history question than a programming one.  I'm just curious how it got the name - was it just good marketing?


[Answer]: I believe it's because, at the time it was invented, it was very much quicker than all (or, rather, most, as speed also depends heavily on the kind of data and in some cases other algorithm become much faster than quicksort) of the algorithms out there.

So yes, it's historical (I don't know precisely that history, however...)

But I agree that its name should instead contain a hint of the algorithm...


[Answer]: In 1962 research on sorting algorithms wasn't as far advanced as today and the computer scientist Tony Hoare found a new algorithm which was quicker than the other so he published a paper called Quicksort and as the paper was quoted the title stayed.

Quoting the abstract:


  A description is given of a new method of sorting in the random-access store of a computer. The method compares very favourably with other known methods in speed, in economy of storage, and in ease of programming. Certain refinements of the method, which may be useful in the optimization of inner loops, are described in the second part of the paper. 



[Answer]: I believe that it was originally called Hoare Sort after the inventor but the name got changed  fairly early due to Hoare sounding a little to close to whore in English. As to why they chose "quick" instead of something else, I'm not sure.


------------------------------------------------------------

ID: cs.stackexchange.com:126981
Title: Quicksort Time Complexity
Body: I am learning the Quicksort algorithm and I am struggling with understanding the time complexity.

Here is the JavaScript ES6 code for the partition function that is used in the algorithm:

let partition = function(arr, low, high) {
  let pivotValue = arr[low];
  let i = low;
  let j = high;

  while (i  pivotValue) {
      j--;
    }

    if (i 

I read that there is n work done for a single invocation of the partition function because there is n - 1 work being done in the while loop and 1 unit of work being done when copying the pivot back.

I don't understand why there is n - 1 work done in the while loop. As per my understanding, every time you make a comparison between the array value and pivotValue (arr[i]  pivotValue), that is a unit of work being done.

So aren't there actually n units of work being done in the while loop, for a total of n + 1 in the partition function?


[Answer]: The number of "work" being done, should be counted with the big-O notation. This means - that you count the while loop as $O(n)$  and any other constant work done (for example, the work after the while) is $O(1)$ and $O(n)+O(1)=O(n)$ so it wont make a difference.

Counting the "work" without big-O is not well-defined in the general case - so you should avoid it altogether


------------------------------------------------------------

ID: cs.stackexchange.com:157185
Title: Tweaking traditional quicksort worst cases
Body: All the sources I find on the internet say the time complexity of quick sort is worse if:


input array is sorted and we choose the leftmost element as the pivot
input array is sorted reversely and we choose the rightmost element as the pivot


I am wondering if time complexity is still $O(n^2)$ if we do the following instead.

input array is sorted and we choose the rightmost element as the pivot
input array is sorted reversely and we choose the leftmost element as the pivot

Does it even matter at the end of the day, we are choosing two extreme-sized partitions?


[Answer]: Quicksort's behavior is bad when every partition is imbalanced (instead of splitting $m$ elements in two, the split is like $1$ vs. $m-1$), leading to $O(n^2)$. A good pivot is one which has a rank close to the median ($O(n\log n)$). In case of an already sorted array, the middle element would be a good choice.


------------------------------------------------------------

ID: cs.stackexchange.com:109076
Title: Worst Case Scenario for Quicksort algorithm with pivot element n/2
Body: What would the worst case array look like if I decide to always take the element on the position $\frac{n}{2}$ as the pivot element?
I know that if I choose the left or rightmost element as pivot ,the worst case occurs if:


Array is already sorted in same order
Array is already sorted in reverse order
All elements are same


and that the complexity in that cases is $\mathcal{O}({n^2})$.
However, this cases should not be a problem if I take the middle index of the partition as my pivot element.


[Answer]: Since you're not guaranteed anything about the order of the input, it's possible that the n/2 position has the smallest/largest element of the input. Then quicksort will proceed and put everything else on one side of the pivot. 


------------------------------------------------------------

ID: cs.stackexchange.com:136274
Title: $O(n^2)$ running time vs $O(n^2)$ worst case
Body: The use of the phrase "worst-case running time" is really confusing to me.
Isn't plainly stating that the time complexity of an algorithm is $O(n^2)$ supposed to mean that the growth rate of the algorithm is sub-quadratic?
If it is then why do we say that $O(n^2)$ is the worst case time?
Or do they have different meanings?
Thank you.


[Answer]: Suppose that you have an arbitrary array of $n$ numbers, and you give it to  Quicksort to sort.
The expected running time of the algorithm is $O(n \log n)$.  However, if the array is sorted, but in the wrong direction, and you pick the first element as pivot, then the algorithm might actually run in time $O(n^2)$.
That means that if you look at how Quicksort behaves with increasingly larger arrays of reversely sorted data, the time it uses grows as $n^2$.
However, if you spend $O(n)$ time to shuffle the array before sorting it, the expected running time is $O(n \log n)$; i.e. very few time will the algorithm spend close to $O(n^2)$ many operations.

A different example is with respect to amortized complexity.  For example, adding a single element to an ArrayList in Java takes $O(1)$ time, most of the time.  However, on a rare occasion, Java needs to create a new array and copy all the elements over to this new one.  That takes $O(n)$ time.
This means that the worst case complexity for ArrayList.add is $O(n)$, but if you do this operation $n$ times, the total complexity is also $O(n)$, so we say that the amortized complexity is $O(1)$.


------------------------------------------------------------

ID: cs.stackexchange.com:134785
Title: why quicksort can have a best big o notation of (n log n)
Body: Why does quicksort have a big $O$ notation of $(n \log n)$.
I would like some help understanding what exactly $(n \log n)$ is, and then how it applies to quicksort.
Also in $(n \log n)$, what is the base for the $\log$?


[Answer]: The "standard" version of quicksort does not have a worst case time complexity of $O(n \log n)$. In fact, it can even require $\Theta(n^2)$ time. However, quicksort does have an average time complexity of $O(n \log n)$.
You can get quicksort to run in $O(n \log n)$ worst-case time if you use a suitable pivot-selection strategy. In particular you want a pivot-selection algorithm that requires at most linear time to find a pivot ensuring that the two recursive calls of quicksort are  performed on at least a constant fraction of the input elements. See Median of medians.
The base of the $\log$ doesn't really matter as long as it is a constant greater than $1$. This is because the big-oh notation hides constant multiplicative factors and if you consider two possible bases $a$ and $b$, you have $\log_a n = \frac{\log_b n}{\log_b a}$, where $\log_b a = \Theta(1)$. That said, $\log$ usually refers to the binary logarithm in computer science.


------------------------------------------------------------

ID: cs.stackexchange.com:98686
Title: What is the Space Complexity of Tail Recursive Quicksort?
Body: Looking at the following tail recursive quicksort pseudocode

QuickSort(A[1, ..., n], lo, hi)
Input: An array A of n distinct integers, the lower index and the higher index
         // For the first call lo = 1 and hi = n
Output: The array A in sorted order

If lo = hi return
         // The array A is already sorted in this case
If lo > hi or indices out of the range 1 to n then return

Else
      Pick an index k in [lo,hi] as the pivot
              // Assume that this can be done using O(1) cells on the stack
      i = Partition(A[lo, ..., hi], k)
              // Use in-place partitioning here so assume that this can be done
              // using O(1) space on the stack

If i - lo 

Assuming that the pivot is chosen adversarially each time I analyzed that it should have a space complexity of O(logn) [which I am not entirely sure is correct], but how would the space complexity be affected if the pivot is then chosen uniformly at random? I am fairly new to understanding space complexity over time complexity, so any feedback is appreciated!


[Answer]: In the worst case the space complexity will be $\Theta(n)$. The first recursive call to quicksort is not optimized hence the stack space of the current call is not reused. This can happen $\Theta(n)$ times if i is always hi.

If the pivot is selected uniformly at random then the space required will be $O(\log n)$ with high probability, since the recursion depth will be $O(\log n)$.


[Answer]: If you either use a compiler that supports tail recursion, or make the small change not to use recursion for the larger partition at all, the recursion depth depends on the sizes of the smaller intervals. 

The worst case for recursion depth is the case that each partition divides your subarray exactly in half, making the depth $log_2 (n)$. The best case for recursion depth is the case that each partition divides your subarray into n-1 and 1 items, recursion depth = 1 (obviously the absolutely worst case for execution time). 

So this is a bit paradoxical: Best case space complexity = worst case time complexity. But on the other hand, worst case space complexity = log n is so little that nobody cares about the space complexity. As long obviously as people don't make the mistake of sorting the larger partition first. 

Without eliminating the recursion for the larger half, the worst case for recursion depth is n, and the order of the calls doesn’t matter. 

Usually you can’t make an assumption that values would be different, so the quality of your partition function is important. Some will misbehave with many identical values. 


------------------------------------------------------------

ID: cs.stackexchange.com:142882
Title: Time Complexity of Quicksort when using Median Pivot in sorted array
Body: I am looking for any literature or reference for the worst case complexity of using quicksort on a sorted array with median as pivot. Different internet sources give conflicting answers and often skip this exact question or don't give logic for their answer
Can someone explain whether is it O(nlogn) or O(n^2) and why so when the array is sorted and we pick the middle element as pivot in constant time
I looked at a lot of questions on quicksort and I don't believe this has been asked before in the current form fairly close questions have been addressed


[Answer]: "the array is sorted and we pick the middle element as pivot in constant time" -- If the array is known to be sorted, you don't need to do anything! What is typically being considered is the case when the array happens to be sorted, and the program still has to work correctly no matter what order the array is in.
As for the time complexity, it depends on the details. Using the Median of medians algorithm (note that this gives an approximate median) results in an overall time complexity of $O(n \log n)$ (but as that page notes, this is not often used in practise because of its large constant factor).
There are some common versions of quicksort that use the median of a small number of elements (as opposed to the median of the whole array) as the pivot; these have $O(n^2)$ worst-case time complexity, occurring when the small number of elements considered are always the lowest or highest. On an already sorted array, using the median of the first, middle, and last elements (one common choice) will select an optimal pivot and have $O(n \log n)$ time complexity in this case, whereas using the median of the first three elements, say, will have $O(n^2)$ time complexity in this case.


[Answer]: Because of your input array $A[1...n]$ already sorted:
First check the size of the $A$ in constant time, so if size of array is odd then the median is $A[\lfloor\frac{n}{2}\rfloor]$ that can be obtain in constant time, because you have access to each indices of array $A$ in $\mathcal{O}(1)$. But if the size of array is even, you free  to choose
$A[\frac{n}{2}]$ as median or $A[\frac{n}{2}+1]$ as median, so we  compute  median  in $\mathcal{O}(1)$, hence the running time of Quick sort is
$$T(n)=2T(\frac{n}{2})+\mathcal{O}(1)=\mathcal{O}(n).$$
Note that, because the algorithm choose median as pivot then it partition input to half, so the worst case isn't occur.


[Answer]: Here is another "worst case" that isn't a sorted array, so the algorithm won't be making useless partitions, which seemed to be your concern. Consider the array $[10,30,50,70,90,100,80,60,40,20]$
Picking the median (100) as the pivot, the first partition of elements clearly takes $n$ time. If we pick the median (now 90) as the pivot again then the second partition will take $n-1$ time since the subarrays to the left and right are divided in the most unbalanced possible way. Observe how this is simply arranging one element at a time, as all the remaining elements only go to one side of the partition. With this logic, the next partition takes $n-2$ time, and so on. In such a worst case, we will have to do this for $n$ steps to finish the algorithm, making the time complexity equal to $$n+(n-1)+(n-2)+ \dots + 2=\frac{n(n-1)}{2}-1=O(n^2).$$
Interestingly, this is analogous to Selection Sort, which indeed takes $O(n^2) $ time.
As many would have mentioned, a way to improve quicksort in order to avoid such cases would be to pick a pivot uniformly at random. Note that this too, will have some worst case, but it can be shown that the probability of such a "bad" event happening every time is small.


------------------------------------------------------------

ID: cs.stackexchange.com:92321
Title: Will we ever achieve a $O(n)$ general purpose sorting algorithm (or at least better than $O(n\log(n)))$?
Body: I've been thinking about this question ever since I learnt about the $O(n\log(n))$ sorting algorithms such as MergeSort, QuickSort (average case is pretty much worse case with a good choice of a pivot) and HeapSort. I've been thinking how can we possibly achieve a greater class of efficiency in worst case? Can we do so using specific hardware implementations? A sort that requires a large amount of space? There is indeed RadixSort and BucketSort, but those are only $O(n)$ in specific use cases.


[Answer]: No, n lg n is a theoretical lower bound for general sorting. You can get linear time for special cases. A proof of this fact can be found on cormen.


[Answer]: To expand on what blue-dino wrote, is it indeed impossible to make a general sorting algorithm with worst-case (or even average-case) complexity which is better than $O(n\log(n))$. In this case "general sorting algorithm" refers to the comparison based model of sorting - it means that the only operation our sorting algorithm is allowed to make is a comparison between two elements which tells us what element is bigger/smaller (or equality), but we get no further information.

The proof of this lower bound is based around building a "comparison-tree" with all possible sortings of a group of $n$ different elements, and proving that there will always be a branch of length $O(n\log(n))$, which corresponds to a similarly long computation or run of the algorithm. As I mentioned beforehand, such length can also be proven to be the average length of a branch in the comparison tree, therefore providing a lower limit to the average complexity as well.


------------------------------------------------------------

ID: cs.stackexchange.com:160511
Title: Clarification of divide-and-conquer recurrence explanation in 'Introduction to Algorithms' (CLRS)
Body: The following excerpt is from page 39 of the 4th edition of 'Introduction to Algorithms' (emphasis added):

2.3.2 Analyzing divide-and-conquer algorithms
[...]
A recurrence for the running time of a divide-and-conquer algorithm falls out from the three steps of the basic method. As we did for insertion sort, let $T(n)$ be the worst-case running time on a problem of size $n$. If the problem size is small enough, say $n \leq n_0$ for some constant $n_0 > 0$, the straightforward solution takes constant time, which we write as $\Theta(1)$.

(I was not able to find a Google Books preview of §2.3.2, but the 3rd edition version of the section can be found here.)
I'm a little confused as to what "straightforward solution" is referring to. I initially thought it was the base case, but can't be sure (why didn't the authors just use the term "base case"?).
My lecture slides also seem to think the authors are referring to the base case, but use the term "brute-force solution" in place of "straightforward solution". Here's the text from the relevant slide (emphasis added):

$T(n)$ = running time on a problem of size $n$
If a problem is small enough (say, $n \leq c$ for some constant $c$), we have a base case. The brute-force solution takes constant time: $\Theta(1)$.


What exactly are the terms "straightforward solution" and "brute-force solution" referring to?
If they are referring to the base case, how can we be sure that the base case will take $\Theta(1)$ time? Is it possible for the base case to be non-trivial so that we spend a non-constant amount of time in it?



[Answer]: Yes, "straightforward solution" refers to the base case (and also "brute-force solution" on your slides). You need at least one base case (there could be more than one in some cases), otherwise recursion never ends and your recurrence equation is wrong. Now, the base case is often assumed to require $O(1)$ (constant time), since it is associate to a very small - actually a constant - number of elements (or objects, items etc). Think about a sorting algorithm designed using divide and conquer. Le $n$ be the size of the array in one recursive invocation. Two possible base cases may be $n=1$ (because if your array only includes one element, then it is already sorted) and $n=2$ (because for two elements one comparison is enough to establish the correct sorted order).


[Answer]: The straightforward solution is an easy but possibly inefficient solution, such as the one you can design for a small number of elements (e.g. sorting 1, 2 or 3 elements).
The brute-force solution is a particular case of straightforward solution, which tries all possibilities without caring for efficiency (for instance trying all permutations until you get an increasing sequence).
If these solutions are used to solve the cases $n, then they indeed correspond to the base case of the recurrence. As $n_0$ is a constant, the running time for $nis perforce bounded, i.e. constant in the asymptotic sense.
E.g. real-world implementations of Quicksort often switch to StraightSelection or similar for $n (say). But you may not conclude that the complexity is $O(n^2)$ because $n; the complexity is constant, bounded by $T_{\text{StraightSelection}}(10)$.

One might think of cases such that we use divide & conquer down to some $n_0$ that is a function of the initial $n$, then switch to a straightforward solution. In this case, the complexity associated to the straightforward solution would not be a constant, though it can still be the base case. But honestly, I have never met such a construction.


------------------------------------------------------------

ID: cs.stackexchange.com:150417
Title: "partial sorting" algorithms (aka "partitioning")
Body: Context:
When trying to tame real-world datasets that contain outliers and noise, the interquartile mean is a handy tool: you sort the data, throw away the top and bottom 25% of the data and take the mean of what's left.  (Of course, you can choose other partitioning than top and bottom 25%.)
Which led me to wonder: is there any efficiency to be gained only partially sorting the array?  That is, if we describe three groups: A is the low quartile, B is the middle, and C is the high quartile, we don't care if A or C are sorted: we're going to discard them.  And we don't care if B is sorted since we're only going to take the mean of its values.  It's sufficient that the data is partitioned into those three groups.
The question:

is there a "partial sorting" algorithm that is more efficient than a full sort that will yield those three groups?
Are there additional savings if the array is always a power of 2 (assume N >= 4)?
What if you want to adjust the partition boundaries other than quartiles?  Does that make it less efficient?

Update
I've added "partitioning" to the title, since (I now know) that's the correct term for what this question is about.  Thank you to everyone with good answers!


[Answer]: The algorithm quickselect can return the $k$-th value of an unordered array in average linear time. It can be "improved" (though not so much in practice) using the median of medians to guarantee worst case linear time.
Using that, you can quickselect the $\frac{N}4$-th, $\frac{N}2$-th and $\frac{3N}4$-th values. The algorithm will partition the array into the four desired parts. All this can be done in linear time. It is optimal since you need to check each element at least once.
As long as you use a constant number of them, you could use other values than quartiles (like deciles, for example).


[Answer]: There are a couple of algorithms which are useful for this particular problem. Although they are usually described as selection algorithms, which compute the $k^{th}$ order statistic of an unordered dataset, they can also be used for in-place partitioning of the data-set; a $V_1, V_2, ..., V_n$ is partitioned at $k$ if $\forall i and $\forall i>k,V_i\ge V_k$.
In other words, $V_k$ is at the position it would be if $V$ were fully sorted, and no preceding element is greater. This is sometimes called an "unordered partial sort", but I think "partition" describes it better.
Unlike sorting, this operation can be performed in time $O(n)$ and space $O(1)$, although the most common algorithm only promises expected $O(n)$ time. (In theory, worst-case $O(n^2)$ is possible, but superlinear times are rare with random data and the guaranteed $O(n)$ time algorithm has a lot of overhead.) The best-known instance of such an algorithm is C.A.R. Hoare's Quickselect but there are many other possibilities.
For computing a truncated subsequence, such as the interquartile range, you can gain a bit of extra efficiency by doing both partitions at the same time. Of course, this doesn't improve the asymptotic efficiency, but the real-world execution time is less.
Quickselect can be thought of as the beginning of a quicksort (and it's not a coincidence that Hoare also developed the quicksort algorithm.) It avoids the $\log n$ factor by only recursing on one side of the pivot. (Or, equivalently, looping, which demonstrates $O(1)$ space.) Since the only outcome required is that the vector be partitioned at $V_k$, it's only necessary to process one side of the selected pivot. This should be reasonably intuitive; after partitioning with the pivot, all the elements preceding the pivot point are no greater than the pivot, and all elements following it are at least as great. There must either be at least $k-1$ preceding elements or at least $n-k$ following elements (or both); whichever side of the array satisfies that condition cannot possibly contain the $k^{th}$ order statistic, and thus there is no need to examine it further.
As with quicksort, selecting a good pivot --or at least, not selecting a bad pivot-- is crucial. If you somehow managed to always select a pivot whose value is near the minimum or maximum of the dataset, you'd end up with quadratic time. (But since all recursive calls are tail calls, even in pathological cases, the space requirement continues to be constant.) In practice, you can avoid this problem by selecting a random pivot and using a partitioning algorithm which is tolerant of datasets with large numbers of repeated values.
To compute the interquartile range, you might start by using the selection algorithm to find $V_{n/4}$ and then call the algorithm again on the upper three-quarters of $V$ to find $V_{3n/4}$. But it's evident that this involves quite a bit of duplication of work at the beginning. A better solution is to partition the array until a pivot is discovered which is inside the interquartile range. Once the array is partitioned at that point, quickselect can be called independently on the two sides of the pivot, to select the first quartile on the left and the third quartile on the right.
Quickselect is probably the best solution for the interquartile range, but if the desired subsequence endpoints are closer to the ends of the array --for example, if you want to find the first and ninth decile-- then you might consider using a partial heapsort. This algorithm partitions at $V_k$ in worst-case $O(n \log k)$ (but on average, the time complexity is close to $O(n)$), as follows:

Use heapify to turn the first $k$ elements of $V$ into a max-heap.
For each of the remaining $n-k$ elements:

If the element is less than the heap's maximum, swap it and the heap's maximum and repair the heap.
Otherwise, continue.



If you build the heap backwards, so that the maximum is at position $k$, then you'll end up with $V$ correctly partitioned. (Otherwise, you'll have to swap $V_1$ and $V_k$.)
Repairing the heap at step 2 takes $O(\log k)$, but note that it doesn't always happen. In fact, assuming the dataset is randomly ordered, it happens less and less frequently as you proceed, since the heap's range shrinks each time an element is replaced making it less likely that a new element will need to be inserted.
Again, this algorithm can be adapted to find a range, by building a reversed max-heap at the beginning of the array and a forward min-heap at the end of the array, and then scanning the elements in between. This doesn't help as much as it does with quickselect, but it still helps a bit.
If you're worried about the possibility of a pathologically sorted input --not so much of a worry as it would be for quicksort, since the worst case here is log-linear rather than quadratic-- you can shuffle the input as you go by swapping each element with a randomly selected element from the unprocessed segment, in effect a single step of the Fisher-Yates shuffle algorithm. Although that makes worst-case performance extremely unlikely, it does have a practical cost, because reading a large array in random order is much less cache-friendly than reading the array sequentially.


[Answer]: 
is there a "partial sorting" algorithm that is more efficient than a full sort that will yield those three groups?

Since you're talking about "real-life" scenarios rather than theoretical asymptotics, here's something you could do:

Take a number of uniformly and independently sampled values from the array, and use them to estimate the distribution of values; or at least - estimate the values of your first and third quartiles. The exact number of samples depends on the probability-and-accuracy combination you're interested in, and what you know about the size of the value space; but let's not go into the statistical details here. Suffice it to say that getting a value in a given quarter happens with probability 1/4, independently of other samples, so you can apply large deviation bounds.

Now that you have a likely-decent-estimate of the quartiles, perform your filtering. With very high probability, you now have a little more or a little less than 50% of the elements, ranging between around the 1st quartile and around the 3rd quartile.

You can know how far you are from having gotten the actual quartiles, by keeping count of how many elements are in each of the 3 sets you've formed.


This is usually enough for your real-life needs, as the choice of quartiles is typically arbitrary, i.e. you usually want "the stuff that's around the middle", and the above gives you this. The overall time complexity will be linear in the length of the array.
... Now, you might be asking - why did I even bother suggesting this solution? You already got suggestions of linear-time solutions (even if it's estimated-linear-time), right?
Well, the answer is this is efficient in real life. Your $O(\cdot)$ constant will be extremely low in term of abstract computational operations; and your memory access pattern (after the initial sampling) will be ideal: A clean single consecutive pass over your elements. Cache friendly, CPU prefetch friendly, and it can even be parallelized relatively well.

Are there additional savings if the array is always a power of 2 (assume N >= 4)?

I doubt it. You're thinking like a theoretician here - they/we always hate it when we have to soil our neat pseudo-code with some "if it's not divisible by two" corner cases.


[Answer]: Basically what you're asking about with partitioning into three groups is a famous Dutch national flag problem posed by Dijkstra himself and analysed throughout (with proofs) in his book "A Discipline of Programming". Highly recommend. :)


[Answer]: You have an array of n items, and you want items number l to r to be where the would be in a fully sorted array, and you don't care where the other elements are.
A very slight modification of quicksort does this easily. Start with the subarray from 1 to n. Using the quicksort partitioning method, you create two portions from 1 to k and from k+1 to n. At this point, quicksort would sort both halves recursively. Instead you only handle partitions containing at least one point from l to r.
Normal Quicksort: Call sort (1, n)
sort(l, r):
    if r > l
        partition (l, r) to a..b and c..d
        sort(a, b)
        sort(c, d)

Quicksort to get range x, y: Call sort(1, n, x, y)
sort(l, r, x, y)
    intersect l..r with x..y
    return if intersection is empty
    partition (l, r) to a..b and c..d
    sort(a, b, x, y)
    sort(c, d, x, y)
    



------------------------------------------------------------

ID: cs.stackexchange.com:144271
Title: How can I compare two algorithms using their Big-Oh complexities?
Body: I have two recursive algorithms to solve a particular problem. I have calculated their time complexities as $O(n^2\times\log n)$ and $O(n^{2.32})$. I need to find which algorithm is better in terms of time complexity. I tried plotting graphs but two functions seem to be going together.


[Answer]: You will need to compare the rate of change for both functions. This can be accomplished by taking the derivative of both functions:
$$ O_1'(n^2 log(n)) = \frac{d}{dn} n^2 log(n) = n + 2n × log(n) $$
$$ O_2'(n^{2.32}) = \frac{d}{dn} n^{2.32} = 2.32n^{1.32} $$
Plot these derivatives and compare the graphs. If they still seem too close, continue to take higher order derivatives such as $\frac{d^2O}{dn^2}$, $\frac{d^3O}{dn^3}$, etc. Eventually, you will notice a clear difference in the rate of change.
For those who don't have access to plotting software, you can evaluate the slope of the tangent line for an arbitrarily large n-value:
$$ O_1'(10^6)=\frac{d}{dn} n^2log(n)\bigg|_{n=10^6} = 10^6 + 2(10^6) × log(10^6) ≈ 2.8631 × 10^7$$
$$ O_2'(10^6)=\frac{d}{dn} n^{2.32}\bigg|_{n=10^6} = 2.32(10^6)^{1.32} ≈ 1.9296 × 10^8$$
This means, for $n=10^6$, $O_1$'s tangent line @ $(10^6, y_1)$ would result for some equation:
$$y - y_1 = (2.8631 × 10^7)(x - 10^6) $$
and $O_2$'s tangent line would result in:
$$y - y_1 = (1.9296 × 10^8)(x - 10^6) $$
Now, the answer is clear: $O(n^{2.32})$ has a much more steeper rate of growth than $O(n^2log(n))$ for large values of $n > 10^6$. Conclusion: $O(n^2log(n))$ is more efficient than its counterpart.


[Answer]: I see, that answer is done and accepted, but let me share some thoughts:
Short answer: knowing only upper bounds it's impossible to compare algorithms. We can compare upper bounds, but this doesn't give answer to algorithms relation.
Long answer: assume we have $f\in O(n^2\log n)$ and $g \in O(n^{2.32})$. Because $\exists N \in \mathbb{N}$ such that for $n > N$ holds $n^2\log n, then $ O(n^2\log n)\subset O(n^{2.32})$, so knowing only $g \in O(n^{2.32})$ does not gives warranty, that it is not in $ O(n^2\log n)$ also.
Let me bring analogy: if $x and $y, then we cannot say how $x$ and $y$ are related to each other, without additional info. For example both variants $x=1,y=2$ and $x=2,y=1$ satisfies given conditions.


[Answer]: $O(n^{2.32})$ is kind of unusual. Do you have a mathematical proof for this, or is it an estimated based on observations? In that case, you can't draw any conclusions from it.
Big-Oh is an upper bound. So first you need to check what is the actual behaviour. If an algorithm  runs in $O(n)$ then it is correct (but not very useful) to say it runs in $O(n^{2.32})$, so check that first.
If both are strict upper bounds (so the algorithms are not $o(n^2 \log n)$ and not $o(n^{2.32})$) with a little o, then the second algorithm will be slower at least for some cases if n is large. If it was $\theta$ then it would be the case for all large n.
In practice, if you want to decide which algorithm to use after you implemented them both, you would measure their execution time for their typical inputs, and find the average time and the worst time. As long as the worst time is acceptable, you'd take whichever one takes less time on average for your inputs. For example, if you need to sort a million arrays of size 10 containing almost sorted, the Big-O of the sorting algorithm is irrelevant; what counts is the average sorting time for arrays of size 10 that are almost sorted.
Quite often the execution time for small n is "fast enough". In that case you will care for cases where one or both of the algorithms are not "fast enough". So if the usual case is "fast enough", you worry more about the unusual, very large cases.
Another problem happens when the average time is low and the worst case should be very rare, but is very slow. In that case an adversary might give you inputs that run very slow. For example, Quicksort is typically fast, but for every deterministic implementation, an adversary can prepare inputs of size n that take O(n^2) time. They will never happen in practice, only when created by an adversary.


[Answer]: $O(n^2\log n)$ is better, since $\lim_{n \to \infty} {n^2\log n \over n^{2.32}} = 0$.


[Answer]: If this is a recursive algorithm, and you want it fast: Calculate the exact runtime for both algorithms for small n. Then algorithm A doesn't recursively call A for smaller n, but the faster of algorithm A and algorithm B. And you will have a base case for small n that is calculated without recursion. Consider that one as well. This may give you overall the fastest algorithm, especially if one algorithm is inferior until n gets quite large.
For recursive algorithms, the execution time often doesn't grow smoothly with n, so you might end up with a table telling you for which n the base case is fastest, for which case A is fastest, and for which case B is fastest.


------------------------------------------------------------

ID: cstheory.stackexchange.com:9563
Title: All recursive algorithms are inherently NOT-inplace, isn't it?
Body: As recursive algorithms depend on the stack whose size is in almost all the cases depend on input, why don't we consider all the recursive algorithms as NOT-inplace algorithms?

Consider for example, Quicksort, we say it as inplace sort, but in reality, (average case) it uses O(log(n)) memory in stack? So, how can it be called inplace sort?!


[Answer]: All sorting algorithms on random-access arrays require O(lg(n)) space. Consider the size of the array indices. It requires ceil(lg(n)/lg(2)) bits to represent an array index variable that can hold n distinct values. If your data is in a random-access array, you will need at least one such variable. Therefore, there is a minimum bound of O(lg(n)) on your space complexity, regardless of what algorithm you use.

Note that mergesort cannot avoid this either. You can either have mergesort run in-place, in which case all merge phases (of which there are O(lg(n))) run in parallel for O(lg(n)) space complexity, or you do a non-in-place sort with O(n) space complexity, or you overwrite your input array, in which case you need that index variable again.

Because this bound is rather fundamental (and typically not a problem), usually one does not bother mentioning it.


[Answer]: Who says that Quicksort is an in-place sort? There is a compact version of Quicksort that uses an in-place partition algorithm, but the overall space usage is, as you say, O(log n).


[Answer]: Recursion is an abstract concept. Talking about whether or not it has in-place semantics is nonsense. (Unless of course the language specifies recursion semantics.) This idea is related to how it is nonsense to talk about the speed of a language.

How recursion is implemented is another question. For example, a compiler might perform tail call optimization where recursion can be converted to a loop under the hood among other things. In this case, recursion maintains constant stack space (assuming your architechure uses stacks :D)

EDIT: What does this all mean? It means that there exist recursive algorithms that are in-place when implemented in certain fashions. Thus the statement "All recursive algorithms are inherently not in-place" is false.


[Answer]: 
  how can it be called inplace sort?


Because O(log2 n) bits still isn't very much storage, and the theoretical contingent almost certainly appropriated the term "inplace" from the practitioners in the first place, anyway.

(Side node: quicksort's space usage can be made worst-case by performing tail-call optimization on the larger subproblem (thanks, Sedgewick).)


[Answer]: Although not for sorting, note that there are algorithms that may naturally be described using recursion but that do not require storage of a call stack. For instance, quickselect can be described as a modified version of quicksort that makes only one of the two recursive calls of quicksort; however, using tail recursion for this call avoids the need for a call stack.

Similarly, reverse search procedures (e.g. for listing the vertices of a convex polytope or for various other combinatorial enumeration procedures) are really just performing a recursive traversal of some implicitly-defined tree, but don't need to store more than a constant number of tree nodes at a time — in this case there is more than one recursive call from each call, but the call stack does not need to be stored because it can instead be calculated from the structure of the tree that is being traversed.


[Answer]: In-place sorting algorithms are called so because only a constant number of elements are out of the input (probably an array) at any given time during the sorting process.
Recursion is only a technique that can be used to solve problems. It is defined as "calling itself directly or indirectly with an input of smaller size than the original problem size". 
They are completely two different ideas.
So, there can be in place sorting algorithms which uses recursion. 


[Answer]: Afaik, any programming language with good execution time optimizes tail recursion.

There are even algorithms for which it's easiest to code their "most in-place" variant in pure functional languages like Haskell because said algorithm requires considerable small modifications to a large data structure (see this question pertaining to an example).  

I'll grant that functional languages aren't nearly as explicit with identfying their in-place data.


------------------------------------------------------------

ID: cs.stackexchange.com:192
Title: How to come up with the runtime of algorithms?
Body: I've not gone much deep into CS. So, please forgive me if the question is not good or out of scope for this site.

I've seen in many sites and books, the big-O notations like $O(n)$ which tell the time taken by an algorithm. I've read a few articles about it, but I'm still not able to understand how do you calculate it for a given algorithm.


[Answer]: A statement like "Algorithm $A$ takes $\cal{O}(n)$ time." does not say much about the actual runtime of the algorithm on a given instance. You (usually) have to read it like this:

"For a fixed $n_0 \in \mathbb{N}$ and for all $n \geq n_0$ the runtime of $A$ on a worst-case instance of size $n$ is bounded from above by a function $f(n) = cn$ with $c \in \mathbb{R}$ fixed."

This is called "asymptotic runtime bounded by $\cal{O}(n)$", a much weaker statement. 


First of all, it does not tell you anything for "small" instances; it might even be the case that $n_0$ is so large that you will only ever encounter instances you have no runtime bound for.
$c$ can be arbitrarily large. Combined with the first item, asymptotically slower algorithms might outperform a faster one on all relevant instances.
A bound given by $\cal{O}$ is not necessarily sharp. For instance, all polynomial algorithms have runtime in $\cal{O}(2^n)$; therefore, such a bound might not characterise the algorithm well at all.
In most cases, only worst-case instances are considered. Often, this is not very representative for the real behaviour of the algorithm. Prominent examples include Quicksort and Simplex algorithm.


All of these items come up because sharper analysis is hard or even impossibly to perform without knowledge about the concrete machine an algorithm is run on. Aside from determining $c$ precisely, deriving $\Theta$-classes (both upper and lower bound), average case analysis, amortised analysis and  smoothed analysis are popular techniques to better describe an algorithm's behaviour.

Note furthermore tha memory hierarchies are usually ignored even though they can heavily influence performance. See my answer here for a quick explanation of the different Landau symbols.

That said, analysing an algorithm is conceptually simple: Count all executed operations and sum up their (relative) runtimes. As this is often (too) hard, some tricks are employed; the desired quality of the resulting bounds inform which one to employ to which extent. Performing a good analysis (i.e. one that yields exactly the desired precision) is definitely an art.


Consider only best- or worst-case instances (often implicitly), or take the average over all instances.
Count only those kinds of operations that dominate runtime, i.e. that occur (asymptotically) most often. Form example, analyses of sorting algorithms often count only element comparisons, which is not always appropriate; determining which operation dominates takes experience. Note that restricting yourself like this will effectively prevent you from obtaining any amount of precision regarding runtime estimations.
Apply general calculation rules. For instance, loops translate to sums and recursive algorithms to recurrence equations.
Simplify your model, e.g. count all elementary operations with a unit cost and ignore pipelines, memory IO peculiarities and caches.


If you want to see exceptionally rigorous analyses, take a peek into Donald Knuth's "The Art of Computer Programming". For examples with a more common resolution try Cormen, Leiserson et al, "Introduction to Algorithms".


[Answer]: This part of computer science is called analysis of algorithms. Many times people are satisfied when they are given a guarantee that an algorithm’s performance is not worse than a specified bound and they dont’t care about the exact performance.

This bound is conveniently denoted with the Landau-notation (or big-Oh notation) and in case of $\mathcal{O}(f(n))$ it is an upper bound. That is for inputs of size $n$ the algorithms complexity is guaranteed not to exceed (a constant times) $f(n)$. Most of the time it is clear from the context what the “unit” this bound is measured in is. Note that “runtime” measured in actual time units (seconds, minutes, etc.) is frowned upon as you can’t compare them meaningful across different computers. Typically a “costly” elementary operation is identified, like compares and exchanges in sorting algorithms, or pushs and pops if the algorithm includes a stack, or updates to a tree data structure used in the algorithm. This elementary operation is understood as to be the dominating one, that has the largest contribution to the algorithm’s complexity, or perhaps it is more expensive than another one in a particular setting (multiplications are considered to be more expensive then additions for example). It has to be selected so that the cost of other operations are (at most) proportional to the number of the elementary operations.

This upper bound is called the worst-case bound or the worst-case complexity of the algorithm. Because it has to hold for all inputs of the same size $n$ and the worst-case inputs incur the highest (worst) cost. The actual performance for a specific input of size $n$ may be a lot lower than this upper bound.

While it is possible to do an exact analysis it is usually much more involved to arrive at an exact result. Also, an exact analysis means accounting for all operations in the algorithm, and that requires a rather detailed implementation; while counting elementary operations can be done from a mere sketch of the algorithm most of the time. It is nice if such an analysis is possible but it is not always necessary.  Because small inputs are not much of a problem, you want to learn what happens when the input size $n$ gets large. Knowing that the algorithm’s complexity is bounded by $5n^2+3n-2\log(n)$ is nice but overkill when looking at the performance asymptotically ($n\rightarrow \infty$), because the term $n^2$ dominates the others for large $n$. So proving an upper bound of $\mathcal{O}(n^2)$ suffices.

If you have two algorithms and you can guarantee that one performs like $\Theta(n^2)$ and the other like $\Theta(n)$ you can easily decide which one is “faster”, has a lower “cost”, uses less elementary operations, just by noting that a quadratic function grows faster than a linear function. In practice that means that the linear algorithm finishes earlier or that it can process larger inputs in the same time. But it might be that the constants hidden by $\Theta$ are such that for practical $n$ the $\Theta(n^2)$ algorithm is faster.

However if you have three algorithms all with a $\mathcal{O}(n^3)$ guarantee on their complexity you are hard pressed when you have to decide which algorithm to choose. Then more detailed bounds are needed, or perhaps even an exact analysis.

It may very well be that this analysis is hard to do and you can’t give a tight bound. Then there is a gap between the actual worst-case performance, perhaps $n^2$ and your bound of perhaps $\mathcal{O}(n^3)$. Then a clever idea, a more involved analysis is necessary to close the gap and provide an improved bound. This is just an improvement of the bound not to the algorithm. Typically you have to argue more carefully when you want to prove a tighter bound.

This all said, an analysis of an algorithm can be as simple as looking at the implementation and counting the nesting depth of the for loops to conclude that the operations in the innermost loop are executed not more than $\mathcal{O}(n^3)$ times when say three loops are nested.

For some types of algorithms the analysis follows always the same pattern, so there is a theorem like the master theorem, that tells you generically what the algorithms performance will be. Then you only have to apply the theorem to get the bound.

Perhaps you are faced with a recursive algorithm and you are able to describe the algorithms complexity by a recurrence relation. Then solving the recurrence gives the desired bound.

Typically you have to exploit some properties of the inputs. Sorting algorithms are deeply connected to permutations and knowing something about the number of inversions in a permutation helps a lot when analysing the performance.

There is no general approach on how to proceed in an analysis. As discussed above it depends on the algorithm, its inputs, its implementation, the elementary operation choosen, what mathematical tools you have at hands, the desired sharpness of the bound.

Some may prefer a smoothed-analysis or an average-case analysis over a treatment of the worst-case. Then different techniques are necessary.


[Answer]: Big O notation ($\mathcal{O}$) ignores all constant factors so that you're left with an upper bound on growth rate.

For a single line statement like assignment, where the running time is independent of the input size $n$, the time complexity would be $\mathcal{O}(1)$:

int index = 5;  *//constant time*   
int item = list[index]; *//constant time*


For a loop like:

for i:=1 to n do  
  x:=x+1;


The running time would be $\mathcal{O}(n)$, because the line $x=x+1$  will be executed $n$ times. 

But for:

for ( i = 0; i 

It'd be $\mathcal{O}(n^2)$ because the statement will be executed $n$ times for every $i$.

For a while statement, it depends on the condition and the statement which will be executed in it. 

i := 1;
while ( i 

The running time is logarithmic, $\mathcal{O}(\log n)$ because of the multiplication by 2.

For instance:

Double test(int n){  
  int sum=0;  -> 1 time
  int i;  -> 0 time
  for(i=0; i n+2 times
  {
    scanf("%d",&a);  -> n+1 times
    sum=sum+a;  -> n+1 times
  }
  return sum;  -> 1 time
}




Which is $3n+6$. Hence, $\mathcal{O}(n)$.

For more information, have a look at this Wikipedia atricle: time complexity


[Answer]: Let me give a graphical representation. Consider two functions $f(n)$ and $g(n)$ that looks something like below:



When you say a function $f(n)$ is bound by $\mathcal{O}(g(n))$  i.e. ($f(n)=\mathcal{O}(g(n))$) what you actually mean is there exists a constant $c \gt 0$ and $n_0$ such that $f(n)\leq c\cdot g(n)$, $\forall n\geq n_0$  like the figure below:


Likewise, when you say a function $f(n)$ is bound by $\Omega (g(n))$  i.e. ($f(n)=\Omega (g(n))$) what you actually mean is there exists a constant $c \gt 0$ and $n_0$ such that $f(n) \geq c\cdot g(n)$, $\forall n\geq n_0$ like the figure below:



In the same manner, when you say a function $f(n)$ is bound by $\Theta (g(n))$  i.e. ($f(n)=\Theta (g(n))$) what you actually mean is there exists a two constants $c_1 \gt 0 $ and $c_2 \gt 0$ for the SAME FUNCTION and $n_0$ and $n_1$ such that $c_1\cdot g(n)\leq f(n) \leq c_2\cdot g(n)$, $\forall n\geq n_0,n_1$  like the figure below:



NOTE: You can remember these notations as — $O$: when you write $O$ you always end up finishing at the top so the line is above, $\Omega$: When you write the notation, you always finish it at the bottom so the line is below the function and $\Theta$: the line is in the middle.

Now, having said this and you have a basic understanding of all asymptotic notations, look at the following figure from here. The following figure dices up the analysis of runtime of the merge sort algorithm:




[Answer]: $\cal{O}$, $\Theta$ and $\Omega$ all denote some sort of expression, with it's constant factors stripped off. The difference is that the O notation sets an upper bound on the algorithm's running time, the Omega notation sets a lower bound, and the Theta notation "sandwiches" the algorithm's running time.

To calculate the running time of an algorithm, you have to find out what dominates the running time. For example, if you've designed an algorithm which does binary search and quick sort once, it's running time is dominated by quick sort.

But you'll finally have to calculate the running time of an algorithm which doesn't (at least partially) consist of algorithms which you have seen before. In this case, you have to find the part that spends the most time. It usually means that you have to look at loops, nested loops and recursive calls.

There is much more to say on algorithm analysis and there are also other techniques, like amortized analysis. To find out more, I highly recommend you to read "Introduction to Algorithms", Third Edition, CLRS.


------------------------------------------------------------

ID: cs.stackexchange.com:5030
Title: Why is Radix Sort $O(n)$?
Body: In radix sort we first sort by least significant digit then we sort by second least significant digit and so on and end up with sorted list. 

Now if we have list of $n$ numbers we need $\log n$ bits to distinguish between those number. So number of radix sort passes we make will be $\log n$. Each pass takes $O(n)$ time and hence running time of radix sort is $O(n \log n)$ 

But it is well known that it is linear time algorithm. Why? 


[Answer]: Be careful with your analysis: what do you assume to make sorting run in $O(n)$ time? This is because each of your digits are in a range from $0$ to $k-1$, meaning your digits can take on $k$ possible values. You need a stable sorting algorithm, so you can for example choose counting sort. Counting sort runs in $\Theta(n+k)$ time. If $k=O(n)$, counting sort runs in linear time.

Each one of your strings or numbers have $d$-digits. As you say, you make $d$ passes over them. Hence, radix sort clearly runs in $\Theta(d(n+k))$ time. But if we consider $d$ to be constant and $k=O(n)$, we see that radix sort runs in linear time.


[Answer]: 
if we have a list of $n$ numbers we need $\log n$ bits

No: if we have a list of numbers between $0$ and $2^k - 1$, we need $k$ bits. There is no relationship between $k$ and $\log n$ in general.
If the numbers are all distinct, then $\log n \le k$, and radix sort on distinct numbers therefore has a time complexity of $\Omega(n \log n)$. In general, the complexity of radix sort is $\Theta(n \, k)$ where $n$ is the number of elements to sort and $k$ is the number of bits in each element.
To say that the complexity of radix sort is $O(n)$ means taking a fixed bit size for the numbers. This implies that for large enough $n$, there will be many duplicate values.

There is a general theorem that an array or list sorting method that works by comparing two elements at a time cannot run faster than $\Theta(n \log n)$ in the worst case. Radix sort doesn't work by comparing elements, but the same proof method works. Radix sort is a decision process to determine which permutation to apply to the array; there are $n!$ permutations of the array, and radix sort takes binary decisions, i.e. it decides whether to swap two elements or not at each stage. After $m$ binary decisions, radix sort can decide between $2^m$ permutations. To reach all $n!$ possible permutations, it is necessary that $m \ge \log (n!) = \Theta(n \log n)$.
An assumption in the proof that I did not write out above is that the algorithm must work in the case when the elements are distinct. If it is known a priori that the elements are not all distinct, then the number of potential permutations is less than the full $n!$. When sorting $k$-bit numbers, it is only possible to have $n$ distinct elements when $n \le 2^k$; in that case, the complexity of radix sort is indeed $\Omega(n \log n)$. For larger values of $n$, there must be collisions, which explains how radix sort can have a complexity that's less than $\Theta(n \log n)$ when $n \gt 2^k$.


[Answer]: I think the assumption $k = \log_2(n)$ is wrong. You can perform radix sort with numbers in, e.g., hex. Thus, at each step you split you array of numbers into $16$ buckets.


------------------------------------------------------------

ID: cstheory.stackexchange.com:10542
Title: When are two algorithms said to be "similar"?
Body: I do not work in theory, but my work requires reading (and understanding) theory papers every once in a while. Once I understand a (set of) results, I discuss these results with people I work with, most of whom do not work in theory as well. During one of such discussions, the following question came up:

When does one say that two given algorithms are "similar"?

What do I mean by "similar"? Let us say that two algorithms are said to be similar if you can make either of the following claims in a paper without confusing/annoying any reviewer (better definitions welcomed):

Claim 1.  "Algorithm $A$, which is similar to algorithm $B$, also solves problem $X$" 

Claim 2. "Our algorithm is similar to Algorithm $C$"

Let me make it slightly more specific. Suppose we are working with graph algorithms. First some necessary conditions for the two algorithms to be similar:


They must be solving the same problem.
They must have the same high level intuitive idea.


For instance, talking about graph traversal, breadth-first and depth-first traversal satisfy the above two conditions; for shortest-path computations, breadth-first and Dijkstra's algorithm satisfy the above two conditions (on unweighted graphs, of course); etc.

Are these also sufficient conditions? More specifically, suppose two algorithms satisfy the necessary conditions to be similar. Would you indeed call them similar, if


they have different asymptotic performance?
for a special class of graphs, one algorithm requires $\Omega(n)$ time while the other requires $O(n^{1/3})$ time?
they have different terminating conditions? (recall, they are solving the same problem)
the pre-processing step is different in the two algorithms?
the memory complexity is different in the two algorithms?


Edit: The question is clearly very context dependent and is subjective. I was hoping that the above five conditions, however, will allow getting some suggestions. I am happy to further modify the question and give more details, if needed to get an answer. Thanks!


[Answer]: It is a tough problem to give even a coherent definition of "Algorithm A is similar to Algorithm B". For one, I don't think that "they must be solving the same problem" is a necessary condition. Often when one says in a paper that "the algorithm $A$ of Theorem $2$ is similar to the algorithm $B$ in Theorem $1$", the algorithm $A$ is actually solving a different problem than that of $B$, but has some minor modifications to handle the new problem. 

Even trying to determine what it means for two algorithms to be the same is an interesting and difficult problem. See the paper "When are two algorithms the same?" http://research.microsoft.com/~gurevich/Opera/192.pdf 


[Answer]: More often than not, it means "I don't want to write out Algorithm B in detail, because all the interesting details are nearly identical to those in Algorithm A, and I don't want to go over the 10-page limit, and anyway the submission deadline is in three hours."


[Answer]: If you mean "similar" in the colloquial sense, I think JeffE's answer captures what some people mean.

In a technical sense though, it depends on what you care about. If asymptotic time complexity is all you care about, the difference between recursion and iteration may not matter. If computatability  is all you care about, the difference between a counter variable and a one-symbol stack do not matter.

To compare algorithms, a first step would be to make the notion of equivalence precise. Intuitively, let $A$ be the space of algorithms and $M$ be a space of mathematical objects and $\mathit{sem}: A \to M$ be a function encoding that $\mathit{sem}(P)$ is the meaning of algorithm $P$. The space $M$ could contain anything ranging from the number of variables in your algorithm, to its state-graph or it's time complexity. I don't believe there is an absolute notion of what $M$ can be. Given $M$ though, we can say two algorithms are equivalent if $\mathit{sem}(P)$ equals $\mathit{sem}(Q)$. Let me add that I think each of the five criteria you mentioned can be formalised mathematically in this manner.

If we want to talk about an algorithm being more general than another (or an algorithm refining another), I would endow $M$ with more structure. Imagine that $(M, \sqsubseteq)$ is a partially ordered set and the order $x \sqsubseteq y$ encodes that $x$ is a more defined object than $y$. For example, if $M$ contains sets of traces of an algorithm and $\sqsubseteq$ is set inclusion, $\mathit{sem}(P) \sqsubseteq \mathit{sem}(Q)$ means that every trace of $P$ is a trace of $Q$. We can interpret this as saying that  $P$ is more deterministic than $Q$. 

Next, we could ask if it's possible to quantify how close two algorithms are. In this case, I would imagine that $M$ has to be endowed with a metric. Then, we can measure the distance between the mathematical objects that two algorithms represent. Further possibilities are to map algorithms to measure spaces or probability spaces and compare them using other criteria. 

More generally, I would ask - what do you care about (in intuitive terms), what are the mathematical objects representing these intuitive properties, how can I map from algorithms to these objects, and what is the structure of this space? I would also ask if the space of objects enjoys enough structure to admit a notion of similarity. This is the approach I would take coming from a programming language semantics perspective. I'm not sure if you find this approach appealing, given the vastly different cultures of thought in computer science.


[Answer]: Very interesting question, and very nice paper Ryan!

I do definitely agree with the idea that making an assessment on the overall similarity between algorithms is mainly a subjective value judgement. While from a technical point of view there are a number of features that are closely observed to decide upon the similarity of algorithms, in the end, it is also a matter of personal taste. I will try to provide a description of the importance of both sides of the same coin while referring to the particular points of your question:

From a technical point of view:


Ryan already pointed out that both algorithms must solve the same problem. One could go even further and generalize this notion by saying that it is usually enough to prove that there is a polynomial transformation of the same instance that is understoodable by algorithm A so that algorithm B can handle it. However, this would be actually very weak. I do prefer to think of the similarity in a stronger sense. 
However, I would never expect for two equivalent algorithms to have the same intuitive idea ---though, again, this is a definition which ain't easy to capture. More than that, it is often the case that algorithms that are deemed to be similar do not follow the main rationale. Consider for example some sorting algorithms which however, originated in different ways following different ideas. As an extreme example consider genetic algorithms which are usually considered by the mathematical community just as stochastic processes (and therefore they are equivalent in their view) which are then modeled and analyzed in quite a different way. 
Moreover, I would even generalize this notion to say that other technicalities such as the termination condition or the pre-processing stage do not matter often. But this is not always the case. See for example Dijkstra's Algorithm versus Uniform Cost Search or a Case Against Dijkstra's Algorithm. Both algorithms are so close that most people do not tell the difference, yet the differences (being technical) were very important for the author of that paper. Much the same happens with the pre-processing step. In case you are familiar with the $N$-puzzle, then observe that an A$^*$-like search algorithm using the Manhattan distance or $(N^2-1)$ additive pattern databases would actually expand the same number of nodes in exactly the same order, and that makes both algorithms (and their heuristics) to be strictly equivalent in a very strong sense, whereas the first approach has no pre-processing and the second one has a significant overhead before starting to solve a particular instance. However, as soon as your Pattern Databases consider more simulatenous interactions there is a huge gap in performance between them, so that they are definitely different ideas/algorithms.
As a matter of fact, I think that most people would judge algorithms for their purpose and performance. Therefore, asymptotic performance is a good metric to reason about the similarity between programs. However, bear in mind that this performance is not necessarily the typical case so that if two algorithms have the same asymptotic performance but behave differently in practice, then you would probably conclude that they are different. The strong evidence in this regard would be that both algorithms have the same performance both on time and memory (and this, as Suresh said, makes DFS and BFS to look different). In case this assertion does not sound convincing to you, please refer to the excellent (and very recommended book): Programming the Universe by Seth Lloyd. In page 189 he refers to a list with more than 30 measures of complexity that can be used to regard algorithms as being different.


So what makes algorithms to be similar/different? In my view (and this is purely speculative), the main difference is about what they suggest to you. Many, many (many!) algorithms differ just in a few technicalities when serving to the same purpose so that the typical case is different for different ranges of the input. However, the greatest of all differences is (to my eye) what they suggest to you. Algorithms have different capabilities and therefore their own strengths and weaknesses. If two algorithms look like being the same but might be extended in different ways to cope with different cases then I would conclude that they are different. Often, however, two algorithms do look much the same so that you would regard them to be the same ... until someone arrives making a key distinction and suddenly, they are completely different!

Sorry, my response was in the end so long ... 

Cheers,


[Answer]: Along the lines of Jeff's answer, two algorithms are similar if the author of one of them expects that the author of the other one might be reviewing her paper. 

But joking aside, in the theory community, I would say that what problem algorithm A is solving is rather tangental to whether it is "similar" to algorithm B, which might be solving a completely different problem. A is similar to B if it "works" because of the same main theoretical idea. For example, is the main idea in both algorithms that you can project the data into a much lower dimensional space, preserve norms with the Johnson-Lindenstrauss lemma, and then do a brute-force search? Then your algorithm is similar to other algorithms that do this, no matter what problem you are solving. There are some small number of heavy-duty algorithmic techniques that can be used to solve a wide variety of problems, and I would think that these techniques form the centroids of many sets of "similar" algorithms. 


[Answer]: Any mention of similarity without defining a similarity metric is not well-defined. There are many ways in which two algorithms can be similar:

Quicksort and Mergesort solve very similar problems, but they use different algorithms to do so. They have similar algorithmic complexity (although their worst-case performance and memory usage can vary). Quicksort and Mergesort are both similar to Bubblesort,however Bubblesort has very different performance metrics. If you ignore complexity statistics Quicksort, Mergesort and Bubblesort are all in the same equivalence class. However, if you care at all about algorithmic complexity, then Quicksort and Mergesort are much more similar to each other than either is to Bubblesort. 

Smith-Waterman dynamic programming and HMM-sequence comparison attempt to solve the problem of aligning two sequences. However, they take different inputs. Smith-Waterman takes two sequences as input, and HMM-sequence comparisons take a HMM and a sequence as input. Both output sequence alignments. In terms of motivating ideas, both of these are similar to Levenshtein's edit distance, but only at a very high level. 

Here are some criteria by which two algorithms might be called similar:


Input/output types
Algorithmic/Memory Complexity
Assumptions about types of inputs (eg only positive numbers or floating point stability)
Nested relationships (eg some algorithms are special cases of others)


The critical decision about the meaning of similarity remains. Sometimes you care about the complexity of an algorithm, sometimes you don't. As the definition of similarity depends on the context of the discussion, the term "similar algorithm" isn't well-defined. 


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:5727
Title: Quicksort and don't bother?
Body: Especially when writing 'standard' (non-HPC) applications, do you consider what sorting algorithm to pick, or just settle with quicksort (which is what most libraries just call sort)? To some extent it may be profitable in specific situations, but on the other hand proper optimization requires some time to analyse the problem and make benchmarks. 


[Answer]: Although it obviously doesn't matter to the bits and timeslices. I find merge sort to be easier to write and understand than quicksort.  So if I'm going to write my own sorting algorithm I'd use that.


[Answer]: This is the time to pull out the classic quote about premature optimization.  In most cases, it really doesn't matter.  Heck, with the speed of CPUs these days, you could probably bubble sort most data sets and not really notice much.  But when you're sorting really large data sets, and sort performance starts to become an issue, then you should definitely look at other options.


[Answer]: In general, using the default methods unless there is a specific need to do something more exotic keeps everything a lot more readable/understandable down the road IMHO.

If you experience (or in some cases, strongly suspect) that you have a performance problem that is the time to add complexity.

On the other hand, if you are using a low enough language that there is not a built-in sort for the kind of objects you need to sort try to pick one or two that cover all your bases and implement those.


[Answer]: At least in a competently written library, I'd expect the built-in sort to be implemented as an Introsort rather than just a Quicksort. The difference rarely matters much, but Introsort eliminates the bad worst-case performance of Quicksort with minimal effect on the more common cases.

To answer your question, however: yes -- that's what you should usually start with, and until/unless you have profiler results indicating that it's a problem, that's where it should remain.


[Answer]: Always call the library routines provided, unless you have very, very good reason not to do so (and you need to document why it is so).

This is because sorting algorithms are hard to get absolutely right.   There was a bug in the Java quicksort with very large datasets, which was identified, fixed and delivered to customers by Sun, so you didn't have to.

Also the default sort in Java 7 has been upgraded to a newer, better sort.  Also for free.

Unless the default sort is provably not good enough for you, stick with it.


[Answer]: At a conference once I heard a nice story about this.

At Microsoft someone was writing a VB app (c. VB 3) and mailed a bunch of people saying that he had a load of values and he wanted them to appear in the combobox in order, how should he go about it.

Everyone dived for their old computer science text books, looking up highly efficient routines and porting them to Visual Basic and mailing them to him.  One guys just mailed back "how many values in the combobox?".

"About 50" came the reply.

"Just set the sorted property to TRUE".

In 99.9999% of instances sorting is best done using a library, control or in the SQL select as the performance difference between the library routine and anything you write will be negligible and the effort and maintenance overhead will massively outweigh the consequences.


------------------------------------------------------------

ID: cseducators.stackexchange.com:5281
Title: Why and how is it efficient to process sorted arrays than unsorted arrays?
Body: It is well accepted that processing sorted arrays is easier and efficient. What would be the pedagogical approach to explain how and why it is more efficient to process sorted collections than unsorted collections.


[Answer]: First, it is not "well accepted" that processing sorted array is more efficient. It depends on what you do. If you spend your time inserting, for instance, then sorted arrays are not necessarily a good choice. You are better off inserting at the end (say if you have resizable arrays such as Vector or ArrayList in Java), and then sort afterwards (you'll pay $O(N*log(N))$ instead of $O(N²)$).

It is true that if you don't have insertions (or very few insertions with respect to searching or other operations for instance) then it is more efficient.
If you want to find an element in a sorted array, you can use binary search which is $O(log(N))$ while if your array is not sorted you have to scan the whole thing which is $O(N)$.

Likewise if you want to perform set operations on two sorted arrays (intersection, union, difference), then doing so on sorted Arrays is linear. If they are not sorted you will be either quadratic (naive algorithm) or have to use an auxiliary data structure (such as a hash table).

Sorted arrays have the same complexity for these operations than the more involved tree structures (AVL or red-black trees) and they have a very compact and simple representation. But you pay the price that insertion is more expensive, since it has to maintain the order of elements.


[Answer]: I would do it unplugged. Give then a sorted stack of cards, get them to find a number. Repeat with an unsorted list. 

Ensure that there are gaps, not a sequence, but sorted. I got caught out with this, pupils could find a card in a sequence in $O(1)$.


[Answer]: If an array is unsorted, and you want to check an item for membership in it, that is O(n) time procedure.  If it is sorted, you can use divide-and-conquer to do this task in O(log(n) time.  


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:234871
Title: Correct way to undo the past few git commits including a merge?
Body: My team is working with git for the first time and has really messed up our git remote due to our inexperience. I'm trying to help educate them on git to solve the problem in the long term, but in the meantime, the commits that have been made are totally wrong. We haven't made very much progress at all, and I don't see the value in retaining these commits when they could cause confusion/additional problems down the line. 

The most recent commit was a merge between the master branch and a second branch that one of the team members unintentionally created. If I try to revert, git states that I must be using the -m flag, but I'm not sure whether I should be reverting or trying to rebase in this case, or how I would do it given the structure of the past couple of commits.

What is the best way to resolve this issue and how should I go about doing it?


[Answer]: I believe you're looking for the git reset command. If you run git log it will show you a commit history for your repo. Then run git reset --hard HEAD-number of commits you'd like to go back.

Documentation here: http://git-scm.com/docs/git-reset


[Answer]: To undo the merge, you can do

git checkout master
git revert -m 1 [SHA of the merge commit]
git push


This will create and push a new commit which reverts all changes that were merged in.

It's also possible to alter master's history so that it appears as if the merge never happened, but I wouldn't recommend it. Your teammates would have trouble when they try to pull master and the history is different. Also it's easy to revert a revert commit if you reverted the wrong changes, but if you alter mater's history, it's harder to undo that.


------------------------------------------------------------

ID: cs.stackexchange.com:159287
Title: Is it better to arrange datatypes in a computer programme using arrays or linked lists?
Body: How should datatypes be organised in a computer programme? Is it better to utilise arrays or linked lists? I'm not sure what the difference is, but I seem to recall hearing that arrays are quicker. Or perhaps it was connected lists? So, which one should I go with?


[Answer]: Whether to use arrays or linked lists to arrange data types in a computer program depends on the specific requirements and characteristics of the program.
Arrays are a good choice when the data size is fixed and known in advance, and when random access to elements is required. They are also more memory efficient than linked lists for small data sets, and the elements of an array are stored in contiguous memory locations, which can result in faster access times.
Linked lists, on the other hand, are a better choice when the data size is not fixed and when frequent insertions or deletions are required. They can be more memory efficient than arrays for large data sets because they only allocate memory for the elements that are actually used, and can be resized dynamically as needed. Linked lists are also useful when elements need to be inserted or deleted frequently, as they can be modified more easily than arrays.
Therefore, the choice between arrays and linked lists depends on the specific requirements of the program. If you need fast access to elements and the size of the data set is known in advance, use an array. If the size of the data set is not fixed and frequent insertions or deletions are required, use a linked list.


[Answer]: Data structures are not chosen "randomly" and there is no absolute preference order.
For a given application, different data structure options can be compared. In particular, for containers or associative containers, one looks at the time and space complexities of insertions, deletions, lookups..., put in relation to what the application will effectively require.
So saying "arrays are quicker" makes little sense. Your question sounds like "is a hammer better than a screwdriver, I heard that they are heavier"?


------------------------------------------------------------

ID: cs.stackexchange.com:68346
Title: Does every data type just boil down to nodes with pointers?
Body: An array or vector is just a sequence of values.  They can surely be implemented with a linked list. This is just a bunch of nodes with pointers to the next node.

Stacks and queues are two abstract data types commonly taught in Intro CS courses.  Somewhere in the class, students often have to implement stacks and queues using a linked list as the underlying data structure, which means we are back to the same "collection of nodes" idea.

Priority queues can be created using a Heap. A heap can be thought of as a tree with the min value at the root.  Trees of all sorts, including BSTs, AVL, heaps can be thought of as a collection of nodes connected by edges.  These nodes are linked together where one node points to another.

It seems like every data concept can always boil down to just nodes with pointers to some other appropriate node.  Is that right?  If it's this simple, why do textbooks not explain that data is just a bunch of nodes with pointers?  How do we go from nodes to binary code?


[Answer]: Well, that is basically what all data structures boil down to. Data with connections.  The nodes are all artificial - they don't actually exist physically. This is where the binary part comes in.  You should create a few data structures in C++ and check out where your objects land in memory.  It can be very interesting to learn about how the data is laid out in memory. 

The main reason for so many different structures is that they all specialize in one thing or another. For example, it is typically faster to iterate through a vector instead of a linked list, due to how pages are pulled from memory.  A linked list is better to store larger sized types because vectors must allocate extra space for unused slots (this is required in the design of a vector).

As a side note, an interesting data structure you may want to look at is a Hash Table.  It does not quite follow the Nodes and Pointers system you are describing.  

TL;DR: Containers basically all Nodes and Pointers but have very specific uses and are better for something and worse for others.


[Answer]: Many (most?) data structures are built of nodes and pointers.  Arrays are another critical element of some data structures.

Ultimately, every data structure is just a bunch of words in memory, or just a bunch of bits.  It's how they are structured and how we interpret and use them that matters.


[Answer]: Let's make an analogy with mathematics. Consider the following sentence: "$f:\mathbb{R} \rightarrow \mathbb{R}$ is a continuous function". Functions are really defined in terms of relations, which are defined in terms of sets. The set of real numbers is the unique complete totally ordered field: all of these concepts have definitions in simpler terms. In order to talk about continuity you need the concept of neighborhood, which is defined in relation to a topology... and so on all the way down to the axioms of ZFC.

Nobody expects you to say all of that just to define a continuous function, otherwise nobody would be able to get any work done at all. We just "trust" that someone made the boring work for us.

Every data structure you can possibly think of boils down to the basic objects that your underlying computational model deals with, integers in some register if you use a random-access machine, or symbols on some tape if you use a Turing machine.

We use abstractions because they free our mind from trivial matters, allowing us to talk about more complex problems. It is perfectly reasonable to just "trust" that those structures work: spiraling down into every single detail is - unless you have a specific reason to do so - a futile exercise that doesn't add anything to your argument.


[Answer]: 
  It seems like every data concept can always boil down to just nodes
  with pointers to some other appropriate node.


Oh, dear no. You are hurting me.

Like I tried to explain elsewhere ("What's the difference between a binary search tree and a binary heap?") even for a fixed data structure there are several levels to understand it.

Like the priority queue you mention, if you  only want to use it, it is an abstract data type. You use it knowing what kind of objects it stores, and what questions you can ask it to do. That's more data structures as a bag of items.
On the next level its famous implementation, the binary heap, can be understood as a binary tree, but the last level is for efficiency reasons implemented as an array. No nodes and pointers there.

And also for graphs for instance, which certainly look like nodes and pointers(edges), you have two basic representations, adjacency array and adjacency lists. Not all pointers I imagine.

When really trying to understand data structures you have to study their good points and teire weaknesses. Sometimes a representation uses an array for efficiency (either space or time) sometimes there are pointers (for flexibility). This holds even when you have good "canned" implementations like the C++ STL, because also there you can choose sometimes the underlying representations. There is always a trade-off there.


[Answer]: Implementation of data structures always boils down to nodes and pointers, yes.

But why stop there?  Implementation of nodes and pointers boils down to bits.

Implementation of bits boils down to electrical signals, magnetic storage, perhaps fiberoptic cables, etc.  (In a word, physics.)

This is the reductio ad absurdum of the statement, "All data structures boil down to nodes and pointers."  It's true—but it only relates to implementation.



Chris Date very able differentiates between implementation and model, though his essay is aimed at databases in particular.

We can go a little bit further if we realize that there is not a single dividing line between model and implementation.  This is similar (if not identical) to the concept of "layers of abstraction."

At a given layer of abstraction, the layers "below" you (the layers on which you are building) are mere "implementation details" for the abstraction or model which you are addressing.

However, the lower layers of abstraction themselves have implementation details.

If you read a manual for a piece of software, you are learning about the abstraction layer "presented" by that piece of software, on which you can build your own abstractions (or just perform actions such as sending messages).

If you learn the implementation details of the piece of software, you will learn how the creators underpinned the abstractions which they built.  The "implementation details" may include data structures and algorithms, among other things.

However, you would not consider voltage measurement to be part of the "implementation details" for any particular piece of software, even though this underlies how "bits" and "bytes" and "storage" actually work on the physical computer.

In summary, data structures are an abstraction layer for reasoning about and implementing algorithms and software.  The fact that this abstraction layer is built on lower-level implementation details such as nodes and pointers is true but irrelevant within the abstraction layer.



A big part of really understanding a system is grasping how the abstraction layers fit together.  So it's important to understand how data structures are implemented.  But the fact that they are implemented, doesn't mean that the abstraction of data structures doesn't exist.


[Answer]: Here's a counter-example: in λ-calculus, every data type boils down to functions. λ-calculus doesn't have nodes or pointers, the only thing it has are functions, therefore everything must be implemented using functions.

This is an example of encoding booleans as functions, written in ECMAScript:




const T   = (thn, _  ) => thn,
      F   = (_  , els) => els,
      or  = (a  , b  ) => a(a, b),
      and = (a  , b  ) => a(b, a),
      not = a          => a(F, T),
      xor = (a  , b  ) => a(not(b), b),
      iff = (cnd, thn, els) => cnd(thn, els)();




And this is a cons list:




const cons = (hd, tl) => which => which(hd, tl),
      first  = list => list(T),
      rest   = list => list(F);




Natural numbers can be implemented as iterator functions.

A set is the same thing as its characteristic function (i.e. the contains method).

Note that the above Church Encoding of Booleans is actually how conditionals are implemented in OO languages like Smalltalk, which don't have booleans, conditionals, or loops as language constructs and implement them purely as a library feature. An example in Scala:

sealed abstract trait Boolean {
  def apply[T, U  U)(els: => V): T
  def ∧(other: => Boolean): Boolean
  def ∨(other: => Boolean): Boolean
  val ¬ : Boolean

  final val unary_! = ¬
  final def &(other: => Boolean) = ∧(other)
  final def |(other: => Boolean) = ∨(other)
}

case object True extends Boolean {
  override def apply[T, U  U)(els: => V): U = thn
  override def ∧(other: => Boolean) = other
  override def ∨(other: => Boolean): this.type = this
  override final val ¬ = False
}

case object False extends Boolean {
  override def apply[T, U  U)(els: => V): V = els
  override def ∧(other: => Boolean): this.type = this
  override def ∨(other: => Boolean) = other
  override final val ¬ = True
}

object BooleanExtension {
  import scala.language.implicitConversions
  implicit def boolean2Boolean(b: => scala.Boolean) = if (b) True else False
}

import BooleanExtension._

(2 


[Answer]: 
  An array or vector is just a sequence of values. They can surely be implemented with a linked list. This is just a bunch of nodes with pointers to the next node.


An array or a vector can be implemented with a linked list, but almost never should be.

That's because accessing the $n$-th element in a linked list requires traversing a chain of $n$ pointers, and thus requires $\Theta(n)$ time.  A tree-based implementation (still made out of just nodes and pointers) is a bit more efficient, requiring just $\Theta(\log n)$ time to access an arbitrary leaf, but that's still far short of the $\Theta(1)$ access time provided by using an actual array (i.e. a sequential block of random access memory).  Also, on the CPU, accessing the actual array is far simpler to implement and faster to execute, and storing it takes less memory due to not having to waste any space on pointers between separate nodes.

Of course, actual physical arrays have their down sides, too: notably, they need $\Theta(n)$ time to insert a new element, something that a linked list can do in $\Theta(1)$ time if you already have a pointer to a neighboring element.  Insertions at the end of the physical array can be amortized down to $\Theta(1)$ on average, at the cost of at most a constant factor of extra memory, just by rounding the actual allocated size of the array up to e.g. the closest power of 2.  But if you need to do a lot of insertions and/or removals of elements in the middle of your list, a physical array may not be the best implementation for your data structure.  Pretty often, though, you can replace insertions and removals with swaps, which are cheap.

If you broaden your scope a little, to include physical contiguous arrays in your toolbox, almost all practical data structures can indeed be implemented with those together with nodes and pointers.

In fact, off the top of my head, I can't think of any common data structure that would need anything else, although in some cases a bit of space or time efficiency can be squeezed out by stepping outside that paradigm.  As an illustrative example of such a case, consider the XOR linked list, which allows a two-way traversable linked list to be implemented using (asymptotically) no more space than a one-way linked list, by replacing the individual next and previous node pointers by their bitwise XOR, and has a few other convenient features too (such as a $\Theta(1)$ reversal operation).  In practice, though, those features are rarely useful enough to overcome its drawbacks, which include extra implementation complexity and incompatibility with standard garbage collection schemes.


[Answer]: 
  If it's this simple, why do textbooks not explain that data is just a bunch of nodes with pointers?


Because that's not what "data" means.  You are conflating abstract ideas with implementations.  "Data" is a highly abstract idea:  It's just another name for "information."  A bunch of linked nodes with pointers (a.k.a., a "linked data structure") is a much more concrete idea:  It's a specific way of representing and organizing information.

Some data abstractions lend themselves very well to "linked" implementations.  There are not many good ways to implement the branching nature of a fully general tree without using explicit nodes and pointers (or, some isomorphism of nodes and pointers.)  But then, there are other abstractions that you would never implement using nodes and pointers. Floating point numbers come to mind.

Stacks and queues fall somewhere in between.  There are times when it makes a lot of sense to do a linked implementation of a stack.  There are other times when it makes much more sense to use an array and a single "stack pointer".


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:254749
Title: 2D linked list vs. multidimensional array/vector
Body: I hope that programmers is the correct stack exchange for this, as it is quite a concept based question.

I'm working on a data structure in C++ that is a represents data in 3D space. The x-y plane is large (say 0 - 10 000) while the z-axis has a very small range (0 - 10). All the data is located on integer (x,y,z) points. However there can be more than one than one data point on an (x,y,z) coordinate.

I have thought of three ways of approaching this subject, and don't have the necessary experience to choose between them.


Using a 2D linked list inside vectors (dynamic arrays) vector> Data where the first list item for that (x,y) point can be accessed using Data[x][y]. structureA contains an its z value, its actual data, as well as a pointer to the next structure in the list (that may, or may not have a different z coordinate, but is located  on the same line defined by x and y). Accessing elements at the middle and end of this list is slow.
Using a three dimensional array/vector: vector>> Data. In this case the first two vectors define the x and y coordinates (as before) and the third vector gives us an array of all of the elements on that line defined by x and y. We can quickly determine how many elements are on this line (by querying the third vector) and can iterate from the back and the front. structureB contains, the point's z value and its data. This seems like a fairly good and safe (regarding memory allocation) solution to me.
Using a three dimensional array/vector and a linked list. In this case the data could be accessed as such: Data[x][y][z] and would point to the first element of a linked list of data that resides at this coordinate. The structure pointed to would not contain its z value here as this is already dealt with in its address in the array. However, many (x,y,z) points have no data, whereas most of those that do have data have 10+ data structures. (All lines defined by a specific (x,y) contain at least one structure). This seems the worst solution to me as in many cases I would have to iterate through empty z coordinates, before hitting a linked list to iterate through.


The emphasis in this program is speed, my data set is less than 100MB on a modern computer, so even an inefficient method of storing it in RAM is fine. Option 2 seems the best to me, although in the past I've normally seen people more inclined to go with option 1. I'm interested to hear your opinions on the best way to handle this along with your reasoning.
Many Thanks.

Update

Thanks for the responses, I stand to learn quite a lot from your comments as I am a hobby programmer, having taken only one short course on the fundamentals of C a few years back.

@MorphingDragon The array is not jagged, all z "slices" have the same range of x and y. The values also don't seem sparse enough for a hash table (which IIRC slow down significantly on hash collision - which is guaranteed as multiple elements have the same (x,y,z)). The dimensions of the set are known on file load (they are included in the file header along with an md5 hash of the file for error checking and a few bit relevant to the analysis).

@J Trana / Florian F The data set is pre-built and is infrequently modified by the program - a piece of data may need to be moved or deleted. For adjusting data particularly just adjusting its z-value (or removing it), which are the most frequent operations. The linked list will be faster as these operations are O(1) as opposed to O(n) in a vector. However, I believe these changes are infrequent enough that having O(n) for changes may be worthwhile if the lookup speed is even marginally faster (as this is the more frequent operation).

The majority of the time the data will be accessed iteratively, taking an n*p slice  from the x-y plane. Pseudocode example:

for each y between startY and (startY + p)
  for each x between startX and (startX + n)
    read all nodes in Data[x][y] //gets nodes for each z on an (x,y) line
    analyse(&nodes) //possibly modify the nodes, possibly just compute something based on their values


Data access is always related to the (x,y) line an element is on, but we  don't always need all the nodes on that line (dependant on their z). So we either have a structure that doesn't require us to retrieve all of them (just the ones we want) or we perform a secondary sort. The data on file represents a sorted list in ascending z for each (x,y).

I will also look into those other data structures. I've never met them, as with very little training, my method is to use STL vectors or lists for everything. I'd like to spend the time to do this properly this time though.

I spent a fair amount of time thinking about this last night after posting the question and seeing the first responses. I realised that "if one piece of data from a point (x,y,z) is required, then all elements from that point are required at the same time." This has led me to think that option 3, my least preferred when I started this yesterday afternoon, is actually the best of the options I've enumerated. If I use a three-dimensional dynamic array of pointers to linked list, I don't exactly lose much speed or heap space setting a pointer to NULL rather than attaching a struct. This also allows full access to any 3d, which while not currently required may reduce some operations from O(n) to O(1).

Having had a brief look over octaves/r-trees, I feel safe saying that my data set is not complex enough to require such an engineering heavy solution. When it comes down to it, i am fairly sure that any of the three options I outlined would work, even if not the fastest, but I would like to learn a good and relatively efficient method, rather than just relying on the awesome power of modern computers to brute force a problem like I normally do.
Thanks for the replies and suggestions so far. :)


[Answer]: I hope it's ok to answer my own question.
I believe I have found the optimal (without overcomplicating the problem) data structure for my problem. There was at least minor idiocy on my part for not recognising this earlier. The data doesn't need to be accessed by (x,y,z) but instead by (x, y, range of z (say 0 - 3)). This give a C++ struct as follows:

struct node {
  struct node *next;
  int zGroup;
  int z;
  50 bytes of misc data };


I can then address this through a 3D dynamic array (vectors):

vector > > Data;


Any given Data[x][y][zGroup] points to the first element of a linked list, the entirety of which is needed every time one element of it is needed. No value of this array is NULL, every one contains a linked-list of at least one element. 

The third dimension of the array - the zGroup has jagged dimensions, but with dynamic arrays this isn't an issue. Given the data and computations being performed on it, I know that the max x and y values are set when the file is read and do not change, neither does the number of z groups on any given (x,y) line, the actual z-values of nodes may change, but they will remain inside the same z-groups, giving a constant-sized, fully populated array.

With the way that the file is structured it is also easy enough to page it in and out of memory if I am brought to do this with much larger data sets.


[Answer]: I don't fully follow what's going from your answer, but a couple of comments:
 1. You should use contiguous memory. At the very least, at the outer level (for the x-y coordinates) you should have a single vector, not a vector of vectors. A vector is basically a pointer to a contiguous block of memory. A vector of vectors is a contiguous block of pointers, to contiguous blocks of memory, i.e. the data itself is not contiguous. Contiguity is incredibly important to speed due to factors like cache missing and pre-fetching.
 2. Linked lists are slow. The working of modern processors tends to give vectors a very large practical advantage. Linked lists are especially terrible for small types, for several reasons. First off, if the types are small the extra space costs of the pointers is huge. Second, when the types are small and in contiguous storage, the processor can read them very efficiently because processors generally read from memory an entire cache line at a time, which is 64 bytes. If your data type is just a double, that means that 8 doubles are read simultaneously by the processor. When you use linked lists you give this up.

The bottom line: you want a loop over your data to read the data precisely sequentially from memory in one contiguous chunk. This is the model that will give the best performance. 

Edit: Ok, I reread your requirements more carefully. Here's what I think is best, however let me preface this: Alexandrescu has a great article on performance where he notes that we have made our processors better and more intelligent, and the cost of a simple mental model of how it works. So it's very hard nowadays to predict the speed of code compared to before. So profiling is more important than ever. So if you really care about speed, profile! (btw, link is https://www.facebook.com/notes/facebook-engineering/three-optimization-tips-for-c/10151361643253920). 

Given that your x and y locations will never change, only z, you can create a struct as you suggested that stores x, y, z and misc data. Take all the initial data points, and put them in a single std::vector of these structs, call this variable DataPoints. Sort them by x, then y, the z sorting is optional. If you are doing more operations that change z, maintaining sort order will be expensive and its easier just to loop through all z's and do an if check. If you usually just scan the data, then keep sort order by z, and you will have to pay O(N) to keep it sorted when modifying the z coordinate, where N is the number of points with the same x and y coords.

Now, we are going to build a lookup table. We create another array, a single contiguous piece of memory, that you do lookups on given an x and y coordinate. This table will return a pair of integers, telling you the first and last index in DataPoints that have that exact x and y. Now, there's just one more clever bit. Since we have this lookup table, we don't actually need the x and y coordinates stored in DataPoints at all. So we create a new DataPointsReduced; it's the same data in the same order, but with the x data stripped. We could strip the y data too, but that may or may not pay off as you'll see.

We're now basically done. When you do the loop over x and y, you can extract all the indices you need from the lookup table. Sadly because you do a range of x and range of y, the resulting data of interest can't be fully contiguous. But the scan should be contiguous over a fixed value of x. The outer loop is x, for that fixed x, since the secondary sorting is y, the range of y's of interest all contain data that is contiguous in memory. So there are only two loops, not three. The inner loop goes over a block of memory containing all z's for a range of y's, for a fixed x. Inside this inner loop you do whatever operations you want. Note that if we strip y too and we need it, we will need to return to three loops, or do some kind of lookup to determine y at each stage, which can be slower. If you don't need y at all once the range of y's is specified, then strip y from your data as well.

That's basically it. If your range of x's and y's gets very large, this lookup table will eventually use a huge amount of space (but note: no more space than your solution, this lookup table could probably store 2 32 bit unsigned integers for every x-y pair, whereas your table would store at least one 64 bit pointer for every x-y pair). You can avoid the lookup table at small cost. Since we've sorted by x and y, you can have a small prestep where you do some clever binary searching to quickly find the indices you need, and then proceed as before. The cost of this prestep is less than linear in the total amount of data being scanned, so asymptotically it doesn't affect running time. However in this case you obviously can't strip the x or y data. This shouldn't make a huge difference though, if the number of points in the contiguous y range is reasonable prefetching should nullify this difference.

This setup (with no lookup table) is pretty damn good if the ranges of x and y are huge, and your data is progressively more sparse. Note that this solution does not have any space requirement proportional to the ranges of x and y. Whereas even dynamically allocated arrays or linked lists at every x-y pair implies at least one pointer per x-y pair, which is space proportional to the ranges of x and y. 

Final thought. This setup has potentially more if branching, because I basically recommend just looping through a contiguous block of memory and doing whatever checks you need to on z, instead of looping over the exact known range of z. It depends exactly what your code is doing, but for very simple if operations (e.g. 1-2 lines of code, no side effects) often the cost of branching is virtually nil: the processor can compute the if condition and both branches simultaneously 
on 3 different pipelines, and then move the appropriate result back.

Final final thought: profile! What I wrote here is based on moderate experience, but honestly it is fully possible that my answer will not work out the way I think it will for any number of reasons.

Good luck.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:265077
Title: Is there ever a reason to use an array when lists are available?
Body: It seems like List in C# can do everything an array can do and more, and seems also just as efficient in memory and performance as an array.  

So why would I ever want to use an array?

I'm obviously not asking about cases where an API or other external constraint (i.e. the Main function) requires me to use an array... I'm only asking about creating new data structures in my own code.


[Answer]: This actually goes for other languages which have lists as well (such as Java or Visual Basic). There are cases where you need to use an array because a method returns an array instead of a List. 

In an actual program, I don't think an array will be used very often, but sometimes you know the data will be a fixed size and you like the small performance gain you get from using an array. Micro-optimisation would be a valid reason, just as a method returning a list, or the need for a multidimensional datastructure. 


[Answer]: 
  So why would I ever want to use an array?


Rarely, you will have a scenario where you know that you need a fixed number of elements. From a design perspective, this should be avoided. If you need 3 things, the nature of business means that you'll very often need 4 in the next release.

Still, when this rare scenario actually occurs, using an array to enforce that fixed size invariant is useful. It provides a signal to other programmers that it is a fixed size, and helps prevent misuse where someone adds or removes an element - breaking expectations elsewhere in code.


[Answer]: Legacy compatibility.

All form personal experience:

Legacy programmers - my colleague uses arrays everywhere, has done for 30+ years, good luck changing his mind with your new fangled ideas.

Legacy code - foo(array bar[]) sure you can use a list/vector/collection toarray function but if you not using any of there additional features its easier to use an array to begin with, often more readable without the type switching.

Legacy boss - my boss was a good programmer before she went into management many years ago and still thinks she's up to date, "were using arrays" can end a meeting, explaining what a collection is can cost everyone lunch.


[Answer]: In addition to reasons listed in other answers, array literal takes fewer characters to declare:

var array = new [] { "A", "B", "C" };
var list = new List() { "A", "B", "C" };


Using array instead of List makes the code a bit shorter and just a bit more readable in cases when (1) you  need to pass any IEnumerable literal, or (2) where other functionality of List doesn't matter and you need to use some list-like literal. 

I've done this occasionally in unit tests.


[Answer]: You need arrays to manage your collection of mutable structs, of course, and what would we do without those.

struct EvilMutableStruct { public double X; } // don't do this

EvilMutableStruct[] myArray = new EvilMutableStruct[1];
myArray[0] = new EvilMutableStruct()
myArray[0].X = 1; // works, this modifies the original struct

List myList = new List();
myList.Add(new EvilMutableStruct());
myList[0].X = 1; // does not work, the List will return a *copy* of the struct


(note that there may be some cases where an array of mutable struct is desireable, but usually this differing behavior of mutable structs within arrays versus other collections is a source of errors that should be avoided)



More seriously, you need an array if you want to pass an element by reference. i.e.

Interlocked.Increment(ref myArray[i]);  // works
Interlocked.Increment(ref myList[i]);   // does not work, you can't pass a property by reference


That can be useful for lock-free threadsafe code.



You need an array if you quickly and efficiently want to initialize your fixed-size collection with the default value.

double[] myArray = new double[1000]; // contains 1000 '0' values
                                     // without further initialisation

List myList = new List(1000) // internally contains 1000 '0' values, 
                                             // since List uses an array as backing storage, 
                                             // but you cannot access those
for (int i =0; i

(note that it would be possible to implement a constructor for List that does the same, it's just that c# does not offer this feature)



you need an array if you want to efficiently copy parts of the collection

Array.Copy(array1, index1, array2, index2, length) // can't get any faster than this

double[,] array2d = new double[10,100];
double[] arraySerialized = new double[10*100];
Array.Copy(array2d, 0, arraySerialized, 0, arraySerialized.Length);
// even works for different dimensions


(again, this is something that could be implemented for List as well, but this feature does not exist in c#)


[Answer]: The same reason I don't drive a truck when going to work. I don't use something that I won't use the features of.

First of all an array is a primitive construct so an array is faster and more efficient than a List<> for sure, so your argument is not true. Array is also available everywhere and known by developers using different languages and platforms.

The most important reason I use an array instead of a List<> is to imply that the data is fixed length. If I won't add or remove any items from that data collection, I want to make sure that the type reflects that.

Another thing is let's say you are implementing a new data structure and you've read some papers about it. Now while implementing specific algorithms, you can't always rely on someone else's implementation of a type that is general purpose. It changes from .NET to Mono and even between different versions of the framework.

And it is sometimes easier to port a piece of code that uses an array instead of a framework dependent type.


[Answer]: This is strictly from an OO perspective. 

While I can't think of a reason to pass just an array around, I can certainly see situations where an array representation internal to the class is probably the best choice.

While there are other options that give similar characteristics, none seem as intuitive as an array for problems dealing with processing permutations, nested for loops, matrix representation, bitmaps and data interleaving algorithms. 

There's a substantial number of scientific fields that rely on matrix math extensively. (e.g. image processing, data error correction, digital signal processing, a ream of applied mathematics problems). Most of the algorithms in those fields are written in terms of using multidimensional arrays/matrixes. So it would be more natural to implement the algorithms as they are defined rather than make them more "software" friendly at the expense of losing the direct tie-ins to the papers the algorithms are based upon.

Like I said, in these cases you can probably get away with using lists but that adds yet another layer of complexity on top of what are already complex algorithms.


[Answer]: Well, I found a use for arrays in a game I have been writing. I used it for creating an inventory system with a fixed number of slots. This had several benefits:


I knew exactly how many open inventory slots the game object had by looking and seeing which slots were null.
I knew exactly what index each item was in.
It was still a "typed" array (Item[] Inventory), so I could add/remove the objects of type "Item".


I figured that if I ever needed to "increase" the size of the inventory, I could do so by transferring the old items into the new array, but since the inventory was fixed by screen space and I had no need to dynamically make it larger/smaller, it worked well for the purpose I was using it for.


[Answer]: If you're traversing all elements of a list, then no, an array is not necessary, 'next' or arbitrary 'selection without replacement' will do fine.

But if your algorithm needs random access to the elements in the collection, then, yes, an array is necessary.

This is somewhat analogous to "is goto necessary?". In a reasonable modern language it is not needed at all. But if you peel away the abstractions, at some point, that's all that is actually available to you, that is, the only way to implement these abstractions is with the  'unnecessary' feature. (Of course, the analogy isn't perfect, I don't think anyone says that arrays are poor programming practice; they are easy to understand and think about).


[Answer]: 1)  There is no multi-dimensional version of List.  If your data has more than one dimension it will be very inefficient to use lists.

2)  When you are dealing with a large number of small data types (say, a map where all you have is one byte for the terrain type) there can be considerable performance differences due to caching.  The array version loads several items per memory read, the list version loads only one.  Furthermore, the array version holds several times as many cells in the cache as the list version--if you're repeatedly processing the data this can make a big difference if the array version fits in cache but the list version does not.

For an extreme case, consider Minecraft.  (Yeah, it's not written in C#.  The same reason applies.)


[Answer]: A 100-element array of some type T encapsulates 100 independent variables of type T.  If T happens to be a value type which has a mutable public field of type Q and one of type R, then each element of the array will encapsulate independent variables of types Q and R.  The array as a whole will thus encapsulate 100 independent variables of type Q and 100 independent variables of type R; any of those variables may be accessed individually without affecting any other.  No collection type other than arrays can allow the fields of structures to be used as independent variables.

If T happened instead to be a class type with public mutable fields of type Q and R, each element of the array holds the only reference, anywhere in the universe, to an instance of T, and if none of the elements of the array will ever be modified to identify an object to which any outside reference exists, then the array will effectively encapsulate 100 independent variables of type Q and 100 independent variables of type R.  Other collection types can mimic the behavior of such an array, but if the only purpose of the array is to encapsulate 100 variables of type Q and 100 of type R, encapsulating each pair of variables in its own class object is an expensive way of doing that. Further, using an array or collection of mutable class type creates the possibility that the variables identified by array elements may not be independent.

If a type is supposed to behave like some kind of object, then it should either be a class type or a private-field structure which provides no means of mutation other than replacement.  If, however, a type is supposed to behave like a bunch of related-but-independent variables stuck together with duct tape, then one should use a type which is a bunch of variables stuck together with duct tape--an exposed-field struct.  Arrays of such types are very efficient to work with, and have very clean semantics.  Use of any other type will lead to muddled semantics, inferior performance, or both.


[Answer]: One important different is memory allocation. For example, traversing a linked list could result in a lot of cache misses and slower performance, whereas an array represents a contiguous chunk of memory holding multiple instances of some particular data type, and do traversing it in order is more likely to hit the CPU cache.

Of course, an array of object references may not benefit as much from cache hits, since dereferencing could still take you anywhere in memory.

Then there are list implementations such as ArrayList, which implement a list using an array. They're useful primitives to have.


[Answer]: Your question has actually already been answered before.


  and seems also just as efficient in memory and performance as an array. 


It isn't. From the question I linked:

List/foreach:  3054ms (589725196)
Array/foreach: 1860ms (589725196)


Arrays are twice as fast in certain important cases. I am certain the memory usage also differs non-trivially.

Since the main premise of your question is thus defeated, I'm assuming this answers your question. In addition to this, sometimes arrays are forced on you by Win32 API, or your GPU's shader, or some other non-DotNet library.

Even within DotNet, some methods consume and/or return arrays (such as String.Split). Which means either you must now eat the cost of calling ToList and ToArray all the time, or you must conform and use array, possibly continuing the cycle by propagating this to poor downstream users of your code.

More questions and answers on Stack Overflow on this topic:


Array versus List: When to use which?
Why do we use arrays instead of other data structures?
Which is better to use array or List<>?



[Answer]: Here's some guidance you can use on when to select Array and when to select List.


Use Array when returning from a method.
Use List as the variable when you are constructing the return value (inside the method). Then use .ToArray() when returning from the method.


In general, use an Array when you don't intend the consumer to add items to the collection. Use List when you intend the consumer to add items to the collection.

Array is meant for dealing with "static" collections while List is meant for dealing with "dynamic" collections.


------------------------------------------------------------

ID: cs.stackexchange.com:151505
Title: Advantages of linked lists over arrays?
Body: Are there other advantages to linked lists over arrays other than constant time removals of the first element?


[Answer]: Linked lists can work well even if memory is fragmented. Arrays usually require a continuous piece of memory. For large piece of data finding this large continuous piece of memory might be hard.
Most of benefits of linked lists requires to remember more links than just the head. For example. If you are doing some sequential checks and deletions or insertions, you can keep this link of the last checked element. Then all of this insertions or deletions are O(1), you have all the data needed, and memory required for insert is small. In other words linked lists are good for local operations. Like text. If you would try to implement a text editor and rewrite the whole array when user adds another letter in the middle, it will be slower than with a linked list, if linked list remembers last edit points and most edits are a bit further than it. Upgrade to this idea are skip list and rope.
Another useful feature of links is atomicity of small edits. With an array you could achieve atomicity by replacing the link to the array itself, but only after making a whole new copy. But with linked list you can build another subchain, and then replace the link to it, changing the small portion of the linked list this way, keeping all the data consistent and not spending lots of memory on another copy.


[Answer]: This isn't really about "advantages", but appropriateness. Sometimes it's more appropriate to use an array, and sometimes to use a linked list.
If it helps, take a look at this large table of the complexity of various operations of a bunch of data structures from the C++ standard library so you can compare them. If you're not familiar with C++, vector is a growable array, deque is a double-ended queue (essentially a chunked array), and list is a linked list. What you can see is that the main tradeoff is:

Dereferencing an element of an array, from a position index, is $O(1)$ operation, and linked lists don't really support that.
Insertion and deletion of an arbitrary element of a linked list is $O(1)$, and $O(n)$ for an array.



[Answer]: I don't have an exhaustive list (pun intended), but off the top of my head:

You can also perform constant time operation at the tail (for a circular doubly linked list).
You can implement the move-to-front heuristic (MTF)  efficiently.
No need to perform costly expansion/contraction (there is a way to prevent this in an array but requires extra memory)



[Answer]: If you make the elements stored in the list already contain the next/prev pointers (called an intrinsic or intrusive linked list) than the list doesn't need to do allocations and risk fragmenting memory further just to store 3-pointer sized structs or an array of pointer that needs reshuffling every time you pull out an element.
This is very handy if you expect the elements to move lists frequently and/or a possibly failing allocation means a panic. Like for example the thread descriptor object moving between various wait queues in the kernel depending on why the thread was interrupted.
With an intrinsic list which is doubly linked then if you can acquire a pointer to the object directly then you can pull the object out of the list without needing to find it in the list or array.


[Answer]: A linked list is appropriate for many use cases independent of performance considerations. In addition to examples given by other answers here, consider that a given object (an instance) can be in more than one linked list (internally linked as well as externally linked) but can only be in one array (it can be pointed to by multiple arrays, but that's not the same, for many use cases).
Consider these diagrams from the book The Design Of An Optimizing Compiler (Wulf, Johnsson, Weinstock, Hobbs) (1973) - first is the multiply-internally-linked symbol table, second is the multiply-internally-linked "graph table" used for recognizing common subexpressions - the "threads" are multiple semantically different, semantically important traversals of the same data structure implemented by separate linked lists into the same objects each representing a symbol or a syntax tree node.


BTW, IMO it is a design mistake to consider performance of a data structure before considering appropriateness.  But I can see that it frequently happens, probably because of the way CS/programming is taught, in many cases by teachers and textbooks, in which performance characteristics are prioritized over appropriateness, and in fact, appropriateness is given short shrift.  See for example many industry "interview questions" which are considered properly answered by discussions of O-bounded performance, where the data structure to use is just assumed to be "obvious".  Or in many many examples in textbooks, websites, and language libraries that have good support for external data structures (linked lists, arrays, dictionaries, hash tables) and never mention at all much less support internally linked data structures as used above. (Note that as in the diagrams above hash tables and trees can be represented using internal links if that is appropriate - it's just never done anymore not because it is wrong by mainly because there's no language or library support.)


[Answer]: An important consideration missing from the other answers: It's easier and more performant to use immutable linked lists than immutable arrays, which is especially important in functional programming languages.
For example, to add an element to an array in an immutable language, you actually create a new array with the extra element added to it, which entails copying the entire array. The compiler might be able to optimize this out (it might detect that the original array is never read again, and just append in-place), but not in general.
Adding an element to an immutable linked list is easy, as long as you're willing to add to the front: Create the new head node, and link it to the old head. The old head still represents the old list, and the new head represents the new list!
This implies another benefit of linked lists: Two lists that differ only in the first few elements can share most of their memory.


[Answer]: Donald Knuth’s “dancing links” algorithm is a well-known one that uses the constant-time insertion and deletion of items in a linked list for performance.  You can hear him discuss it, twenty-four years later, here.
An important optimization for linked lists, in a lazy functional language with immutable linked lists, is to treat them just as a sequence and avoid ever actually creating the list itself, compiling to something much like yield return in C#.


[Answer]: If you mean fixed arrays (whose lengths are fixed in advance) versus linked lists, then linked lists have the advantage that they use up memory only on an as needed basis - a new cell is allocated each time a new insert operation is done.  Whereas, arrays would require you to specify the maximum length of the list in advance, and any unused space is wasted, and you can’t insert more elements than this maximum length even if you encounter more data. So, if you don’t know in advance how many elements will be inserted into the list, a linked list would be a better data structure to use.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:128520
Title: What are concrete rules for using a linked list instead of an array?
Body: A linked list can be used when you want cheap insertion and deletion of elements and when it doesn't matter that the elements aren't next to each other in memory.

This is very abstract and I would like a concrete explanation of why a linked list should be used rather than an array. I'm not very experienced with programming, so I haven't got much (if any) real-world experience.


[Answer]: A linked list can be used to implement a message queue.

A message queue is a structure in which we store information about events for later processing. For example, when the user presses a key or moves the mouse, this is an event. An application might be busy at the moment when the event occurs, so it cannot be expected to process the event at the precise moment when it occurs. So, the event is placed in a message queue, (information about which key was pressed, or where the mouse has moved,) and when the application has some time to spare, it checks its message queue, fetches events from it, and process them. (This happens within a time frame of milliseconds, so it is not noticeable.) 

From the usage scenario that I just described, it should be obvious that we never care to have random access to events stored in the message queue; we only care to be able to store messages in it, and retrieve them. So, it makes sense to use a linked list, which provides optimal insertion/removal time.

(Please do not butt in to point out that a message queue is likely, or more likely, or almost as likely, to be implemented using a circular array-list; that's a technical detail, and it has a limitation: you can only store a limited number of messages in it.)


[Answer]: 
Hashtables that use chaining to resolve hash collisions typically have one linked list per bucket for the elements in that bucket.
Simple memory allocators use a free list of unused memory regions, basically a linked list with the list pointer inside the free memory itself
In a FAT filesystem, the metadata of a large file is organized as a linked list of FAT entries.



[Answer]: The C-language "call stack" is implemented as a linked list in the x86 (and most other) binary APIs.

That is, C-language procedure calling follows a first-in, last-out discipline. The result of executing (possibly recursive) function calls gets referred to as a "call stack", or sometimes even just "the stack".

The CALL x86 instruction ends up implmenting a linked list using the "call stack". A CALL instruction pushes the contents of %EIP register, the address of the instruction after the CALL onto stack memory. The called-fuction prolog pushes the contents of the %EBP register, the lowest address of local variables in the calling functio, onto stack memory. Then the called function prolog sets %EBP to the current function's stack base.

That means that %EBP is a pointer to a memory location that holds the address of the calling function's %EBP value.  That's nothing more than a linked list, implemented partially in hardware via CALL. 

As far as what this is good for, it's how x86 CPUs implement function calls, particularly function calls where the function has it's own copy of arguments, and variables local to the function. Every function call pushes some information on the "call stack" that allows the CPU to pick up where it left off in the calling function, without interference from the called function or the calling function.


[Answer]: Here is something part way between an example and an analogy. You have some errands to do, so you grab a piece of paper and write:


bank
groceries
drop off drycleaning


Then you remember that you also need to buy stamps. Because of the geography of your town, you need to do that after the bank. You could copy your whole list onto a new piece of paper:


bank
stamps
groceries
drop off drycleaning


or you could scribble on the one you had:


bank    .......         STAMPS
groceries
drop off drycleaning


As you thought of other errands, you might write them at the bottom of the list, but with arrows reminding yourself what order to do them in. This is a linked list. It's quicker and easier than copying the whole list around every time you add something.

Then your cell phone rings while you're at the bank "hey, I got the stamps, don't pick up any more". You just cross STAMPS off the list, you don't rewrite a whole new one without STAMPS in it.

Now you could actually implement an errands list in code (maybe an app that puts your errands in order based on your geography) and there's a reasonable chance you would actually use a linked list for that in code. You want to add and remove lots of items, order matters, but you don't want to recopy the whole list after each insertion or deletion. 


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:77772
Title: Which sorting algorithms have a different worst case complexity than their average case?
Body: I'v been working on sorting for a while now but I can't figure these two questions apart,  I'm kind of getting mixed up somewhere ... Somebody help

a) Which sorting algorithms have a different worst case complexity than their average case? 

b) Which sorting algorithms have a different best case complexity than their average case? 


[Answer]: The most obvious example would be Quicksort -- its average case is O(N log N), and worst case is O(N2). As its normally written, the best case is O(N log N), but some versions have included a pre-scan to exit early when data was sorted, which gives a best-base of O(N), though I suppose it's open to argument that it's no longer purely a Quicksort at that point.

I don't know whether it does any more, but the implementation of qsort in Microsoft's C standard library used to do that -- mostly to make up for a poor implementation (always used the first element as the pivot) that would otherwise have been O(N2) for sorted data.


[Answer]: Insertion sort is an example of a sort algorithm with a better best case than average case. Best case is O(n), average case and worst case are O(n²)


[Answer]: Here's a complete overview of the complexities.

Some of the most popular ones:


Quick sort: O(n²) in the worst case, O(n lg(n)) on average and in the best case.
Insertion sort: O(n²) in the worst case & average case, O(n) in the best case.



[Answer]: The crucial thing here is recognizing that the algorithm needs to take shortcuts to avoid having to compare each element to every other element (which would result in a O(N^2) algorithm) but only to some of the other elements and to do it again when sorting those elements.  This is the reason why "divide work in piles and handle each of those piles by dividing it in piles and handle each of those piles etc" ends up with O(N log N) complexity.

The hard part is picking those elements, and to handle working with those elements.  Picking is hard because you must choose well to get the subpiles equally sized.  Managing is hard because you cannot spend too much time moving things around if you want a fast algorithm.

For instance, QuickSort works by choosing an element (traditionally called "pivot") and divide the work in two piles.  One having elements smaller than the pivot, and the other having larger.  Then quicksort each pile, and merge the two sorted piles back.  If the pivot is the smallest element each time, you end up with an empty sub-pile and a big subpile with all the elements but the pivot.  This will result in a worst case of O(N^2).  If you choose well, you get half-sized sub-piles and a running time of O(N log N).

So a typical way to choose the pivot is to look at three randomly chosen values from the pile and choose the one in the middle.  This at least avoids the empty sub-pile.


[Answer]: Bubble Sort, O(N) in the best case, O(N ** 2) in the average and worst cases.

To get the O(N) behaviour, set a swapped boolean to false at the beginning of each pass, set it to true whenever you swap two elements. If you have swapped elements, you need to do another pass. This also works as a general termination criterion, as long as comparisons are deterministic, which means you can skip an O(N) counting pass before starting.


[Answer]: There are implementations that first check if the array starts or ends with a sequence of elements in ascending or descending order, and takes advantage if many such elements are found. So best case is linear (if the array is mostly sorted in ascending or descending order). What the average case would be can be highly debatable. 


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:124097
Title: How can a PHP programmer best prepare for a C++ course?
Body: I have never programmed in C++, but am going to be taking a class that involves programming in C++ in January. I want to be able to excel in the class, and looking for advice, as I come from a PHP background. How can I best prepare myself for success?


[Answer]: Your question is like asking, 'How can a baseball player prepare for a basketball match?'. Once a programmer learns a language, usually he/she has no problem learning a new language; same as a baseball player adapting to a basketball game.

The sportsman is fit, and the programmer has a basic understanding of language syntax and structure.

You will find several difference between C++ and PHP, yet a lot of time you will be thinking on how C++ is like PHP.

PHP is less strict than C++; C++ is what is considered a general-purpose programming language while PHP is a scripting language aimed at web development.

C++ process is different, a c++ program is converted into binary code, then loaded into memory and run directly.  

If you really want to excel, then in my book you have to actually learn it on your own. I excelled in programming courses, because I knew most of the course beforehand. I see the best and most passionate programmers learning the language on their own.

If there is a book included with the course try to get your hands on it, and read it before the start of the course at least get ahead. Most of us don't have photographic memory and we can only learn by hearing something over and over again.

Learn it on your own and the course lecture would be the review. Warning: Following this advice might cause you to fall asleep in lectures.

Again start reading tutorials and books, start building things and you won't have problem excelling in your C++ class.

Books:

The C++ Programming Language

C++ Primer

Thinking in C++: Introduction to Standard C++

The C++ Standard Library

Extra:
THe New Boston - C++


[Answer]: I find that proficiency at a given programming language is much like proficiency at any skill. While a good deal of study can help mitigate difficulties, the real "trick" is to practice1. And yes, some languages will be closer than others, just like certain disciplines will be easier to transfer than others — it is harder for a guitar player to learn viola than it is for a violinist to learn viola. The best thing you can do, then, is to focus on the major differences. And you'll definitely notice several major paradigm shifts between the two languages. 

Here are some recommendations to help bridge that gap: 


At a bare minimum, the idea of the PHP reference is very different from the C++ pointer. Considering its importance, you might want to consider studying that first. 
After that, I would recommend learning first how to compile something larger than one or two files. While your at it, make sure you know the difference between #import ; and #import "lib";
Learn the importance (and the difference) between .h and .cpp files.
Pre-processor directives/macros. Learn them and be able to write them.



1. How do you get to Carnegie Hall? Put this into Google Maps: 881 7th Ave. New York, New York.


[Answer]: C++ and PHP are worlds apart in terms of applications and usage patterns. Your PHP knowledge will help you by giving you a head start on basic programming concepts such as


Objects
Conditional Statements (If/Else)
Loops
etc...


As well as any program designing experience you may have picked up while developing PHP applications.

If you want to get the leg up in your course the best way is to start by ignoring your PHP background beyond the things outlined above and focus on some of the following:


Memory Management (Pointers, new/delete, Constructors/Destructors, RAII)
Differences between the PHP object model and the C++ object model
Basic Datastructures (Arrays, Trees, Linked Lists)
Basic Algorithms (Sorting, Searching and being able to determine the best algorithm for the job)


Another good resource is your teacher for the course. I'm sure if you send them an email they will be able to give you some resources to get started.


[Answer]: The best thing to do is to forget everything you know about PHP. It will only serve to hold you back in the C++ world. Many things which appear similar are in fact not similar at all, and it's best to just not try to apply your PHP knowledge.


[Answer]: My two cents-


Data Types: PHP is a dynamically typed language, while C++ is statically typed. So, you have to mention and keep track of data type explicitly everywhere.
Declaration: You have to declare a variable with it's type before using.
Array: The concept of an array is very much different in C++ compared to PHP. In C++, only numbers can be used as array indices. Like other variables, the array has to be declared with specific size which cannot be changed later. The index is 0-based.
String Handling: In C++(and C), strings are related with arrays and are manipulated differently.


Pay attention to C++ specific OO features, some C features like pointers and it's libraries. Good luck.


[Answer]: I took a series of Java programming classes a few years ago after working mostly with PHP for five years. In my case, I attended a community college.

The best advice I have is to stick with it. The first few weeks will undoubtedly be a little boring, but if you are inattentive, you may miss important key concepts. Don't be afraid to ask you instructor if you don't understand something and be careful not to rely too much on prior knowledge if you are unclear on what is being taught. That being said you are likely to find instruction on basic syntax redundant.

If you want to excel, read a chapter or two ahead before going to lecture and come to class armed with a question or two. A good attitude will help you learn. Also, speak with the instructor about your PHP knowledge before the class starts or after the first lecture. If enough people in your class are already programmers, your instructor may pace the class differently than if everyone is new to coding.


[Answer]: Pointers, sir

Binky will help you. 

OOP

Hoping that you are a good PHP developer and you have used classes (i.e. object oriented programming) you'll be pretty much conceptually ready to take on other concepts such as inheritance and polymorphism.


[Answer]: That's a good topic. But I keep wondering why other programmers feel offended anytime the issue of PHP comes up? Looking around the web, PHP is clearly the winner - l don't even know the importance of all these other languages, when almost 70% of the web is in the hand of PHP. You don't need to abandon your PHP background, a good programmer be it in PHP, Java or any other language can pick up any other language easily, just study the syntax and you are good to go. As a PHP programmer, it never took me long to start piloting java, but not much use.Rick Rhodes, l like your answer.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:167964
Title: When does 'optimizing code' == 'structuring data'?
Body: A recent article by ycombinator lists a comment with principles of a great programmer.  


  #7. Good programmer: I optimize code. Better programmer: I structure data. Best programmer: What's the difference?


Acknowledging subjective and contentious concepts - does anyone have a position on what this means?  I do, but I'd like to edit this question later with my thoughts so-as not to predispose the answers.


[Answer]: Nine times out of ten, when you structure your code/models well, optimization will become obvious. How many times have you seen a hornets nest and found it totally suboptimal, where upon restructuring it, lots of redundancies became extremely obvious.


  A designer knows he has achieved perfection not when there is nothing left to add, but when there is nothing left to take away. 
  - Antoine de Saint-Exupery


A well structured system will be minimal in nature, and due to it's minimal nature it will be optimized because how little there is to it relates directly to how little it does to accomplish it's goal.

Edit:
To expound upon the point other's have taken away from this, it's also completely accurate to see the statement as identifying the relation between code and data. That relation is thusly: If you change the structure of your data, you will need to change your code to respect the altered structure. If you wish to optimize your code, chances are you will need to change the structure of your data to make your code capable of handling the data more optimally.

That said, there is a totally separate possibility that was being eluded to here, and that would be that this fellow having relations with YCombinator may be referring to code AS data in the LISP tradition of homoiconicity. It's a stretch to surmise this as the meaning in my mind, but it is YCombinator so I wouldn't rule out that the quote is simply saying LISPers are the "Best Programmer"s.


[Answer]: I think the author is hinting that any restructuring of the data leads to code restructuring. Therefore, restructuring the data with the goal of optimizing your system will force you to optimize your code as well, prompting the "what's the difference?" response.

Note that an "uber-excellent programmer" may reply to "what's the difference?" that there is some difference left in there: once you venture into optimizing for improved use of the CPU cache, you may keep the layout of your data structures the same, but change the order in which you access them can make a great deal of a difference.


[Answer]: I don't agree with the statement mentioned above, well at least without explanation. I see coding is the activity involving the utilization of some data structures. Data structures would generally influence coding. So there is a difference between the two in my opinion.

I think the author should have written the last part as "Best programmer: I optimize both."

There is a great book (at least it was in when published) called: Algorithms+Data Structures = Programs.


[Answer]: Consider the most obvious example of this - "searching for user data is too slow!"

If your user data is not indexed or at least sorted, then restructuring your data will quickly yield increased code performance. If the data is structured properly and you're just iterating through the collection (rather than using the indexes or doing something like a binary search) then modifying the code yields increased code performance.

Programmers are problem solvers. While it is useful to distinguish between algorithms and data structures, they cannot often exist in isolation. The best programmers know this, and don't isolate themselves unnecessarily.


[Answer]: Optimizing code can sometimes improve speed by a factor of two, and occasionally by a factor of ten or even twenty, but that's about it.  That may sound like a lot, and if a 75% of a program's execution time is spent in a five-line routine whose speed easily could be doubled, such an optimization may well be worth making.  On the other hand, one's selection of data structures may affect execution speed by many orders of magnitude.  A modern hyper-optimized multi-threaded processor running super-optimized code to look up data by key in a 10,000,000-item linear linked list stored in RAM would be slower than a much slower processor running a rather simply-coded nested hash table.  Indeed, if one had the data laid out properly, even a 1980's computer fetching data from a hard drive might beat the modern CPU using the inferior data structure.

That having been said, designing efficient data structures often requires more complex trade-offs than optimizing code.  For example, in many cases the data structures which allow data to be accessed most efficiently are less efficient to update (sometimes by orders of magnitude) than those which allow fast updates, and those which allow the fastest updates may allow the slowest access.  Further, in many cases, data structures which are optimal for large data sets may be comparatively inefficient with small ones.  A good programmer should strive to balance those competing factors with the amount of programmer time required to implement and maintain various data structures, and be able to strike a decent balance among them.


[Answer]: 
  Best programmer: What's the difference?


Best programmer? No. Lousy programmer. I'm assuming the word "optimization" means those things that programmers typically try to optimize, memory or CPU time. In this sense, optimization goes against the grain of almost every other software metric. Understandability, maintainability, testability, etc.: These all take short shrift when optimization is the goal -- unless what one is trying to optimize is human understandability, maintainability, testability, etc. Not to mention cost. Writing an speed / space optimal algorithm costs considerably more in terms of developer time than does naively coding the algorithm as presented in some text or journal. A lousy programmer doesn't know the difference. A good one does. The best programmer knows how to determine exactly what needs to be optimized and does so judiciously.


[Answer]: Data structures drive a lot of things relative to performance.  I think that we can look at problems hard and long with a preconceived idea about the ideal data structure, and in this context of thinking, even create proofs (often by induction) of optimality.  For example, if we put a sorted list into an array and evaluate things like the cost to insert an element we might decide on average we need to shift 1/2 of the array for each insertion.  For each binary search, we can find a matching item (or not) in log n steps.  

Alternatively, if we defer our decision about data structure (avoid premature optimization) and study the data coming in and the context where we will use it, how big it is, what latencies occur and which ones matter to users, how much memory we have vs. would use with data representations we know or can devise.  

In an area like sorting and searching, there is a lot to know.  Truly great programmers have been working on this a long time.  Understanding these problems well is useful, and it is a great thing if you know more methods than when you finished undergrad data structures class.  Binary trees can provide superior performance for insertions in exchange for higher memory use.  Hash tables provide even bigger improvements, but for more memory still.  A radix tree and radix sort can carry improvements even further.

Creative structuring of the data can help reframe a problem and open the door to new algorithms that make hard applications faster and sometimes impossible tasks possible.


[Answer]: To articulate my best guess at what the article means, I'll assume an unspoken subtext (which seems to be missing in the article) that any programmer should understand about optimization:


optimization comes only after you've got the program up and running correctly:

make it run correctly, then make it run fast
this principle is the point of Knuth's maxim, "premature optimization is the root of all evil"

if and when you've determined that optimization is not premature, you must measure it properly first to determine what actually needs optimizing, and again and again during optimization, to tell what effects your attempts at optimization are having.

if your code runs in development, the profiler is your friend in this.
if your code runs in production, you must instrument your code, and make friends with your logging system instead.





Now, then: your measurements will tell you where in your code the machine is burning the most cycles.  A "good" programmer will focus on optimizing those parts of the code, rather than wasting time optimizing the irrelevant parts.

However, you can often make larger gains by looking at the system as a whole, and finding some way to allow the machine to do less work.  Frequently, these changes require reworking the organization of your data; thus, a "better" programmer will find himself structuring data more often than not.

The "best programmer" will have a thorough mental model of how the machine works, a good grounding in algorithm design, and a practical understanding of how they interact.  This allows him to consider the system as an integrated whole -- he will see no difference between optimizing the code and the data, because he evaluates them at an architectural level.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:65139
Title: Should data structures be integrated into the language (as in Python) or be provided in the standard library (as in Java)?
Body: In Python, and most likely many other programming languages, common data structures can be found as an integrated part of the core language with their own dedicated syntax. If we put LISP's integrated list syntax aside, I can't think of any other languages that I know which provides some kind of data structure above the array as an integrated part of their syntax, though all of them (but C, I guess) seem to provide them in the standard library.

From a language design perspective, what are your opinions on having a specific syntax for data structures in the core language? Is it a good idea, and does the purpose of the language (etc.) change how good this could be of a choice?

Edit: I'm sorry for (apparently) causing some confusion about which data structures I mean. I talk about the basic and commonly used ones, but still not the most basic ones. This excludes trees (too complex, uncommon), stacks (too seldom used), arrays (too simple) but includes e.g. sets, lists and hashmaps.


[Answer]: Perl has hashmaps and PL/SQL supports records, and I have very foggy memories of matlab having syntax to support vectors and matrices of all different dimensions (though I could be wrong about this one and it could be argues that these are data types not data structures)... I would say that having some native support for very common structures is nice to have. Usually it seems that arrays and hashmaps/associative arrays are the most common natively supported structures, and they are probably the most commonly used as well.

Don't forget that if you add native syntax support for other structures such as binary-trees, those structures also have be implemented by the language's supporting tools (compiler/runtime/etc). How many strucutres do you want to build support for?

You'll have to invent new notation for the less commonly natively supported structures... Keep It Simple!.


[Answer]: In my opinion, it's an amazingly simple addition which can come handy surprisingly often, at least if done with caution - i.e. at most for tuples, lists, maps and sets as those have well-recognized literals.


It's cheap to add to a language. It doesn't cost you much of that precious complexity budget:

the grammar is basically someBracket {expr ','} someBracket or someBracket {expr ':' expr ','} someBracket, with some dead simple extras if you want things like optional trailing commas. The float literals can easily be longer in the grammar.
In many languages, none of the popular literals clash with existing syntax (an exception I can think of is a language with brace-like blocks as expressions, a comma operator and no semicolon, as in {1, 2})
The semantics can be defined in less than five sentences, the informal version being "Instantiate a new $collection, then call .add/.append/.setItem once per given expressions with that (those) expression(s) as arguments".

Due to the previous third point, it's also very easy to implement.
It comes in incredibly handy when you need one, and doesn't (need to) impact the syntax of other elements, i.e. you don't "pay" for it when you don't use it.



[Answer]: APL (and related modern variants, A+, J and K) have scalar, vector and matrix as first-class data structures.

Yes, they can be deprecated as mere variants on array.  But they're also free from complex declarations and don't come from a separate library, they feel like complex data structures that are a first-class part of the language.


[Answer]: My favourite example here is Lua. Lua has only one built-in data type, the "table", but its flexibility and speed mean you actually use them in place of regular arrays, linked lists, queues, maps and they're even the basis for Lua's object-oriented features (i.e. classes).

Lua is such an amazingly simple language, but the flexibility of the table data structure makes it quite powerful as well.


[Answer]: Clojure is a lisp but supports 

Lists: (x1 x2)
Vectors: [x1 x2]
Maps: {k1 v1 k2 v2}
Sets: #{x1 x2}



[Answer]: The more data structures you have in the language itself the more difficult the language would be to learn. It might be a personal preference but I tend to prefer a simpler language and then any extras can be supplied by libraries.

Languages designed for specific fields can sometimes benefit from having certain data structures built-in to the language such as Matlab. But too many can overwhelm you.


[Answer]: It depends what the language is for.

Some examples (somewhat stolen from other answers):


Perl has special syntax for hashtables, arrays, strings.  Perl is often used for scripting, these are useful for scripting.
Matlab has special syntax for lists, matrices, structures.  Matlab is for doing matrix and vectorial mathematics for engineering.
Java/.NET support string and arrays.  These are general purpose languages where arrays and strings are often used (less and less with use of new collection classes)
C/C++ support arrays.  These are languages that do not hide hardware from you.  Strings are partially supported (no concatenation, use strcpy, etc.)


I think it depends what the purpose/spirit/audience of your language is; how abstract and how far away from hardware you want it to be.  Generally the languages that support lists as primitives allow you to create infinitely long lists.  While low level such as C/C++ would never have these, because that is not the goal, the spirit of those languages.

To me, garbage collection follows the same logic: does the audience of your language care about knowing exactly when and if memory is being allocated or freed?  If yes, malloc/free; if no, then garbage collection.


[Answer]: 
  From a language design perspective, what are your opinions on having a specific syntax for data structures in the core language?  Is it a good idea, and does the purpose of the language (etc.) change how good this could be of a choice?


List and map literals and a convenient closure syntax are essential features of high-level languages.  

The difference between this Java code:

Thing t = new Thing();
t.setFoo(3);
t.setBar(6.3);
t.setBaz(true);


and this Groovy code:

t = new Thing(foo: 3, bar: 6.3, baz: true)


is enormous.  It's the difference between a 40,000 line program and a 10,000 line program.  Syntax matters.


[Answer]: For a language to be really useful, it has to do a certain degree of tasks out of the box. Because practical day to day programming requires tools that solve their problems at some generic level. Minimalism looks compact and cool but when you want to start using to solve big but repeated problems, you need a level of abstraction on top of which you can build upon.

So I think programming languages should ship support for most commonly used data structures in the syntax for the tasks the language is designed for.


[Answer]: You don't have to have dedicated syntax for every high-level data type.  For example, it's tolerable to have set([1, 2, 3]) (as Python 2.x did) instead of {1, 2, 3}.

The important thing is to have some convenient way to construct a high-level data structure.  What you want to avoid is code like:

s = set()
s.add(1)
s.add(2)
s.add(3)


which annoys me greatly when I use std::vector, std::set, and std::map in C++.  Thankfully, the new standard will have std::initializer_list.


[Answer]: Sure it depends on the application of the programming language, but for higher level languages it should be as convenient as possible to work with any common data structure. Have a look at the list of abstract data types in Wikipedia for examples. I found the following basic principles most common (but I'd like to hear other opinions too):


ordered sequences (1-dimensional): array, queue, stack, lists...
ordered multi-dimensional structures: table, vector, matrice..
maps: hashmap, dictionary, set, multimap... (1-dimensional) 
multi-dimensional maps: functions, maps of maps...
graph types: trees, directed graphs...


You can emulate any structure with any other structure - it only depends on how easy and clear the programming language allows it. For instance:


queue and stack are easy to emulate with arrays or lists, of the latter provide operations like push, pop, shift etc.
ordered sequences can be emulated with maps that have numeric keys
sets can be emulated by maps that map values to a boolean
most graph types can be emulated by nesting sequences or maps
functions can be used to emulate maps if you can easily modify their definition


Most languages provide at least one type for ordered sequences, one for 1-dimensional maps, and one for multi-dimensional maps, limited to functions. Personally, I often miss sets and ordered multi-dimensional structures in languages like Perl, PHP, JavaScript, Lua... because emulating them is not convenient enough.


[Answer]: I think it is a bad idea to have too many privileged data types that get special syntax. This complicates the language syntax needlessly, making code harder to read, making it harder for beginners to learn and making it harder to develop tools for the language.

It's OK to make an exception for a small number of very common data structure types. I'd probably allow at a maximum:


Fixed-length arrays
Sets
Hashmaps
Sequences / lists
Records / structs / classes


Anything more sophisticated than that should probably be left to libraries to handle, using the language's normal syntax for custom data types.

In particular, things like Red/Black trees, Priority Queues etc. have quite a lot of possible implementation options, so it isn't wise to bake a particular implementation into the core language. It's better to let people choose the most appropriate implementation for their situation. Examples of implementation choices that I might not want a language designer to restrict my choice on:


Mutable or immutable?
Allows nulls or not?
Synchronised or not?
Backed by persistent storage or not?



[Answer]: In general I find convenient to have literals for lists, sets and so on. But it sometimes bugs me that I do not know anything about the actual implementation of - say - the Python list or the Javascript array. The only thing I can be sure of is that they expose a given interface.

I take as a benchmark of a language expressiveness how well it can write its own data structures as libraries, and how convenient is to use them.

For instance, Scala provides various collections with different implementation and performance guarantees. All of them are implemented in Scala itself, and the syntax to use them is only slightly more complex than if they were builtin and had runtime support.

The only basic structure that really needs support from the runtime itself, at least in a managed language, is the array: if you do not manage the memory, you will have a hard time getting a bunch of adjacent bytes. Every other structure can be built out of arrays and pointers (or references).


[Answer]: I believe that with most modern programming languages, having data structures defined in core or "standard library" (like asked in the question) makes little difference. What is instead very different is having them in core/standard vs separated packages.
The advantages of having good, efficient implementations in core/standard means that then all packages can reuse (and eventually extend) them for their task (like in Julia), otherwise each package will need to implement its own "version" (like numpy, tensorflow,.. in Python or the tidyverse ecosystem for R).


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:315964
Title: How to write correct loops?
Body: Most of time while writing loops I usually write wrong boundary conditions(eg: wrong outcome) or my assumptions about loop terminations are wrong(eg: infinitely running loop). Although I got my assumptions correct after some trial and error but I got too frustrated because of the lack of correct computing model in my head. 

/**
 * Inserts the given value in proper position in the sorted subarray i.e. 
 * array[0...rightIndex] is the sorted subarray, on inserting a new value 
 * our new sorted subarray becomes array[0...rightIndex+1].
 * @param array The whole array whose initial elements [0...rightIndex] are 
 * sorted.
 * @param rightIndex The index till which sub array is sorted.
 * @param value The value to be inserted into sorted sub array.
 */
function insert(array, rightIndex, value) {
    for(var j = rightIndex; j >= 0 && array[j] > value; j--) {
        array[j + 1] = array[j];
    }   
    array[j + 1] = value; 
};


The mistakes that I did initially were:


Instead of j >= 0 I kept it j > 0.
Got confused whether array[j+1] = value or array[j] = value.


What are tools/mental models to avoid such mistakes?


[Answer]: Use unit testing/TDD

If you really need to access sequences through a for loop, you can avoid the mistakes through unit testing, and especially test driven development.

Imagine you need to implement a method which takes the values which are superior to zero, in reverse order. What test cases could you think of?


A sequence contains one value which is superior to zero.

Actual: [5]. Expected: [5].

The most straightforward implementation which satisfies the requirements consists of simply returning the source sequence to the caller.
A sequence contains two values, both superior to zero.

Actual: [5, 7]. Expected: [7, 5].

Now, you cannot just return the sequence, but you should reverse it. Would you use a for (;;) loop, another language construct or a library method doesn't matter.
A sequence contains three values, one being zero.

Actual: [5, 0, 7]. Expected: [7, 5].

Now you should change the code to filter the values. Again, this could be expressed through an if statement or a call to your favorite framework method.
Depending on your algorithm (since this is white-box testing, the implementation matters), you may need to handle specifically the empty sequence [] → [] case, or maybe not. Or you may ensure that the edge case where all values are negative [-4, 0, -5, 0] → [] is handled correctly, or even that boundary negative values are: [6, 4, -1] → [4, 6]; [-1, 6, 4] → [4, 6]. In many cases, however, you'll only have the three tests described above: any additional test won't make you change your code, and so would be irrelevant.


Work at higher abstraction level

However, in many cases, you can avoid most of those errors by working at a higher abstraction level, using existent libraries/frameworks. Those libraries/frameworks make it possible to revert, sort, split and join the sequences, to insert or remove values in arrays or doubly-linked lists, etc.

Usually, foreach can be used instead of for, making boundary conditions checking irrelevant: the language does it for you. Some languages, such as Python, don't even have the for (;;) construct, but only for ... in ....

In C#, LINQ is particularly convenient when working with sequences.

var result = source.Skip(5).TakeWhile(c => c > 0);


is much more readable and less error prone compared to its for variant:

for (int i = 5; i 


[Answer]: Test
No, seriously, test.
I've been coding for over 20 years and I still don't trust myself to write a loop correctly the first time.  I write and run tests that prove it works before I suspect it works. Test each side of every boundary condition. For example a rightIndex of 0 should do what? How about -1?
Keep it simple
If others can't see what it does at a glance you're making it too hard.  Please feel free to ignore performance if it means you can write something easy to understand.  Only make it faster in the unlikely event that you really need to. And even then only once you're absolutely sure you know exactly what is slowing you down. If you can achieve an actual Big O improvement this activity may not be pointless, but even then, make your code as readable as possible.
Off by one
Know the difference between counting your fingers and counting the spaces between your fingers.  Sometimes the spaces are what is actually important.  Don't let your fingers distract you. Know whether your thumb is a finger. Know whether the gap between your pinky and thumb counts as a space.
Comments
Before you get lost in the code try to say what you mean in English.  State your expectations clearly.  Don't explain how the code works. Explain why you're having it do what it does. Keep implementation details out of it. It should be possible to refactor the code without needing to change the comment.
The best comment is a good name.
If you can say everything you need to say with a good name, DON'T say it again with a comment.
Abstractions
Objects, functions, arrays, and variables are all abstractions that are only as good as the names they are given. Give them names that ensure when people look inside them they won't be surprised by what they find.
Short names
Use short names for short lived things. i is a fine name for an index in a nice tight loop in a small scope that makes it's meaning obvious.  If i lives long enough to get spread out over line after line with other ideas and names that can be confused with i then it's time to give i a nice long explanatory name.
Long names
Never shorten a name simply due to line length considerations.  Find another way to lay out your code.
Whitespace
Defects love to hide in unreadable code.  If your language lets you choose your indentation style at least be consistent.  Don't make your code look like a stream of word wrapped noise.  Code should look like it's marching in formation.
Loop constructs
Learn and review the loop structures in your language.  Watching a debugger highlight a for(;;) loop  can be very instructive.  Learn all the forms:  while, do while, while(true), for each. Use the simplest one you can get away with.  Look up priming the pump.  Learn what break and continue do if you have them.  Know the difference between c++ and ++c.  Don't be afraid to return early as long as you always close everything that needs closing. Finally blocks or preferably something that marks it for automatic closing when you open it: Using statement / Try with Resources.
Loop alternatives
Let something else do the looping if you can. It's easier on the eyes and already debugged. These come in many forms: collections or streams that allow map(), reduce(), foreach(), and other such methods that apply a lambda. Look for specialty functions like Arrays.fill(). There is also recursion but only expect that to make things easy in special cases. Generally don't use recursion until you see what the alternative would look like. If someone tells you tail recursion will magically keep you from blowing the stack then first check if your language optimizes tail recursion. Not all of them do.
Oh, and test.
Test, test, test.
Did I mention testing?
There was one more thing.  Can't remember.  Started with a T...


[Answer]: The introduction to your question makes me think you haven't learned to code properly.  Anyone who is programming in an imperative language for more than a few weeks should really be getting their loop bounds right first-time in more than 90% of cases.  Perhaps you are rushing to start coding before you've thought through the problem sufficiently.

I suggest you correct this deficiency by (re-)learning how to write loops -- and I'd recommend a few hours working through a range of loops with paper and pencil.  Take an afternoon off to do this.  Then spend 45 min or so a day working on the topic until you really get it.

It's all very well testing, but you should be testing in the expectation that you generally get your loop bounds (and the rest of your code) right. 


[Answer]: 
  I got too frustrated because of the lack of correct computing model in my head.


Is a very interesting point to this question and it generated this comment:-


  There is only one way: understand your problem better. But that is as general as your question is. – Thomas Junk


...and Thomas is right.  Not having a clear intent for a function should be a red-flag - a clear indication that you should immediately STOP, grab a pencil and paper, step away from the IDE, and break the problem down properly; or at very least sanity-check what you have done.  

I've seen so many functions and classes that have become a complete mess because the authors have attempted to define the implementation before they have fully defined the problem.  And it's so easy to deal with.

If you don't fully understand the problem then you're also unlikely to be coding the optimal solution (either in terms of efficiency or clarity), nor are you going to be able to create genuinely useful unit tests in a TDD methodology.  

Take your code here as an example, it contains a number of potential flaws which you've not considered yet for example:-


what if rightIndex is too low?  (clue: it will involve data loss)
what if rightIndex is outside the array bounds? (will you get an exception, or did you just create yourself a buffer overflow?)


There are a few other issues related to the performance and design of the code...


will this code need to scale?  Is keeping the array sorted the best option, or should you look at other options (like a linked-list?)
can you be sure of your assumptions?  (can you guarantee the array be sorted, and what if it isn't?)
are you reinventing the wheel?  Sorted arrays are a well-known problem, have you studied the existing solutions?  Is there a solution already available in your language (such as SortedList in C#)?
should you be manually copying one array entry at a time?  or does your language provide common functions like JScript's Array.Insert(...)? would this code be clearer?


There are plenty of ways this code could be improved but until you have properly defined what you need this code to do, you're not developing code, you're just hacking it together in the hope that it will work.  Invest the time in it and your life will get easier.


[Answer]: I agree with other people who say test your code.  However, it's also nice to get it right in the first place.  I have a tendency to get boundary conditions wrong in many cases, so I've developed mental tricks to prevent such problems.

With a 0-indexed array, your normal conditions are going to be:

for (int i = 0; i 

or

for (int i = length - 1; i >= 0; i--)


Those patterns should become second nature, your shouldn't have to think about those at all.

But not everything follows that exact pattern.  So if you're not sure if you wrote it right, here's your next step:

Plug in values and evaluate the code in your own brain.  Make it as simple to think about as you possible can.  What happens if the relevant values are 0s?  What happens if they are 1s?

for(var j = rightIndex; j >= 0 && array[j] > value; j--) {
    array[j + 1] = array[j];
}   
array[j + 1] = value;


In your example, you're not sure if it should be [j] = value or [j+1] = value.  Time to start evaluating it manually:

What happens if you have an array length 0?  The answer becomes obvious: rightIndex must be (length - 1) == -1, so j starts at -1, so to insert at index 0, you need to add 1.

So we've proved the final condition correct, but not the inside of the loop.

What happens if you have an array with 1 element, 10, and we try to insert a 5?  With a single element, rightIndex should start at 0.  So the first time through the loop, j = 0, so "0 >= 0 && 10 > 5".  Since we want to insert the 5 at index 0, the 10 should get moved to index 1, so array[1] = array[0].  Since this happens when j is 0, array[j + 1] = array[j + 0].

If you try to imagine some large array and what happens inserting into some arbitrary location, your brain will probably get overwhelmed.  But if you stick to simple 0/1/2 size examples, it should be easy to do a quick mental run through and see where your boundary conditions break down.

Imagine you never heard of the fence post problem before and I tell you I have 100 fence posts in a straight line, how many segments between them.  If you try to imagine 100 fence posts in your head, you're just going to get overwhelmed.  So what's the fewest fence posts to make a valid fence?  You need 2 to make a fence, so imagine 2 posts, and the mental image of a single segment between the posts makes it very clear.  You don't have to sit there counting posts and segments because your made the problem into something intuitively obvious to your brain.

Once you think it's correct, it's then good to run it through a test and make sure the computer does what you think it should, but at that point it should just be a formality.


[Answer]: When programming it is useful to think of:


pre-conditions
post-conditions
variants
and invariants (of loops or types)


and when exploring uncharted territory (such as juggling with indices) it can be very, very, useful to not just think about those but actually make them explicit in the code with assertions.

Let's take your original code:

/**
 * Inserts the given value in proper position in the sorted subarray i.e. 
 * array[0...rightIndex] is the sorted subarray, on inserting a new value 
 * our new sorted subarray becomes array[0...rightIndex+1].
 * @param array The whole array whose initial elements [0...rightIndex] are 
 * sorted.
 * @param rightIndex The index till which sub array is sorted.
 * @param value The value to be inserted into sorted sub array.
 */
function insert(array, rightIndex, value) {
    for(var j = rightIndex; j >= 0 && array[j] > value; j--) {
        array[j + 1] = array[j];
    }   
    array[j + 1] = value; 
};


And check what we have:


pre-condition: array[0..rightIndex] is sorted
post-condition: array[0..rightIndex+1] is sorted
invariant: 0  but it seems redundant a bit; or as @Jules noted in the comments, at the end of a "round", for n in [j, rightIndex+1] => array[j] > value.
invariant: at the end of a "round", array[0..rightIndex+1] is sorted


So you can first write a is_sorted function as well as a min function working on an array slice, and then assert away:

function insert(array, rightIndex, value) {
    assert(is_sorted(array[0..rightIndex]));

    for(var j = rightIndex; j >= 0 && array[j] > value; j--) {
        array[j + 1] = array[j];

        assert(min(array[j..rightIndex+1]) > value);
        assert(is_sorted(array[0..rightIndex+1]));
    }   
    array[j + 1] = value; 

    assert(is_sorted(array[0..rightIndex+1]));
};


There is also the fact that your loop condition is a bit complicated; you might want to make it easier on yourself by splitting things up:

function insert(array, rightIndex, value) {
    assert(is_sorted(array[0..rightIndex]));

    for (var j = rightIndex; j >= 0; j--) {
        if (array[j]  value);
        assert(is_sorted(array[0..rightIndex+1]));
    }   
    array[j + 1] = value; 

    assert(is_sorted(array[0..rightIndex+1]));
};


Now, the loop is straightforward (j goes from rightIndex to 0).

Finally, now this needs to be tested:


think of boundary conditions (rightIndex == 0, rightIndex == array.size - 2)
think of value being smaller than array[0] or larger than array[rightIndex]
think of value being equal to array[0], array[rightIndex] or some middle index


Also, do not underestimate fuzzing. You have assertions in place to catch mistakes, so generate a random array and sort it using your method. If an assertion fires, you found a bug and can extend your test suite.


[Answer]: Perhaps I should put some flesh on my comment:


  There is only one way: understand your problem better. But that is as general as your question is


Your point is 


  Although I got my assumptions correct after some trial and error but I got too frustrated because of the lack of correct computing model in my head.


When I read trial and error, my alarm bells start ringing. Of course many of us know the state of mind, when one wants to fix a small problem and has wrapped his head around other things and starts guessing in one or the other way, to make the code seem to do, what is supposed to do. Some hackish solutions come out of this - and some of them are pure genius; but to be honest: most of them are not. Me included, knowing this state.

Independendly from your concrete problem, you asked questions, about how to improve:

1) Test

That was said by others and I would have nothing valuable to add

2) Problem Analysis

It is hard to give some advice to that. There are only two hints I could give you, which probably help you improving your skills on that topic:


the obvious and most trivial one is in the long term the most effective: solve many problems. While practicing and repeating you develop the mindset which helps you for future tasks. Programming is like any other activity to be improved by hard work practicing


Code Katas are a way, which might help a bit. 


  How do you get to be a great musician? It helps to know the theory, and to understand the mechanics of your instrument. It helps to have talent. But ultimately, greatness comes from practicing; applying the theory over and over again, using feedback to get better every time.


Code Kata

One site, which I like very much: Code Wars


  Achieve mastery through challenge
  Improve your skills by training with others on real code challenges


They are relatively small problems, which help you sharpen your programming skills. And what I like most on Code Wars is, that you could compare your solution to one of others.

Or maybe, you should have a look at Exercism.io where you get feedback from the community.


The other advice is nearly as trivial: Learn to break down problems
You have to train yourself, breaking down problems into really small problems.
If you say, you have problems in writing loops, you make the mistake, that you see the loop as a whole construct and do not deconstruct it into pieces. If you learn to take things apart step by step, you learn to avoid such mistakes.


I know - as I said above sometimes you are in such a state - that it is hard to break "simple" things into more "dead simple" tasks; but it helps a lot.

I remember, when I first learned professionally programming, I had huge problems with debugging my code. What was the problem? Hybris - The error can not be in such and such region of the code, because I know that it can not be.
And in consequence? I skimmed through code instead of analyzing it
I had to learn - even if it was tedious to break my code down instruction for instruction.

3) Develop a Toolbelt

Besides knowing your language and your tools - I know these are the shiny things of which developers think first - learn Algorithms (aka reading).

Here are two books to start with:


Introduction into algortihms
Algorithms


This is like learning some recipes to start off cooking. At first you don't know what to do, so you have to look, what prior chefs cooked for you. The same goes for algortihms. Algorithms are like cooking recipies for common meals (data structures, sorting, hashing etc.) If you know them (at least try to) by heart, you have a good starting point.

3a) Know programming constructs

This point is a derivative - so to say.
Know your language - and better: know, what constructs are possible in your language.

A common point for bad or inefficent code is sometimes, that the programmer does not know the difference between different types of loops (for-, while- and do-loops). They are somehow all interchangeably useable; but in some circumstances choosing another looping construct leads to more elegant code.

And there is Duff's device ...

P.S.:


  otherwise your comment is no good than Donal Trump.


Yes, we should make coding great again!

A new motto for Stackoverflow.


[Answer]: Getting this right easily and fluently is a matter of experience.  Even though the language doesn't let you express it directly, or you're using a more complex case than the simple built-in thing can handle, what you think is a higher level like "visit each element once in revere order" and the more experienced coder translates that into the right details instantly because he's done it so many times.

Even then, in more complex cases it's easy to get wrong, because the thing you're writing is typically not the canned common thing. With more modern languages and libraries, you don't write the easy thing because there is a canned construct or call for that. In C++ the modern mantra is "use algorithms rather than writing code". 

So, the way to make sure it's right, for this kind of thing in particular, is to look at the boundary conditions.  Trace through the code in your head for the few cases on the edges of where things change. If the index == array-max, what happens? What about max-1? If the code makes a wrong turn it will be at one of these boundaries.  Some loops need to worry about the first or last element as well as the looping construct getting the bounds right; e.g. if you refer to a[I] and a[I-1] what happens when I is the minimal value?

Also, look at cases where the number of (correct) iterations is extreme: if the bounds are meeting and you will have 0 iterations, will that just work without a special case? And what about just 1 iteration, where the lowest bound is also the highest bound at the same time?

Examining the edge cases (both sides of each edge) is what you should do when writing the loop, and what you should do in code reviews.


[Answer]: In your example, the body of the loop is quite obvious. And it is quite obvious that some element has to be changed at the very end. So you write the code, but without the start of the loop, the end condition, and the final assignment. 

Then you go away from the code and find out which is the first move that needs to be performed. You change the start of the loop so that the first move is correct. You go away from the code again and find out which is the last move that needs to be performed. You change the end condition so that the last move will be correct. And finally, you go away from your code and figure out what the final assignment must be, and fix the code accordingly. 


[Answer]: Off-by-one errors are one of the most common programming mistakes. Even experienced developers get this wrong sometimes. Higher level languages usually have iteration constructs like foreach or map which avoids explicit indexing altogether. But sometimes you do need explicit indexing, as in your example.

The challenge is how to think of ranges of array cells. Without a clear mental model, it becomes confusing when to include or exclude the end points.

When describing array ranges, the convention is to include lower bound, exclude upper bound. For example the range 0..3 is the cells 0,1,2. This conventions is used throughout 0-indexed languages, for example the slice(start, end) method in JavaScript returns the subarray starting with index start up to but not including index end.

It is clearer when you think about range indexes as describing the edges between array cells. The below illustration is an array of length 9, and the numbers below the cells is aligned to the edges, and is what is used to describe array segments. E.g. it is clear from the illustration than the range 2..5 is the cells 2,3,4.

┌───┬───┬───┬───┬───┬───┬───┬───┬───┐
│ 0 │ 1 │ 2 │ 3 │ 4 │ 5 │ 6 │ 7 │ 8 │   -- cell indexes, e.g array[3]
└───┴───┴───┴───┴───┴───┴───┴───┴───┘
0   1   2   3   4   5   6   7   8   9   -- segment bounds, e.g. slice(2,5) 
        └───────────┘ 
          range 2..5


This model is consistent with having array length be the upper bound of an array. An array with length 5 have cells 0..5, which means there are the five cells 0,1,2,3,4. This also means the length of a segment is the higher bound minus the lower bound, i.e. the segment 2..5 has 5-2 = 3 cells.

Having this model in mind when iterating either upwards or downwards makes it a lot clearer when to include or exclude the end points. When iterating upwards you need to include the start point but exclude the end point. When iterating  downwards you need to exclude the start point (the higher bound) but include the end point (the lower bound). 

Since you are iterating downwards in your code, you need to include the low bound, 0, so you iterate while j >= 0.

Given this, your choice to have the rightIndex argument represent the last index in the subarray breaks the convention. It means you have to include both endpoints (0 and rightIndex) in the iteration. It also makes it difficult to represent the empty segment (which you need when you are starting the sort). You actually have to use -1 as rightIndex when inserting the first value. This seem pretty unnatural. It seem more natural to have rightIndex indicate the index after the segment, so 0 represent the empty segment.

Of course your code is extra confusing because it expands the sorted subarray with one, overwriting the item immediately after the initially sorted subarray. So you read from index j but writes the value to j+1. Here you should just be clear that j is the position in the initial subarray, before the insertion. When index operations gets too tricky it helps me to diagram it on a piece of grid paper.


[Answer]: Are you simply confused about what a for loop actually does and the mechanics of how it works?

for(initialization; condition; increment*)
{
    body
}



First, the initialization is executed
Then the condition is checked
If the condition is true, the body is run once.  If not goto #6
The increment code is executed
Goto #2
End of loop


It might be useful for you to rewrite this code using a different construct.  Here's the same using a while loop:

initialization
while(condition)
{
    body
    increment
}


Here are some other suggestions:


Can you use another language construct like a foreach loop?  This takes care of the condition and increment step for you.  
Can you use a Map or Filter function?  Some languages have functions with these names that internally loop through a collection for you.  You just supply the collection and the body.
You should really spend more time familiarizing yourself with for loops.  You'll use them all the time.  I suggest that you step through a for loop in a debugger to see how it executes.


* Note that while I use the term "increment", it's just some code that's after the body and before the condition check.  It can be a decrement or nothing at all.


[Answer]: If I understood the problem correctly, your question is how to think to get loops right from the first try, not how to make sure your loop is right (for which the answer would be testing as explained in other answers).

What I consider a good approach is to write the first iteration without any loop. After you've done this, you should notice what it should be changed between iterations.

Is it a number, like a 0 or an 1? Then you most likely need a for, and bingo, you also have your starting i. Then think how many times you want to run the same thing, and you'll also have your end condition.

If you don't know EXACTLY how many times it will run, then you don't need a for, but a while or a do while.

Technically, any loop can be translated to any other loop, but the code is easier to read if you use the right loop, so here are some tips:


If you find yourself writing a if(){...;break;} inside a for, you need a while and you already have the condition
"While" is maybe the most used loop in any language, but it shouldn't imo. If you find yourself writing bool ok=True; while(check){ do something and hopefully change ok at some point}; then you don't need a while, but a do while, because it means that you have everything you need to run the first iteration.


Now a bit of context... When I first learned to program (Pascal), I didn't speak English. For me, "for" and "while", didn't make much sense, but the "repeat" (do while in C) keyword is almost the same in my mother tongue, so I would use it for everything. In my opinion, repeat (do while) is the most natural loop, because almost always you want something to be done and then you want it to be done again, and again, until a goal is reached. "For" is just a shortcut that gives you an iterator and weirdly places the condition at the beginning of the code, even though, almost always, you want something to be done until something happens. Also, while is just a shortcut for if(){do while()}. Shortcuts are nice for later, but the easiest way to get the loops right from the first try is to write them in the same way as your brain is thinking, which is "repeat(do) {something} until(while) (you have to stop)" and then use the appropriate shortcut if necessary.


[Answer]: I'll try to stay clear of the topics mentioned galore already.


  What are tools/mental models to avoid such mistakes?


Tools

For me, the biggest tool to write better for and while loops is not to write any for or while loops at all. 

Most modern languages try to target this problem in some fashion or other. For example, Java, while having Iterator right from the start, which used to be a bit clunky to use, introduced short-cut syntax to use them easier in a latler release. C# has them as well, etc.

My currently favoured language, Ruby, has taken on the functional approach (.each, .map etc.) full-front. This is very powerful. I just did a quick count in some Ruby code-base I'm working on: in about 10.000 lines of code, there are zero for and about 5 while.

If I were forced to pick a new language, looking for functional/data based loops like that would be very high on the priority list.

Mental models

Keep in mind that while is the barest minimum of abstraction you can get, just one step above goto. In my opinion, for makes it even worse instead of better since it chunks all three parts of the loop tightly together. 

So, if I am in an evironment where for is used, then I make damn sure that all 3 parts are dead simple and always the same. This means I will write

limit = ...;
for (idx = 0; idx 

But nothing very more complex. Maybe, maybe I'll have a countdown sometimes, but I will do my best to avoid it. 

If using while, I stay clear of convoluted inner shenannigans involving the loop condition. The test inside the while(...) will be as simple as possible, and I will avoid break as best I can. Also the loop will be short (counting lines of code) and any larger amounts of code will be factored out. 

Especially if the actual while condition is complex, I will use a "condition-variable" which isvery easy to spot, and not place the condition into the while statement itself:

repeat = true;
while (repeat) {
   repeat = false; 
   ...
   if (complex stuff...) {
      repeat = true;
      ... other complex stuff ...
   }
}


(Or something like that, in the correct measure, of course.)

This gives you a very easy mental model which is "this variable is running from 0 to 10 monotonously" or "this loop runs until that variable is false/true". Most brains seem to be able to handle this level of abstraction just fine.

Hope that helps.


[Answer]: I am going to give a more detailed example of how to use pre/post conditions and invariants to develop a correct loop. Together such assertions are called a specification or contract.

I am not suggesting that you try to do this for every loop. But I hope that you will find it useful to see the thought process involved.

In order to do so, I will translate your method into a tool called Microsoft Dafny, which is designed to prove the correctness of such specifications. It also checks termination of each loop. Please note that Dafny does not have a for loop so I have had to use a while loop instead.

Finally I will show how you can use such specifications to design an, arguably, slightly simpler version of your loop. This simpler loop version does infact have the loop condition j > 0 and the assignment array[j] = value - as was your initial intuition.

Dafny will prove for us that both of these loops are correct and do the same thing.

I will then make a general claim, based on my experience, about how to write correct backwards loop, that will perhaps help you if faced with this situation in the future.

Part One - Writing a specification for the method

The first challenge we are faced with is determining what the method is actually supposed to do. To this end I designed pre and post conditions which specify the behaviour of the method. To make the specification more exact I have enhanced the method to make it return the index where value was inserted.

method insert(arr:array, rightIndex:int, value:int) returns (index:int)
  // the method will modify the array
  modifies arr
  // the array will not be null
  requires arr != null
  // the right index is within the bounds of the array
  // but not the last item
  requires 0 

This specification fully captures the behaviour of the method. My main observation about this specification is that it would be simplified if the procedure was passed the value rightIndex+1 rather than rightIndex. But since I cannot see where this method is called from I do not know what effect that change would have on the rest of the program.

Part Two - determining a loop invariant

Now we have a specification for the behaviour of the method, we have to add a specification of the loop behaviour that will convince Dafny that executing the loop will terminate and will result in the desired final state of array.

The following is your original loop, translated into Dafny syntax with loop invariants added. I have also changed it to return the index where value was inserted.

{
    // take a copy of the initial array, so we can refer to it later
    // ghost variables do not affect program execution, they are just
    // for specification
    ghost var initialArr := arr[..];


    var j := rightIndex;
    while(j >= 0 && arr[j] > value)
       // the loop always decreases j, so it will terminate
       decreases j
       // j remains within the loop index off-by-one
       invariant -1 

This verifies in Dafny. You can see it yourself by following this link. So your loop does correctly implement the method specification that I wrote in part one. You will need to decide if this method specification is really the behaviour that you wanted. 

Note that Dafny is producing a proof of correctness here. This is a far stronger guarantee of correctness than can possibly be obtained by testing. 

Part Three - a simpler loop

Now that we have a method specification that captures the behaviour of the loop. We can safely modify the implementation of the loop while still retaining confidence that we have not changed the loop behaviour. 

I have modified the loop so that it matches your original intuitions about the loop condition and final value of j. I would argue that this loop is simpler than the loop you described in your question. It is more often able to use j rather than j+1.


Start j at rightIndex+1
Change the loop condition to j > 0 && arr[j-1] > value
Change the assignment to arr[j] := value
Decrement the loop counter at the end of the loop rather than the begining


Here is the code. Note that the loop invariants are also somewhat easier to write now:

method insert2(arr:array, rightIndex:int, value:int) returns (index:int)
  modifies arr
  requires arr != null
  requires 0  0 && arr[j-1] > value)
       decreases j
       invariant 0 

Part Four - advice about backward looping

After having written and proved correct many loops over quite a few years, I have the following general advice about looping backwards.

It is almost always easier to think about and write a backward (decrementing) loop if the decrement is performed at the beginning of the loop rather than the end.

Unfortunately the for loop construct in many languages makes this difficult. 

I suspect (but cannot prove) that this complexity is what caused the difference in your intuition about what the loop should be and what it actually needed to be. You are used to thinking about forward (incrementing) loops. When you want to write a backward (decrementing) loop you try to create the loop by trying to reverse the order that things happen in a forward (incrementing) loop. But because of the way the for construct works you neglected to reverse the order of the assignment and loop variable update - which is needed for a true reversal of the order of operations between a backward and forward loop.

Part Five - bonus

Just for completeness, here is the code you get if you pass rightIndex+1 to the method rather than rightIndex. This changes eliminates all the +2 offsets that are otherwise required to think about the correctness of the loop.

method insert3(arr:array, rightIndex:int, value:int) returns (index:int)
  modifies arr
  requires arr != null
  requires 1  0 && arr[j-1] > value)
       decreases j
       invariant 0 


[Answer]: This is why I would avoid writing loops that operate on raw indices, in favor of more abstracted operations, whenever possible.

In this case, I would do something like this (in pseudo code):

array = array[:(rightIndex - 1)] + value + array[rightIndex:]



[Answer]: Reverse loops, in particular, can be difficult to reason about because many of our programming languages are biased toward forward iteration, both in the common for-loop syntax and by the use of zero-based half-open intervals.  I'm not saying that it's wrong that the languages to made those choices; I'm just saying that those choices complicate thinking about reverse loops.

In general, remember that a for-loop is just syntactic sugar built around a while loop:

// pseudo-code!
for (init; cond; step) { body; }


is equivalent to:

// pseudo-code!
init;
while (cond) {
  body;
  step;
}


(possibly with an extra layer of scope to keep variables declared in the init step local to the loop).

This is fine for many kinds of loops, but having the step come last is awkward when you're walking backwards.  When working backwards, I find it's easier to start the loop index with the value after the one I want and to move the step portion to the top of the loop, like this:

auto i = v.size();  // init
while (i > 0) {  // simpler condition because i is one after
    --i;  // step before the body
    body;  // in body, i means what you'd expect
}


or, as a for loop:

for (i = v.size(); i > 0; ) {
    --i;  // step
    body;
}


This can seem unnerving, since the step expression is in the body rather than the header.  That's an unfortunate side effect of the inherent forward-bias in for loop syntax.  Because of this, some will argue that you instead do this:

for (i = v.size() - 1; i >= 0; --i) {
    body;
}


But that's a disaster if your index variable is an unsigned type (as it might be in C or C++).

With this in mind, let's write your insertion function.


Since we'll be working backward, we'll let the loop index be the entry after the "current" array slot.  I would design the function to take the size of the integer rather than an index to the last element because half-open ranges are the natural way to represent ranges in most programming languages and because it gives us a way to represent an empty array without resorting to a magic value like -1.

function insert(array, size, value) {
  var j = size;

While the new value is smaller than the previous element, keep shifting.  Of course, the previous element can be checked only if there is a previous element, so we first have to check that we're not at the very beginning:

  while (j != 0 && value 
This leaves j right where we want the new value.

  array[j] = value; 
};



Programming Pearls by Jon Bentley gives a very clear explanation of insertion sort (and other algorithms), which can help build your mental models for these kinds of problems.


[Answer]: Attempt of additional insight

For non-trivial algorithms with loops, you could try the following method:


Create a fixed array with 4 positions, and put some values in, to simulate the problem;
Write your algorithm to solve the given problem, without any loop and with hard-coded indexings;
After that, substitute hard-coded indexings in your code with some variable i or j, and increment/decrement these variables as necessary (but still without any loop);
Rewrite your code, and put the repetitive blocks inside a loop, fulfilling the pre and post conditions;
[optional] rewrite your loop to be in the form you want (for/while/do while);
The most important is to write your algorithm correctly; after that, you refactor and optmize your code/loops if necessary (but this might make the code non-trivial anymore for a reader)


Your problem

//TODO: Insert the given value in proper position in the sorted subarray
function insert(array, rightIndex, value) { ... };


Write the body of the loop manually multiple times

Let's use a fixed array with 4 positions and try to write the algorithm manually, without loops:

           //0 1 2 3
var array = [2,5,9,1]; //array sorted from index 0 to 2
var leftIndex = 0;
var rightIndex = 2;
var value = array[3]; //placing the last value within the array in the proper position

//starting here as 2 == rightIndex

if (array[2] > value) {
    array[3] = array[2];
} else {
    array[3] = value;
    return; //found proper position, no need to proceed;
}

if (array[1] > value) {
    array[2] = array[1];
} else {
    array[2] = value;
    return; //found proper position, no need to proceed;
}

if (array[0] > value) {
    array[1] = array[0];
} else {
    array[1] = value;
    return; //found proper position, no need to proceed;
}

array[0] = value; //achieved beginning of the array

//stopping here because there 0 == leftIndex


Rewrite, removing hard-coded values

//consider going from 2 to 0, going from "rightIndex" to "leftIndex"

var i = rightIndex //starting here as 2 == rightIndex

if (array[i] > value) {
    array[i+1] = array[i];
} else {
    array[i+1] = value;
    return; //found proper position, no need to proceed;
}

i--;
if (i  value) {
    array[i+1] = array[i];
} else {
    array[i+1] = value;
    return; //found proper position, no need to proceed;
}

i--;
if (i  value) {
    array[i+1] = array[i];
} else {
    array[i+1] = value;
    return; //found proper position, no need to proceed;
}

i--;
if (i 

Translate to a loop

With while:

var i = rightIndex; //starting in rightIndex

while (true) {
    if (array[i] > value) { //refactor: this can go in the while clause
        array[i+1] = array[i];
    } else {
        array[i+1] = value;
        break; //found proper position, no need to proceed;
    }

    i--;
    if (i 

Refactor/rewrite/optimize the loop the way you want:

With while:

var i = rightIndex; //starting in rightIndex

while ((array[i] > value) && (i >= leftIndex)) {
    array[i+1] = array[i];
    i--;
}

array[i+1] = value; //found proper position, or achieved beginning of the array


with for:

for (var i = rightIndex; (array[i] > value) && (i >= leftIndex); i--) {
    array[i+1] = array[i];
}

array[i+1] = value; //found proper position, or achieved beginning of the array


PS: code assumes input is valid, and that array does not contain repetitions;


------------------------------------------------------------

ID: cs.stackexchange.com:133973
Title: Complex/Hybrid data structures- do people ever combine structures like graphs and hash tables together?
Body: I just built a LRU cache that combines a hash table and a double linked list. To me it was a brilliant idea to combine those two structures and use the strengths of each together, so it got me thinking, can people or do people combine hash tables with other more complex data structures like trees and graphs? Are there use cases and what would they be called? I don't see much written about these on the usual sites like Wikipedia.
Obviously the hash table gives constant lookup time (or nearly so allowing for collision resolution), and the tree or graph gives you ways and means of doing traversals.
I know you can hang a new tree's parent node from every single hash entry but I mean having one tree main tree or graph overlayed and interconnected with one main hash table


[Answer]: I am convinced that in many practical cases such hybrid data structures are key to effectiveness, so I like your question pretty much!
There are many examples, and I would like to detail one that makes a true difference both in practice and in theory.
First, a bit of context. There are two main data structures for representing graphs: adjacency lists, where we have the list of neighbors for each vertex; and adjacency matrices, where we have a double-entry array in which cell $i,j$ tells if there is an edge between vertices $i$ and $j$.
Let us consider a graph $G$ with $n$ vertices and $m$ edges. In practice, $m$ is generally much lower than its maximal value $\frac{n\cdot (n-1)}{2} \sim n^2$. This makes adjacency lists appealing, because they need only $O(m)$ space, whereas adjacency matrices need $O(n^2)$ space. However, one cannot query edge existence in constant time in an adjacency list representation, whereas one can do this in an adjacency matrix. This is why the two approaches coexist in the wide world of graph algorithms, with some relying on adjacency lists, and some on adjacency matrices.
In some cases, though, it is possible to take advantage of the best of the two worlds. This is particularly true when one considers large graphs with heterogeneous degrees. The degree of a vertex is its number of neighbors. Heterogeneous degrees mean that there are vertices with only few neighbors, as well as some with many many neighbors. In practice, there are often many nodes with low degree, as well as few nodes with very high degree (and some nodes with intermediate degrees). This leads to the following idea: represent low degree neighborhoods by adjacency lists, and then parsing them is fast (as they are small); and represent each high degree neighborhors by an array of size $n$. If one carefully chooses which degrees are considered small and which are considered large, then it is possible to have a compact graph representation that allows very fast edge existence queries.
I used this approach in triangle listing/counting algorithms (triangles are sets of three vertices all linked together):
Theory and Practice of Triangle Problems in Very Large (Sparse (Power-Law)) Graphs
In this case, one may formally show that the algorithmic complexity depends on the way vertices are divided into low and high degrees. The practical benefit is huge too.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:322657
Title: Role and importance of static method in OOP
Body: Background

Thinking about OOP I feel that it binds data and behavior together, taking the real world example we already have array data type which is a collection of homogeneous type and in Java we have build a nice abstraction over it hence, we have methods like clone which does its job perfectly i.e. we have data and we have associated an operation to it making client code simple and logic of clone encapsulated behind a nice API.

Hopefully I am correct till now.

Question

So, suppose we need to sort an array, it would be good to have sort method exposed as the API which does sorting on the array elements whose ordering is based on the element type which seems perfect but wait I see no such methods on Array ADT and here comes the confusion we have page full of documentation here which lists a number of static methods. If I remember properly it is a most hated pattern in TDD community to have static methods because it makes testing a hell, even we ignore their concern then also I see a violation of OOP concept here, we already have data then why not have all these methods in the array itself? 

Update

The presence of Array example here doesn't mean that I am confused about the Array in Java, my main concern is regarding the static method in general. Normally I get myself in the situation where I think it would be best to have a static method but many times I have seen community go against it hence there should be a solid reasoning about whether my design is flawed or static method is the best alternative in that situation.


[Answer]: This is a Java-specific design. For example in D, sort is an instance method on arrays, not a static method as in Java.

But in Java arrays are special. They are defined as objects, but they are not instances of classes. The only instance methods they have are the ones inherited directly from Object. There are no Array-specific instance methods since there is no Array class where they could be defined. (Note that the Arrays class you link to is just a utility class - arrays are not instances of this class.) 

A more logical design would be to have arrays be instances of the class Array, but since the initial version of Java did not have generics, this design was not possible.

On the generic lists (like List) the sort method is an instance method. So we can conclude the designers of Java agrees sort should ideally be an instance method, but it was not possible for Arrays due to limitations in the core language design.

In C# a different design was chosen, and arrays are actually instances of an Array class, which allows array-specific instance methods (Sort is still defined as static though, for whatever reason).

In short: Arrays in Java are not "true" OO, so they cannot confirm to OO design principles.


[Answer]: The notion that static methods are impossible to unit test is a myth that has proven difficult to kill.

What makes a method hard to test in isolation is stuff like hidden dependencies and accessing static state. There is no difference whatsoever between a static method and an instance method, except for the fact that invocations of instance methods get a hidden this reference parameter that is used to access private members.

If your method has an honest API, that is, it clearly states what dependencies it has, and if it is pure in every other sense, then it is trivially testable regardless of whether or not it's static.

If your method updates global variables, news up database connections and writes to disk, then it's untestable regardless of whether or not it's an instance method.

Yes, (virtual) instance methods can be overridden, but that's quite beside the point. Accessing global state and writing to disk AND marking the method virtual doesn't make it testable. That just makes it stubbable, so you can decouple from it when testing OTHER things. And that's not even mentioning the fact that an abundance of fakes and mocks and spies etc. is a testing anti-pattern in itselft and leads to rigid and fragile test code.

The key is, and has always been to make cohesive and decoupled modules and classes. As long as you're honest in your API and don't reach out and pull things out of the void, then there is no problem.

Now, as for why one would make a method like sort a static helper method instead of an instance method, it could have several reasons. At the end of the day it's just a design decision and you could go either way. One thing that speaks against putting too many (or indeed any) utility methods on entities is that their api can easily become bloated. The class will also grow and grow. Everyone requires different helpers, so how do you decide which ones are "important" enough to put on the type and which ones to put in an external class? And as a programmer, how do you know where to look?

There is a good case to be made for keeping pure data-carrying classes separate from computational/processing classes (of course while not breaking encapsulation). This can allow you to make a general sorting algorithm for any type implementing List instead of having to write one for arrays, one for arraylists, one for linked lists and so on (without having to resort to using a common abstract base class).


[Answer]: To make it simple:


Static state is bad because it is effectively global state since everyone has access to it. Wrapping it in a singleton doesn't change anything. In general it is good to avoid global state which is hard to test.
Static functions that are pure functions or that only mutate their arguments are perfectly fine. If they don't do any bookkeeping or keep any state there is nothing wrong with them.


As for arrays, they're just quirky in Java as the other answers mention. That said other collections are also sorted through Collections.sort rather than a sort method on them. The rationale is that a collection should not be aware of how it is supposed to sort itself. This is more debatable since different languages expose this on the collection directly (JavaScript's someArray.sort() for example, or Swift's someArray.sort() or C#'s .Sort and .OrderBy).

This decision is up to the designer and both approaches are fine and depend on your mental model. It is more clear cut when we discuss adding something to a collection or removing something from it - I believe that the underlying collection should be in charge in this case so it can enforce invariants. Sorting is another story since anything you can iterate you can sort which is an layer you can use abstraction. (Although, not as efficiently and polymorphic dispatch still makes sense in my opinion).


[Answer]: I'm with you on the non-OOP nature of static classes part. There are several drawbacks.

It is hidden dependency by the very nature of static-ness. You just can't inject a class, you can inject only an object. The most vivid example is, as @kai mentioned in comments, utility classes.

Classes tend to be big to huge. Since classes with static methods have nothing to do with objects, they don’t know who they are, what they should do and what they should not do. The boundaries are blurred, so we just write one instruction after another. It’s hard to stop until we’re done with our task. It is inevitably imperative and non-OOP process.

Hence the class is getting less and less cohesive. If the class has a lot of dependencies, chances are that it does more than it should.

Static methods mean that they can be called from anywhere. They can be called from a lot of contexts. They have a lot of clients. So if one class needs some little special behavior to be implemented in static method, you need to make sure that none of the other clients got broken. So such reuse simply doesn’t work. I can compare it with noble (and failed) attempt to compose and reuse microservices. The resulting classes are too generic and completely unmaintainable. This results in the whole system being tightly coupled.

To illustrate my point, I'l; take your example of sorting. I want my code to be declarative, so I separate their creation and work. So instead of having a sort method, I'd have a Sorted class implementing an Array interface (not talking about any specific language now). So my code could look like:

$array =
    new Sorted(
        new Filtered(
            new Mapped(
                [1, 2, 3],
                function (int $i) {
                    return $i * 3;
                }
            ),
            function (int $i) {
                return $i %2 == 0;
            }
        ),
        function ($a, $b) {
            return $b > $a;
        }
    )
;

print_r($array->value()); // here the objects actually work



------------------------------------------------------------

ID: cstheory.stackexchange.com:31538
Title: Runtime of Tucker's algorithm for generating a Eulerian circuit
Body: What is the time complexity of Tucker's algorithm for generating a Eulerian circuit? 
The Tucker's algorithm takes as input a connected graph whose vertices are all of even degree, constructs an arbitrary 2-regular detachment of the graph and then generates a Eulerian circuit.

I couldn't find any reference that says, for example, how the algorithm constructs an arbitrary 2-regular detachment of the graph, what data structures it uses, what the time complexity is, and so on. Any ideas?


[Answer]: It can be done on $O(|E|\log |E|)$, nevertheless implementing it might be cumbersome. I won't get into details of how implement, but a general overview so we can analyze its running time:

For this we will need and adjacency list instead of an adjacency matrix.

Step 1.-, for each vertex split away the vertices, you can split away all the edges for each vertex, it can be done in $O(|E|)$ in total, by traversing each of the adjacency list of each vertex. Then do a DFS to find and store each cycle components in a doubly linked circular list. And we will need an array A of lists, such that list A[i] will have a pointer to each component such that vertex $i$ appears in that component. This can be done while doing DFS.

Step 2 and 3. For each of the lists at each vertex, we will take two adjacent components, and we will ask if both are the same, if they are, remove the first pointer, and continue. If both are different components, join the two lists, for this, in the list we should have a pointer to the position in the doubly linked list of such vertex. Then we can merge two components in $O(1)$. Then remove one of the entries and proceed until we have only one list, and remove it. Repeat for each vertex until only one list remains, such doubly linked circular list will be our eulerian cycle.

The tricky part, and where the $O(\log |E|)$ comes from is on finding if two components are the same. For this we can do relabeling the smallest component, and we will get an amortized time of $O(\log |E|)$, (see Cormen et al's chapter on Kruskal algorithm). Or maybe use a union-find data structure, in such case you can make it amortized $O(|E|\alpha(|E|))$, which is a little better time. It could be possible to get O(|E|) time but a clever use of pointers might be implied and escapes from my intuition currently.

Note that this operation to determine if two components are the same would be in function of $|E|$ instead of $n$, since we are duplicating vertices, but the number of vertices and its duplicates is bounded by $|E|$.

Notice that you are using a graph theory book and for the pure graph theory point of view the complexity is usually irrelevant, and even more irrelevant the implementation details, from that point of view is enough to show that it can be done systematically, and the focus is on how simple you can explain and prove that it is correct, it is very uncommon to find data structures mentioned in such a book, since that is usually out the book's scope.


[Answer]: A version of Tucker's algorithm was used in B. Awerbuch, A. Israeli, and Y. Shiloach, Finding Euler Circuits in Logarithmic Parallel Time, STOC 1984, to find Euler tours efficiently in parallel, and similar ideas can be used to make the algorithm run sequentially in linear time.

Suppose you have a connected graph $G$ in which all vertices have even degree, and start by pairing off edges at each vertex. This forms a collection of cycles that together cover the graph, and Tucker's algorithm works by joining the cycles together one at a time.

To make this repeated joining process efficient, first run a connected components algorithm (e.g. DFS) to find which edges belong to which cycles. Make a bipartite multigraph $H$, where the vertices on one side of the bipartition correspond one-for-one with the vertices of $G$, the vertices on the other side of the bipartition correspond with the cycles you've constructed, and the edges of $H$ correspond to incidences between cycles and vertices in $G$. Construct a spanning tree $T$ of $H$ (e.g. DFS again), rooted at one of the cycle vertices.

Now, for each cycle $C$ that is not the root of $T$, join $C$ to its grandparent in $T$ (by swapping the edge pairing at the common vertex of the two cycles, represented by the parent of $C$), in constant time per join.

The same technique also works for directed graphs (and that's the version that Awerbuch et al use for their main presentation); in that case, the only difference is that you want equally many incoming and outgoing edges at each vertex, and the edge-pairing at each vertex must pair incoming edges with outgoing edges.


------------------------------------------------------------

ID: cs.stackexchange.com:60965
Title: Is C actually Turing-complete?
Body: I was trying to explain to someone that C is Turing-complete, and realized that I don't actually know if it is, indeed, technically Turing-complete. (C as in the abstract semantics, not as in an actual implementation.) 

The "obvious" answer (roughly: it can address an arbitrary amount of memory, so it can emulate a RAM machine, so it's Turing-complete) isn't actually correct, as far as I can tell, as although the C standard allows for size_t to be arbitrarily large, it must be fixed at some length, and no matter what length it is fixed at it is still finite. (In other words, although you could, given an arbitrary halting Turing machine, pick a length of size_t such that it will run "properly", there is no way to pick a length of size_t such that all halting Turing machines will run properly)

So: is C99 Turing-complete?


[Answer]: I'm not sure but I think the answer is no, for rather subtle reasons. I asked on Theoretical Computer Science a few years ago and didn't get an answer that goes beyond what I'll present here.

In most programming languages, you can simulate a Turing machine by:


simulating the finite automaton with a program that uses a finite amount of memory;
simulating the tape with a pair of linked lists of integers, representing the content of the tape before and after the current position. Moving the pointer means transferring the head of one of the lists onto the other list.


A concrete implementation running on a computer would run out of memory if the tape got too long, but an ideal implementation could execute the Turing machine program faithfully. This can be done with pen and paper, or by buying a computer with more memory, and a compiler targeting an architecture with more bits per word and so on if the program ever runs out of memory.

This doesn't work in C because it's impossible to have a linked list that can grow forever: there's always some limit on the number of nodes.

To explain why, I first need to explain what a C implementation is. C is actually a family of programming languages. The ISO C standard (more precisely, a specific version of this standard) defines (with the level of formality that English allows) the syntax and semantics a family of programming languages. C has a lot of undefined behavior and implementation-defined behavior. An “implementation” of C codifies all the implementation-defined behavior (the list of things to codify is in appendix J for C99). Each implementation of C is a separate programming language. Note that the meaning of the word “implementation” is a bit peculiar: what it really means is a language variant, there can be multiple different compiler programs that implement the same language variant.

In a given implementation of C, a byte has $2^{\texttt{CHAR_BIT}}$ possible values. All data can represented as an array of bytes: a type t has at most 
$2^{\texttt{CHAR_BIT} \times \texttt{sizeof(t)}}$ possible values. This number varies in different implementations of C, but for a given implementation of C, it's a constant.

In particular, pointers can only take at most $2^{\texttt{CHAR_BIT} \times \texttt{sizeof(void*)}}$ values. This means that there is a finite maximum number of addressable objects.

The values of CHAR_BIT and sizeof(void*) are observable, so if you run out of memory, you can't just resume running your program with larger values for those parameters. You would be running the program under a different programming language — a different C implementation.

If programs in a language can only have a bounded number of states, then the programming language is no more expressive than finite automata. The fragment of C that's restricted to addressable storage only allows at most $n \times 2^{\texttt{CHAR_BIT} \times \texttt{sizeof(void*)}}$ program states where $n$ is the size of the abstract syntax tree of the program (representing the state of the control flow), therefore this program can be simulated by a finite automaton with that many states. If C is more expressive, it has to be through the use of other features.

C does not directly impose a maximum recursion depth. An implementation is allowed to have a maximum, but it's also allowed not to have one. But how do we communicate between a function call and its parent? Arguments are no good if they're addressable, because that would indirectly limit the depth of recursion: if you have a function int f(int x) { … f(…) …} then all the occurrences of x on active frames of f have their own address and so the number of nested calls is bounded by the number of possible addresses for x.

A C program can use non-addressable storage in the form of register variables. “Normal” implementations can only have a small, finite number of variables that don't have an address, but in theory an implementation could allow an unbounded amount of register storage. In such an implementation, you can make an unbounded amount of recursive calls to a function, as long as its argument are register. But since the arguments are register, you can't make a pointer to them, and so you need to copy their data around explicitly: you can only pass around a finite amount of data, not an arbitrary-sized data structure that's made of pointers.

With unbounded recursion depth, and the restriction that a function can only get data from its direct caller (register arguments) and return data to its direct caller (the function return value), you get the power of deterministic pushdown automata.

I can't find a way to go further.

(Of course you could make the program store the tape content externally, through file input/output functions. But then you wouldn't be asking whether C is Turing-complete, but whether C plus an infinite storage system is Turing-complete, to which the answer is a boring “yes”. You might as well define the storage to be a Turing oracle — call fopen("oracle", "r+"), fwrite the initial tape content to it and fread back the final tape content.)


[Answer]: In practice, these restrictions are irrelevant to Turing completeness.  The real requirement is to allow the tape to be arbitrary long, not infinite.  That would create a halting problem of a different kind (how does the universe "compute" the tape?)

It's as bogus as saying "Python isn't Turing complete because you can't make a list infinitely large".

[Edit: thanks to Mr. Whitledge for clarifying how to edit.]


[Answer]: Choose size_t to be infinitely large

You could choose size_t to be infinitely large.  Naturally, it is impossible to realize such an implementation.  But that's no surprise, given the finite nature of the world we live in.

Practical Implications

But even if it were possible to realize such an implementation, there would be practical issues.  Consider the following C statement:

printf("%zu\n",SIZE_MAX);


This prints a decimal representation of SIZE_MAX to standard out.  Presumably, SIZE_MAX is $O(2^{size\_t})$.  So if size_t is infinitely large, SIZE_MAX is also infinitely large.  The only way I know to print a decimal form of an infinitely large number is to produce an infinite stream of decimal digits.  This means that printf will not terminate in some cases.

Fortunately, for our theoretical purposes, I could not find any requirement in the specification that guarantees printf will terminate for all inputs.  So, as far as I am aware, we do not violate the C specification here.

On Computational Completeness

It still remains to prove that our theoretical implementation is Turing Complete.  We can show this by implementing "any single-taped Turing Machine".

Most of us have probably implemented a Turing Machine as a school project.  I won't give the details of a specific implementation, but here's a commonly used strategy:


The number of states, number of symbols, and state transition table are fixed for any given machine.  So we can represent states and symbols as numbers, and the state transition table as a 2-dimensional array.
The tape can be represented as a linked list.  We can either use a single double-linked list, or two single-linked lists (one for each direction from the current position).


Now let's see what's required to realize such an implementation:


The ability to represent some fixed, but arbitrarily large, set of numbers.  In order to represent any arbitrary number, we choose MAX_INT to be infinite as well.  (Alternatively, we could use other objects to represent states and symbols.)
The ability to construct an arbitrarily large linked list for our tape.  Once again, there is no finite limit on the size.  This means we cannot construct this list up-front, as we would spend forever just to construct our tape.  But, we can construct this list incrementally if we use dynamic memory allocation.  We can use malloc, but there's a little more we must consider:


The C specification allows malloc to fail if, e.g., available memory has been exhausted.  So our implementation is only truly universal if malloc never fails.
However, if our implementation is run on a machine with infinite memory, then there is no need for malloc to fail.  Without violating the C standard, our implementation will guarantee that malloc will never fail.

The ability to dereference pointers, lookup array elements, and access the members of a linked list node.


So the above list is what is necessary to implement a Turing Machine in our hypothetical C implementation.  These features must terminate.  Anything else, however, may be allowed to not terminate (unless required by the standard).  This includes arithmetic, IO, etc.


[Answer]: The main argument here was that the size of the size_t is finite, although can be infinitely large.

There is a workaround for it, though I am not sure if this coincides with ISO C.

Assume you have a machine with infinite memory. Thus you are not bounded for pointer size. You still have your size_t type. If you ask me what is sizeof(size_t) the answer will be just sizeof(size_t). If you ask if it is greater than 100 for example the answer is yes. If you ask what is sizeof(size_t) / 2 as you could guess the answer is still sizeof(size_t). If you want to print it we can agree on some output. The difference of these two can be NaN and so on.

The summary is that relaxing the condition for size_t have finite size won't break any programs already existing.

P.S. Allocating memory sizeof(size_t) is still possible, you need only countable size, so let's say you take all evens (or similar trick). 


[Answer]: C99's addition of va_copy to the variadic argument API may give us a back door to Turing-completeness. Since it becomes possible to iterate through a variadic arguments list more than once in a function other than the one that originally received the arguments, va_args can be used to implement a pointerless pointer.

Of course, a real implementation of the variadic argument API is probably going to have a pointer somewhere, but in our abstract machine it can be implemented using magic instead.

Here's a demo implementing a 2-stack pushdown automaton with arbitrary transition rules:

#include 
typedef struct { va_list va; } wrapped_stack; // Struct wrapper needed if va_list is an array type.
#define NUM_SYMBOLS /* ... */
#define NUM_STATES /* ... */
typedef enum { NOP, POP1, POP2, PUSH1, PUSH2 } operation_type;
typedef struct { int next_state; operation_type optype; int opsymbol; } transition;
transition transition_table[NUM_STATES][NUM_SYMBOLS][NUM_SYMBOLS] = { /* ... */ };

void step(int state, va_list stack1, va_list stack2);
void push1(va_list stack2, int next_state, ...) {
    va_list stack1;
    va_start(stack1, next_state);
    step(next_state, stack1, stack2);
}
void push2(va_list stack1, int next_state, ...) {
    va_list stack2;
    va_start(stack2, next_state);
    step(next_state, stack1, stack2);
}
void step(int state, va_list stack1, va_list stack2) {
    va_list stack1_copy, stack2_copy;
    va_copy(stack1_copy, stack1); va_copy(stack2_copy, stack2);
    int symbol1 = va_arg(stack1_copy, int), symbol2 = va_arg(stack2_copy, int);
    transition tr = transition_table[state][symbol1][symbol2];
    wrapped_stack ws;
    switch(tr.optype) {
        case NOP: step(tr.next_state, stack1, stack2);
        // Note: attempting to pop the stack's bottom value results in undefined behavior.
        case POP1: ws = va_arg(stack1_copy, wrapped_stack); step(tr.next_state, ws.va, stack2);
        case POP2: ws = va_arg(stack2_copy, wrapped_stack); step(tr.next_state, stack1, ws.va);
        case PUSH1: va_copy(ws.va, stack1); push1(stack2, tr.next_state, tr.opsymbol, ws);
        case PUSH2: va_copy(ws.va, stack2); push2(stack1, tr.next_state, tr.opsymbol, ws);
    }
}
void start_helper1(va_list stack1, int dummy, ...) {
    va_list stack2;
    va_start(stack2, dummy);
    step(0, stack1, stack2);
}
void start_helper0(int dummy, ...) {
    va_list stack1;
    va_start(stack1, dummy);
    start_helper1(stack1, 0, 0);
}
// Begin execution in state 0 with each stack initialized to {0}
void start() {
    start_helper0(0, 0);
}


Note: If va_list is an array type, then there are actually hidden pointer parameters to the functions. So it would probably be better to change the types of all va_list arguments to wrapped_stack.


[Answer]: Removable media allows us to circumvent the unbounded memory problem. Perhaps people will think this is an abuse, but I think it's OK and essentially unavoidable anyway.

Fix any implementation of a universal Turing machine. For the tape, we use removable media. When the head runs off the end or beginning of the current disc, the machine prompts the user to insert the next or previous one. We can either use a special marker to denote the left end of the simulated tape, or have a tape that's unbounded in both directions.

The key point here is that everything the C program must do is finite. The computer only needs enough memory to simulate the automaton, and size_t only needs to be big enough to allow addressing that (actually rather small) amount of memory and on the discs, which can be of any fixed finite size. Since the user is only prompted to insert the next or previous disc, we don't need unboundedly large integers to say "Please insert disc number 123456..."

I suppose the principal objection is likely to be to the involvement of the user but that seems to be unavoidable in any implementation, because there seems to be no other way of implementing unbounded memory.


[Answer]: Nonstandard arithmetic, maybe?
So, it seems that the issue is the finite size of sizeof(t). However, I think I know a work around.
As far as I know, C does not require an implementation to use the standard integers for its integer type. Therefore, we could use a non-standard model of arithmetic. Then, we would set sizeof(t) to some nonstandard number, and now we will never reach it in a finite number of steps. Therefore, the length of the Turing machines tape will always be less than the "maximum", since the maximum is literally impossible to reach. sizeof(t) simply is not a number in the regular sense of the word.
This is one technicality of course: Tennenbaum's theorem. It states that the only computable model of Peano arithmetic is the standard one, which obviously would not do. However, as far as I know, C does not require implementations to use data types that satisfy the Peano axioms, nor does it require the implementation to be computable, so this should not be an issue.
What should happen if you try to output a nonstandard integer? Well, you can represent any nonstandard integer using a nonstandard string, so just stream digits from the front of that string.


[Answer]: IMO, a strong limitation is that the addressable space (via the pointer size) is finite, and this is unrecoverable.

One could advocate that memory can be "swapped to disk", but at some point the address information will itself exceed the addressable size.


[Answer]: The answer is yes, but for an unexpected reason.  

I believe the above comments are correct — given size_t is bounded, you cannot represent a turing machine with unbounded states by using C as intended.  However, we can use C in a way which completely circumvents the size_t issue — using only the C preprocessor.  

I will not go over the whole proof here — this answer explains it best.  Essentially, using deferred expressions it is possible to create recursively expanding macros that expand forever.  In this way, the depth of the recursion becomes limited only to the number of scans which the machine executes — this is a physical limitation, but in theory the machine could scan forever.  The answer also explains how logical operations can be constructed.

So in conclusion: yes, C is turing-complete, but you have to totally misuse the preprocessor.


[Answer]: You could define a language quite similar to C, where it is allowed to change sizeof, for example by assigning
sizeof(int) = 20;
sizeof(void*) += 4;

etc. That would be Turing complete. As your requirements go up, you would just modify sizeof(void*) to be sufficiently large, same with sizeof(size_t), and then you can use realloc to make your arrays bigger.


[Answer]: The problem here is that you're artificially limiting the set of C programs to just the ones that use pointers (and size_t, intptr_t, what have you), but that's not necessary. If you're forced to use only the standard library, then "no" is kinda correct like in the accepted answer, but the semantics of C are indeed Turing complete.
Say you have a Real Turing Machine (TM) and it provides a library for pushing and popping from two stacks of unbounded size, and (for thoroughness) lets also say that they just compile to push and pop assembler instructions. Now your C program can push and pop infinitely to two stacks, unrestrained by the semantics of C. Et voila, Turing completeness.
This is not the only way we can show that C is Turing complete, and it turns out that this is not a high bar to meet for programming languages. The real challenge is in constructing a language that is not turing complete (e.g. SQL), and giving a proof for it.
Note: Other answers seem to suggest that Turing completeness requires arbitrary memory access, but that's simply not true. A push-down automaton with two unbounded stacks is sufficient, hence the scenario above.


[Answer]: Mapping "C" to a Turing equivalent abstract model of computation
C as it currently exists is not Turing complete because C inherently requires some fixed pointer size. We can however map a C like language to an abstract model of computation that is Turing complete.
The basic syntax and semantics of C could be mapped to an abstract model of computation that is Turing equivalent. The RASP model of computation  is a model that "C" can be mapped to.
A variation of the RASP model is shown below that the x86/x64 language maps to. Since it is already known that "C" maps to the x86/x64 concrete models of computation we know that "C" maps to the following abstract model:
Instruction
     : INTEGER ":" OPCODE                     // Address:Opcode 
     | INTEGER ":" OPCODE INTEGER             // Address:Opcode Operand 
     | INTEGER ":" OPCODE INTEGER "," INTEGER // Address:Opcode Operand, Operand
HEXDIGIT [a-fA-F0-9]
INTEGER  {HEXDIGIT}+ 
OPCODE   HEXDIGIT{4} 
// OPCODE distinguishes between INTEGER literal and machine address operand.
// OPCODE INTEGER is space delimited.

Mapping the C language to the above abstract model of computation would enable the C language to become equivalent to a Turing machine.
Because the x86/x64 language maps to the above abstract model this also provides the basis for recognizing the subset of Turing equivalent x86/x64/C computations. As long as the required pointer size is no larger than the pointer size that is available the computation is Turing equivalent on finite hardware.
Turing equivalent computations derive equivalent output or fail to halt on equivalent input between the concrete machine and its Turing machine equivalent.


[Answer]: The problem isn’t that computers, being limited by the real world, cannot implement a Turing-complete machine. The problem in this question here is that the C language itself doesn’t allow it.
The C language could easily be changed to be Turing complete.  Adding an integer type of unlimited size that can be used in malloc() and in pointer arithmetic would have solved the problem. In practice it wouldn’t make any difference, since we can’t even build a computer where 64 bit pointer sizes are the limit.


------------------------------------------------------------

ID: cs.stackexchange.com:54283
Title: Sorting algorithm that moves element to a 2-dimensional array
Body: I had an idea for a sorting algorithm wich is very fast, but can (potentially) use a lot of memory. I'm not a Computer Science student/graduate, only a self-taught programmer so I don't know how to evaluate it's viability. Also, I would like to know if it has already been documented and under what name.

Algorithm:


Get an unsorted array
Iterate through it, and find the highest and lowest value stored in it
Determine "range" (highest - lowest)
Make a 2 dimensional array presorted,
in which the first dimension's size is "range" + 1
For each element in array "unsorted"


Add the current element into presorted[current_element_value - lowest]

Make array "sorted", and add each element of presorted's second dimension, ignoring the empty first dimensions.
return the sorted array


An example using said algorithm would be the following:

unsorted[] = {5, 8, 2, 4, 6, 8, 2, 0, 4, 5, 6, 3, 3, 2, 1}

After iterating through it once, we get:
lowest: 0
highest: 8

range = highest - lowest = 8

Make 2 dimensional array presorted,
 consisting of range+1 arrays:

presorted[0] = {}
presorted[1] = {}
presorted[2] = {}
presorted[3] = {}
presorted[4] = {}
presorted[5] = {}
presorted[6] = {}
presorted[7] = {}
presorted[8] = {}

Next we add the elements, first element of unsorted is 5.
 5 - lowest (0) is 5

presorted[0] = {}
presorted[1] = {}
presorted[2] = {}
presorted[3] = {}
presorted[4] = {}
presorted[5] = {5}
presorted[6] = {}
presorted[7] = {}
presorted[8] = {}

After doing this which each element:

presorted[0] = {0}
presorted[1] = {1}
presorted[2] = {2, 2, 2}
presorted[3] = {3, 3}
presorted[4] = {4, 4}
presorted[5] = {5, 5}
presorted[6] = {6, 6}
presorted[7] = {}
presorted[8] = {8, 8}

Now, we simply create an empty array called "sorted",
 to which we add all the elements
 of non-empty arrays in presorted:

sorted = {0, 1, 2, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 8, 8}


This is the C++ source code for it

std::vector Sort(std::vector &unsorted)
{
   int min = unsorted[0];
   int max = unsorted[0];

   int range;

   std::vector sorted;

   for (int i = 0; i  max)
         max = unsorted[i];
      if (unsorted[i] > presorted(range+1);

   for (int i = 0; i 

Is this algorithm viable?

I think it could be used in certain scenarios (when range is not too big) and it has the advantage that you can determine if it's appropriate to use it with simply iterating once through the unsorted list.

I'll do some tests and report back.


[Answer]: What you have come up with is a sligthly less memory efficient version of counting sort. 

This algorithm works in $\mathcal{O}(n + range)$ time where $n$ is the number of elements in the vector $unsorted$ and $max$ and $min$ are as defined in your program.

Your implementation uses $\mathcal{O}(n + range)$ memory. You can make it more memory efficient by just keeping track of how many times the same number in the vector 'unsorted' occurs rather than maintaining a vector with repeated instances of the same element i.e.

vector presorted(range+1);
for(int i = 0; i 

It will now use only $\mathcal{O}(range)$ memory.

Yes you are correct, this method might potentially use up a lot of memory if the difference between the smallest and largest elements of the vector is too great.

For those who are not fluent in C++:


OP is finding the smallest and largest elements of the unsorted array $max$ and $min$ respectively. 
He is initializing an array of size $max - min$ and creating an empty linked list in each position of the array (in this code it is the vector of vector of integers $presorted$). 
Then each element of $unsorted$ is added as a new node in the linked list at $presorted[unsorted[i] - min]$. In a random access computational model this operation will take $\mathcal{O}(1)$ time and hence each element of $unsorted$ is added to its appropriate bucket in constant time.
Then he is just iterating in the array of linked lists and compressing it to the final array $sorted$ which is the final output.



------------------------------------------------------------

ID: softwareengineering.stackexchange.com:348798
Title: What kind of algorithm requires a set?
Body: On my first programming courses I was told I should use a set whenever I need to do things like remove duplicates of something. E.g.: to remove all duplicates from a vector, iterate through said vector and add each element to a set, then you're left with unique occurrences. However, I could also do that by adding each elemento to another vector and checking if the element already exists. I assume that depending on the language used there might be a difference in performance. But is there a reason to use a set other than that?

Basically: what kinds of algorithms require a set and shouldn't be done with any other container type?


[Answer]: 
I assume that depending on the language used there might be a difference in performance. But is there a reason to use a set other than that?

Oh yes, (but it's not performance.)
Use a set when you can use one because not using it means you have to write extra code. Using a set makes what your doing easy to read. All that testing for uniqueness logic is hidden off somewhere else where you don't have to think about it. It's in a place that's already tested and you can trust that it works.
Write your own code to do that and you have to worry about it. Bleh.  Who wants to do that?

Basically: what kinds of algorithms require a set and shouldn't be done with any other container type?

There is no algorithm that "shouldn't be done with any other container type". There are simply algorithms that can take advantage of sets. It's nice when you don't have to write extra code.
Now there is nothing particularly special about set in this regard. You should always use the collection that best fits your needs.  In java I've found this picture to be helpful in making that decision. You'll notice that it has three different kinds of sets.

And as @germi rightly points out, if you use the right collection for the job your code becomes easier for others to read.


[Answer]: Software entities that represent real-world entities often are logically sets. For example, consider a Car. Cars have unique identifiers and group of cars forms a set. The set notion serves as a constraint on the collection of Cars that a program may know about and constraining data values is very valuable.

Also, sets have a very well defined algebra. If you have a set of Cars owned by George and a set owned by Alice, then the union is clearly the set owned by both George and Alice even if George and Alice both own the same car. So the algorithms that should use sets are those where the logic of the entities involved exhibit set characteristics. That turns out to be quite common.

How sets are implemented and how the uniqueness constraint is guaranteed is another matter. One hopes to be able to find an appropriate implementation for the set logic that eliminates duplicates given that sets are so fundamental to logic, but even if you do the implementation yourself, the uniqueness guarantee is intrinsic to the insertion of an item in a set and you should not have to be "checking if the element already exists".


[Answer]: To check whether a set containing n elements contains another element X takes typically constant time. To check whether an array containing n elements contains another element X takes typically O (n) time. That's bad, but if you want to remove the duplicates from n items, suddenly it takes O (n) in time instead of O (n^2); 100,000 items will bring your computer to its knees. 

And you are asking for more reasons? "Apart from the shooting, did you enjoy the evening, Mrs. Lincoln?"


[Answer]: 
  However, I could also do that by adding each elemento to another vector and checking if the element already exists.


If you do that, then you are implementing the semantics of a set on top of the vector datastructure. You're writing extra code (which could contain errors), and the result will be extremely slow if you have a lot of entries.

Why would you want to do that over using an existing, tested, efficient set implementation?


[Answer]: There are all kinds of set based algorithms, particularly where you need to perform intersections and unions of sets and have the result be a set.

Set based algorithms are used heavily in various path finding algorithms, etc.

For a primer on set theory check out this link: http://people.umass.edu/partee/NZ_2006/Set%20Theory%20Basics.pdf

If you need set semantics, use a set.  It's going to avoid bugs due to spurious duplicates because you forgot to prune the vector/list at some stage, and it's going to be faster than you can do by constantly pruning your vector/list.


[Answer]: You are asking about sets specifically but I think your question is about a larger concept: abstraction.  You are absolutely correct that you can use a Vector to do this (if you are using Java, use ArrayList instead.)  But why stop there?  What do you need the Vector for?  You can do this all with arrays.

When ever you need to add an item to the array, you can simply loop over every element and if it's not there, you add it at the end.  But, actually, you need to first check whether there's room in the array.  If there isn't you'll need to create a new array that is larger and copy all the existing elements from the old array to the new array and then you can add the new element.  Of course, you also need to update every reference to the old array to point to the new one.  Got all that done?  Great!  Now what were we trying to accomplish again?

Or, instead you could use a Set instance and just call add().  The reason that sets exist is that they are an abstraction that is useful for lots of common problems.  For example, let's say you want to track items and react when a new one is added.  You call add() on a set and it returns true or false based on whether the set was modified.  You could write that all by hand using primitives but why?

There might actually be a case where you have a List and you want to remove duplicates.  The algorithm you propose is pretty much basically the slowest way you could do that.  There are a couple of common quicker ways: bucketing them or sorting them.  Or, you could add them to a set that implements one of those algorithms.

Early on in your career/education the focus is on building these algorithms and understanding them and it's important to do that.  But that's not what professional developers do on a normal basis.  They use these approaches to build much more interesting things and using pre-built and reliable implementations saves boatloads of time.


[Answer]: Apart from the performance characteristics (which are very significant, and shouldn't be so easily dismissed), Sets are very important as an abstract collection.

Could you emulate Set behavior (ignoring performance) with an Array? Yes, absolutely! Every time you insert, you can check if the element is already in the array, and then only add the element if it wasn't already found. But that's something you consciously have to be aware of, and remember every time you insert into your Array-Psuedo-Set. Oh what's that, you inserted once directly, without first checking for duplicates? Welp, your array has broken its invariant (that all elements are unique, and equivalently, that no duplicates exist).

So what would you do to get around that? You would create a new data type, call it (say, PsuedoSet), which wraps an internal Array, and exposes an insert operation publicly, which will enforce the uniqueness of elements. Since the wrapped array is only accessible through this public insert API, you guarantee that duplicates can never come about. Now add some hashing to improve performance of the contains checks, and sooner or later you'll realize that you implemented a full-out Set.

I would also respond with a statement and follow up question:


  On my first programming courses I was told I should use an Array whenever I need to do things like store multiple ordered elements of something. E.g.: to store a collection of names of coworkers. However, I could also do that by allocating raw memory, and setting the value of the memory address given by the start pointer + some offset.


Could you use a raw pointer and fixed offsets to mimic an Array? Yes, absolutely! Every time you insert, you can check if the offset doesn't wander off the end of the allocated memory you're working with. But that's something you consciously have to be aware of, and remember every time you insert into your Pseudo-Array. Oh what's that, you inserted once directly, without first checking the offset? Welp, there's a Segmentation fault with your name on it! 

So what would you do to get around that? You would create a new data type, call it (say, PsuedoArray), which wraps a pointer and a size, and exposes an insert operation publicly, which will enforce that the offset doesn't exceed the size. Since the wrapped data is only accessible through this public insert API, you guarantee that no buffer overflows can occur. Now add some other convenience functions (Array resizing, element deletion, etc.), and sooner or later you'll realize that you implemented a full-out Array.


[Answer]: I actually find standard set containers to be mostly useless myself and prefer to just use arrays but I do it in a different way.

To compute set intersections, I iterate through the first array and mark elements with a single bit. Then I iterate through the second array and look for marked elements. Voila, set intersection in linear time with far less work and memory than a hash table, e.g. Unions and differences are equally simple to apply using this method. It does help that my codebase revolves around indexing elements rather than duplicating them (I duplicate indices to elements, not the data of the elements themselves) and rarely needs anything to be sorted, but I haven't used a set data structure in years as a result.

I also have some evil bit-fiddling C code I use even when the elements offer no data field for such purposes. It involves using the memory of the elements themselves by setting the most significant bit (which I never use) for the purpose of marking traversed elements. That's pretty gross, don't do that unless you're really working at near-assembly level, but just wanted to mention how it can be applicable even in cases when elements don't provide some field specific for traversal if you can guarantee that certain bits will never be used. It can compute a set intersection between 200 million elements (bout 2.4 gigs of data) in less than a second on my dinky i7. Try doing a set intersection between two std::set instances containing a hundred million elements each in the same time; doesn't even come close.

That aside...


  However, I could also do that by adding each elemento to another
  vector and checking if the element already exists.


That checking to see if an element already exists in the new vector is generally going to be a linear time operation, which will make the set intersection itself a quadratic operation (explosive amount of work the bigger the input size). I recommend the technique above if you just want to use plain old vectors or arrays and do it in a way that scales wonderfully.


  Basically: what kinds of algorithms require a set and shouldn't be
  done with any other container type?


None if you ask my biased opinion if you're talking about it at the container level (as in a data structure specifically implemented to provide set operations efficiently), but there are plenty that require set logic at the conceptual level. For example, let's say you want to find the creatures in a game world which are capable of both flying and swimming, and you have flying creatures in one set (whether or not you actually use a set container) and ones that can swim in another. In that case, you want a set intersection. If you want creatures that can either fly or are magical, then you use a set union. Of course you don't actually need a set container to implement this, and the most optimal implementation generally doesn't need or want a container specifically designed to be a set.

Going Off Tangent

All right, I got some nice questions from JimmyJames regarding this set intersection approach. It's kinda veering off subject but oh well, I'm interested in seeing more people use this basic intrusive approach to set intersection so that they're not building whole auxiliary structures like balanced binary trees and hash tables just for the purpose of set operations. As mentioned the fundamental requirement is that the lists shallow copy elements so that they are indexing or pointing to a shared element that can be "marked" as traversed by the pass through the first unsorted list or array or whatever to then pick up on the second pass through the second list.

However, this can be accomplished practically even in a multithreading context without touching the elements provided that:


The two aggregates contain indices to the elements.
The range of indices is not too large (say [0, 2^26), not billions or more) and are reasonably densely occupied.


This allows us to use a parallel array (just one bit per element) for the purpose of set operations. Diagram:



Thread synchronization only needs to be there when acquiring a parallel bit array from the pool and releasing it back to the pool (done implicitly when going out of scope). The actual two loops to perform the set operation need not involve any thread syncs. We don't even need to use a parallel bit pool if the thread can just allocate and free the bits locally, but the bit pool can be handy to generalize the pattern in codebases that fit this kind of data representation where central elements are often referenced by index so that each thread doesn't have to bother with efficient memory management. Prime examples for my area are entity-component systems and indexed mesh representations. Both frequently need set intersections and tend to refer to everything stored centrally (components and entities in ECS and vertices, edges, and polygons in indexed meshes) by index.

If the indices are not densely occupied and sparsely scattered, then this is still applicable with a reasonable sparse implementation of the parallel bit/boolean array, like one which only stores memory in 512-bit chunks (64 bytes per unrolled node representing 512 contiguous indices) and skips allocating completely vacant contiguous blocks. Chances are you are already using something like this if your central data structures are sparsely occupied by the elements themselves. 



... similar idea for a sparse bitset to serve as a parallel bit array. These structures also lend themselves towards immutability since it's easy to shallow copy chunky blocks which don't need to be deep copied to create a new immutable copy.

Again set intersections between hundreds of millions of elements can be done in under a second using this approach on a very average machine, and that's within a single thread.

It can also be done in under half the time if the client doesn't need a list of elements for the resulting intersection, like if they only want to apply some logic to the elements found in both lists, at which point they can just pass a function pointer or functor or delegate or whatever to be called back to process ranges of elements that intersect. Something to this effect:

// 'func' receives a range of indices to
// process.
set_intersection(func):
{
    parallel_bits = bit_pool.acquire()

    // Mark the indices found in the first list.
    for each index in list1:
        parallel_bits[index] = 1

    // Look for the first element in the second list 
    // that intersects.
    first = -1
    for each index in list2:
    {
         if parallel_bits[index] == 1:
         {
              first = index
              break
         }
    }

    // Look for elements that don't intersect in the second
    // list to call func for each range of elements that do
    // intersect.
    for each index in list2 starting from first:
    {
        if parallel_bits[index] != 1:
        {
             func(first, index)
             first = index
        }
    }
    If first != list2.num-1:
        func(first, list2.num)
}


... or something to this effect. The most expensive part of the pseudocode in the first diagram is intersection.append(index) in the second loop, and that applies even for std::vector reserved to the size of the smaller list in advance.

What If I Deep Copy Everything?

Well, stop that! If you need to do set intersections, it implies that you are duplicating data to intersect against. Chances are that even your tiniest objects aren't smaller than a 32-bit index. It is very possible to reduce the addressing range of your elements to 2^32 (2^32 elements, not 2^32 bytes) unless you actually need more than ~4.3 billion elements instantiated, at which point a totally different solution is needed (and that definitely isn't using set containers in memory).

Key Matches

How about cases where we need to do set operations where the elements aren't identical but could have matching keys? In that case, same idea as above. We just need to map each unique key to an index. If the key is a string, for example, then interned strings can do just that. In those cases a nice data structure like a trie or a hash table is called for to map string keys to 32-bit indices, but we don't need such structures in order to do the set operations on the resulting 32-bit indices.

A whole lot of very cheap and straightforward algorithmic solutions and data structures open up like these when we can work with indices to elements in a very reasonable range, not the full addressing range of the machine, and so it's often more than worth it to be able to obtain a unique index for each unique key.

I Love Indices!

I love indices just as much as pizza and beer. When I was in my 20s, I got really into C++ and started designing all kinds of fully standard-compliant data structures (including the tricks involved to disambiguate a fill ctor from a range ctor at compile-time). In retrospect that was a large waste of time.

If you revolve your database around storing elements centrally in arrays and indexing them rather than storing them in a way that's fragmented and potentially across the entire addressable range of the machine, then you can end up exploring a world of algorithmic and data structure possibilities by just designing containers and algorithms that revolve around plain old int or int32_t. And I found the end result to be so much more efficient and easy to maintain where I wasn't constantly transferring elements from one data structure to another to another to another.

Some example use cases when you can just assume that any unique value of T has a unique index and will have instances residing in a central array:

Multithreaded radix sorts which work well with unsigned integers for indices. I actually have a multithreaded radix sort which takes about 1/10th of the time to sort a hundred million elements as Intel's own parallel sort, and Intel's is already 4 times faster than std::sort for such large inputs. Of course Intel's is much more flexible since it's a comparison-based sort and can sort things lexicographically, so it's comparing apples to oranges. But here I often only need oranges, like I might do a radix sort pass just to achieve cache-friendly memory access patterns or filter out duplicates quickly.

Ability to build linked structures like linked lists, trees, graphs, separate chaining hash tables, etc. without heap allocations per node. We can just allocate the nodes in bulk, parallel to the elements, and link them together with indices. The nodes themselves just become a 32-bit index to the next node and stored in a big array, like so:



Friendly for parallel processing. Often linked structures aren't so friendly for parallel processing, since it's awkward at the very least to try to achieve parallelism in tree or linked list traversal as opposed to, say, just doing a parallel for loop through an array. With the index/central array representation, we can always go to that central array and process everything in chunky parallel loops. We always have that central array of all elements we can process this way, even if we only want to process some (at which point you might process the elements indexed by a radix-sorted list for cache-friendly access through the central array).

Can associate data to each element on the fly in constant-time. As with the case of the parallel array of bits above, we can easily and extremely cheaply associate parallel data to elements for, say, temporary processing. This has use cases beyond temporary data. For example, a mesh system might want to allow users to attach as many UV maps to a mesh as they want. In such a case, we can't just hard-code how many UV maps there will be in every single vertex and face using an AoS approach. We need to be able to associate such data on the fly, and parallel arrays are handy there and so much cheaper than any kind o sophisticated associative container, even hash tables.

Of course parallel arrays are frowned upon due to their error-prone nature of keeping parallel arrays in sync with each other. Whenever we remove an element at index 7 from the "root" array, for example, we likewise have to do the same thing for the "children". However, it's easy enough in most languages to generalize this concept to a general-purpose container so that the tricky logic to keep parallel arrays in sync with each other only need exist in one place throughout the entire codebase, and such a parallel array container can use the sparse array implementation above to avoid wasting lots of memory for contiguous vacant spaces in the array to be reclaimed upon subsequent insertions.

More Elaboration: Sparse Bitset Tree

All right, I got a request to elaborate some more which I think was sarcastic, but I'm gonna do so anyway cause it's so much fun! If people want to take this idea to whole new levels, then it is possible to perform set intersections without even linearly looping through N+M elements. This is my ultimate data structure which I've been using for ages and basically models set:



The reason it can perform set intersections without even inspecting each element in both lists is because a single set bit at the root of the hierarchy can indicate that, say, a million contiguous elements are occupied in the set. By just inspecting one bit, we can know that N indices in the range, [first,first+N) are in the set, where N could be a very large number.

I actually use this as a loop optimizer when traversing occupied indices, because let's say there are 8 million indices occupied in the set. Well, normally we would have to access 8 million integers in memory in that case. With this one, it can potentially just inspect a few bits and come up with index ranges of occupied indices to loop through. Further, the ranges of indices it comes up with are in sorted order which makes for very cache-friendly sequential access as opposed to, say, iterating through an unsorted array of indices used to access the original element data. Of course this technique fares worse for extremely sparse cases, with the worst-case scenario being like every single index being an even number (or every one being odd), in which case there are no contiguous regions whatsoever. But in my use cases at least, that practically never happens.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:147645
Title: What is the difference between an Array and a Stack?
Body: According to Wikipedia, a stack:


  is a last in, first out (LIFO) abstract data type and linear data structure.


While an array:


  is a data structure consisting of a collection of elements (values or variables), each identified by at least one array index or key.


As far as I understand, they are fairly similar. So, what are the main differences? If they are not the same, what can an array do that a stack can't and vice-versa?


[Answer]: You can retrieve an item from any index of an A\array.

With a stack, you can retrieve an item in the middle of stack A, by using another stack: B.

You keep taking the top item out of A and putting it into B until you are at the desired item of A, then you put the items from B back on top of stack A.

So, for data that requires the ability to retrieve an arbitrary index, the stack is more difficult to work with.

In the situation where you want "last in, first out" behavior, a stack will give you less overhead than an array.


[Answer]: Well, you can certainly implement a stack with an array.  The difference is in access.  In an array, you have a list of elements and you can access any of them at any time.  (Think of a bunch of wooden blocks all laid out in a row.)

But in a stack, there's no random-access operation; there are only Push, Peek and Pop, all of which deal exclusively with the element on the top of the stack.  (Think of the wooden blocks stacked up vertically now. You can't touch anything below the top of the tower or it'll fall over.)


[Answer]: I wouldn't go so far as to say they're "very similar."

An array is used to hold things that will later be accessed sequentially or through the index.  The data structure doesn't imply any sort of access method (FIFO, LIFO, FILO, etc...) but it can be used that way if you wanted.

A stack is way of track of things as they're generated.  An access method is implied / required depending upon the type of stack that's created.  A frame stack would be a LIFO example.  Disclaimer - I may be mixing my data structure taxonomy here and a stack may truly only allow LIFO.  It would be a different type of queue otherwise.

So I could use an array as a stack (although I wouldn't want to) but I can't use a stack as an array (unless I work really hard at it).


[Answer]: The data in an array can be accessed by a key or index. The data in a stack can be accessed when it is popped off of the top of the stack. This means that accessing data from an array is easy if you know its index. Access data from a stack means you have to keep popping elements until you find the one you were looking for, which means data access can take longer.


[Answer]: 
  Their responsibilities are different:



Stack must be able to pop elements onto the stack and push elements from the stack, hence why it normally has methods Pop() and Push()
Array's responsibility is to get/set element at a specified index



[Answer]: I think the biggest confusion going on here is implementation versus basic data structures.

In most (more basic languages) an array represents a fixed length set of elements which you can access any at a given time. The fact that you have a bunch of elements like this tells you nothing about how it is supposed to be used (and frankly a computer won't know/care how you use it, as long as you don't violate usage).

A stack is an abstraction used to represent data that should be handled in a certain way. This is an abstract concept because it just says that it has to have some subroutine/method/function that can add to the top or remove from the top, while data below the top doesn't get touched. Purely your choice to use an array this way. 

You can make a stack out of many different kinds of data structures: arrays (with some max size), dynamic arrays (can grow when out of space) or linked lists. Personally I feel that a linked list represents the restrictions of a stack the best as you have to put in a bit of effort to see things beyond the first element and it's very easy to add to the front and remove from the front. 

So you can use an array to MAKE a stack, but they are not equivalent


[Answer]: In a pure stack, the only allowable operations are Push, Pop, and Peek but in practical terms, that's not exactly true.  Or rather, the Peek operation often allows you to look at any position on the stack, but the catch is that it's relative to the one end of the stack.

So, as others have said, an array is random access and everything's referenced to the beginning of the array.

In a stack, you can only add/remove at the working end of the stack, but you still have random access read but it's referenced to the working end.  That's the fundamental difference.

For instance, when you pass parameters on a stack to a function, the callee doesn't have to pop the parameters off to look at them.  It just pushes local variables on the stack and references all the local variables and parameters based on an offset from the stack pointer.  If you were using just an array, then how would the callee know where to look for its parameters?  When the callee is done, it pops off its local variables, pushes a return value, returns control to the caller, and the caller pops the return value (if any), and then pops the parameters off the stack.  The beauty is that it works no matter how far nested you are into your function calls (assuming you don't run out of stack space).

That's one particular use/implementation, but it illustrates the difference:  array is always referenced from the beginning but stacks are always referenced from some working end position.

One possible implementation of a stack is an array plus an index to remember where the working end is.


[Answer]: An array is from the programmers perspective, fixed in place and size, you know where you are in it and where the whole thing is.  You have access to all of it.

With a stack, you sit on one end of it but have no idea of its size or how far you can safely go.  Your access to it is limited to the amount you have allocated, you often dont even know if when allocating the amount you wanted if you just slammed into the heap or program space.  Your view of a stack is a small array that you have allocated yourself, the size you wanted, you have control over and can see.  Your portion is no different than an array.  The difference lies in your array is tacked onto one end of another array for sake of argument and terminology, which you have no visibility of, no idea how big or small, and you cant touch it without possibly causing harm.  An array, unless global, is often implemented on the stack anyway, so the array and the stack share the same space for the duration of that function.

If you want to get into the hardware side, then it is processor specific of course, but generally the array is based off of a known starting point/address, the size is known by the compiler/programmer, and addresses are computed into it, sometimes using register offset addressing (load a value from the address defined by this base register value plus this offset register value, likewise when compiled it could be an immediate offset not necessarily register based, depends on the processor of course) which in assembly very much resembles accessing an array in high level code.  Likewise with the stack, if available, you can use register or immediate offset addressing, often though it uses a special register, either the stack pointer itself, or a register reserved by the compiler/programmer to be used for accessing the stack frame for that function.  And for some processors special stack access functions are used and/or required to access the stack.  You have push and pop instructions but they are not used as often as you would think and dont really apply to this question.  For some processors push and pop are aliases for instructions that can be used with any register, anywhere, not just the stack pointer on the stack, further removing push and pop from being related to this question. 


------------------------------------------------------------

ID: cs.stackexchange.com:132239
Title: Position in the Linked-list
Body: I am reading Data Structures and Algorithms by Alfred Aho, Jeffrey Ullman, et al. In the section about lists it's said:

We should note that a position in a linked-list implementation of a list behaves
differently from a position in an array implementation. Suppose we have a list with
three elements a, b, c and a variable p, of type position, which currently has position
3 as its value; i.e., it points to the cell holding b, and thus represents the position of c.
If we execute a command to insert x at position 2, the list becomes a, x, b, c, and
element b now occupies position 3. If we use the array implementation of lists
described earlier, b and c would be moved down the array, so b would indeed occupy
the third position.
However, if the linked-list implementation is used, the value of p, which is a pointer
to the cell containing b, does not change because of the insertion, so afterward, the
value of p is "position 4," not 3. This position variable must be updated, if it is to be
used subsequently as the position of b.†

I don't understand how do position works? It's said that position works differently in pointer implementation that that of array but, the result seems to be the same on both. In both implementation position (pointer) moves forward. Then, what's the difference? I appreciate your answers.


[Answer]: First of all, let me strongly recommend avoiding outdated textbooks such as Aho, Hopcroft and Ullman.
Let me also replace Aho, Hopcroft and Ullman's use of "position" with the more common notion of pointer. In a linked list, each cell consists of a value and pointer to the following cell (if any). The linked list itself is represented as a pointer to its first cell (if any). Therefore, if we consider the list $a,b,c$, then a pointer which is pointing at the third element will be pointing at the cell containing $c$.
(This is also a good place to mention that unless you're programming in Fortran or in Excel, then your arrays are zero-based; that is, the first element is element number 0.)
Now suppose that $p$ is a pointer which currently points at the cell containing $c$. If you insert $x$ between $a$ and $b$, then $p$ will still be pointing at $c$. However, $c$ is now the fourth element rather than the third element.
In contrast, if the list were implemented as an array, then $p$ will be a pointer to the third element of the array. Therefore, after inserting $x$ between $a$ and $b$, the pointer $p$ will be pointing at $b$.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:446257
Title: Object immutability and persistence
Body: While I was learning functional programming I have encounterd two for me very similar terms: immutability and persistence. Also I have read simular questions from stackoverflow, but I am still confused.
Wikipedia says:

In object-oriented and functional programming, an immutable object (unchangeable object) is an object whose state cannot be modified after it is created.


In computing, a persistent data structure or not ephemeral data structure is a data structure that always preserves the previous version of itself when it is modified.

So the only way to change the immutable variable state is to assign a new value to its identifier, the old value will be deleted if there are no other variables pointing to itself. Every other variable pointing to old state will not see changes (keeping old state). Persistent data structures behaves the same, also both persisten and immutable objects are used in concurrent programming. I am confused, are there any differences between those two terms? My current understandings are that every persistent data structure is immutable and vice versa.


[Answer]: Persistence means something very different to most imperative programmers, so there might be some confusion if you're searching the term.
The simplest persistent data structure in the functional/immutable sense is a linked list. Say you have the following linked list:
2 -> 3 -> 4

And you prepend a 1 to the list, giving:
1 -> 2 -> 3 -> 4

Note that any pointer that pointed to the previous version of the list with head 2 still works just fine. Most of the data structure was reused, not copied. Persistent data structures are a subset of immutable data structures that have this sharing property. Without persistence, you basically have to copy the entire list whenever you make a change if you want to maintain immutability.
You can extend that same sharing idea to trees and then use trees to make even more complex structures. Chris Okasaki wrote the canonical book on the subject if you want to learn more.


[Answer]: tl;dr summary: every persistent data structure is immutable, i.e. persistence is a subset of immutability, but not the other way around.
Note: the Wikipedia article is about the more general concept of persistent data structures, not limited to functional programming. You are asking specifically about the context of functional programming, though, where the term is used in a narrower sense, so I will restrict my answer to this narrower sense.
Immutability
Immutable means unchangeable, i.e. something which cannot be changed. I've never learned Latin, but I am reasonably sure that is quite literally the translation with im- being a negative prefix – think of immature, immaterial, etc. – and mutability meaning changeability – think of mutation.
The primary example of a data type that is immutable even in languages that otherwise fully embrace mutability is numbers. I can't name any language off the top of my head that would allow to mutate the state of a number such that, e.g. 1 + 1 suddenly equals 4.
An example of a more complex data structure that is immutable in some (but not all) languages, is strings. They are immutable in Java, C#, Python, and ECMAScript, all of which are languages which otherwise embrace mutability. They are mutable in Ruby, for example, although efforts are underway to restrict that mutability.
Immutability means that I cannot just "change" a sub-element of a complex data structure, such as an array. If I have an immutable array [1, 2, 3], I cannot change an element of the array so that I now have [1, 99, 3]. I have to copy the entire array with one element changed, so that I now have two arrays, I still have the original [1, 2, 3] and I also have the new [1, 99, 3].
This copying process is slow: every time I want to change one element, I need to copy the entire array, i.e. changing one element has a worst-case step complexity of θ(length of the array).
This is where Persistence comes in:
Persistence
An informal definition of a persistent data structure is a data structure that is BOTH

immutable AND
has "efficient operations".

Now, I don't want to go into too much detail of what we mean by "efficient operations", but I want to motivate it with an example. Let's look at dynamic arrays.
Example: dynamic arrays
Dynamic arrays are arrays which can change their size, i.e. you can add and remove elements. Many collections libraries have dynamic arrays, both mutable (e.g. Python's list, Ruby's Array, Java's ArrayList, .NET's ArrayList) and immutable (e.g. Clojure's Vector, Scala's Vector).
Mutable dynamic arrays
… naïve implementation
A simple and naïve implementation of mutable dynamic arrays is to allocate a new array of size n+1 and copy all the existing elements over every time you append an element. This gives you a worst-case step complexity of θ(1) for indexing and mutation of an element, just like a mutable non-resizable array, and it gives you θ(length of the array) appending, which is an operation a static array cannot do.
… the clever implementation
However, we can do better by using geometric resizing. Instead of just adding one storage place when we append one element, we add c × n storage places (i.e. a constant multiplier of the current size) whenever we run out of storage. A popular textbook choice is c = 2, i.e. doubling the array in size whenever it gets full, although smaller constants are sometimes preferable.
This still has the same worst-case step complexity as we had before, since we still need to copy the entire array in the worst case.
However, the amortized worst-case step complexity is much better: it is θ(1) for appending. Dynamic arrays are the standard introductory example for amortized analysis, so I will not repeat the proof here as you can find it readily online or in any textbook. Just to give you an intuition, though, the fundamental "trick" is that the "worst-case" of needing to copy all θ(n) elements only happens every θ(n) operations, so if you amortize this worst-case "cost" over many successive appends, each append on average only takes θ(1).
Immutable dynamic arrays
… naïve implementation
The naïve implementation of immutable dynamic arrays is that we copy the array for every "mutating" operation. This means that almost all operations, i.e. append, pop (delete last element), insert in the middle, delete from the middle, and mutate (change an element at a fixed index) are θ(n), only indexing is θ(1).
This is similar to the naïve mutable dynamic array, except that the naïve mutable array has θ(1) mutation of an element.
So, this is clearly an undesirable implementation. Can we get an implementation that is closer to the performance of the "clever" mutable dynamic array implementation but still immutable?
… implementation based on difference
One idea for implementing immutable data structures efficiently is to only record the difference to the previous version instead of copying the whole thing. So, for example, when inserting an element into the array, instead of copying the whole array, we create a special "array proxy" that behaves externally just like an array, but internally, it contains a reference to the previous array and a description of the difference, e.g. "at index i, insert value 42, shift all later indices by 1".
However, this means that the operations get slower the longer you use the data structure, as the list of differences gets longer and longer and ever more complex. Everybody who has ever used a version control system that is based on storing a forward chain of patches knows that checking out a file takes longer and longer the older the project is, because in order to reconstruct the current state of the file, all patches have to be applied one-by-one from the original version.
We can implement a scheme similar to the clever dynamic mutable array, where we "collapse" the differences into a new array when they reach a certain size or complexity. However, that still means that we have different performance characteristics for what looks like the same array, depending on how it was constructed. For example, if I construct a new array directly as [1, 2, 3, 4, 5], it will have faster operations than if I start with an empty array [], then append 1, then append 2, and so on, even though the result should behave the same.
This is what persistent data structures are about: they are immutable and they are efficient, and this efficiency applies regardless of how many "version changes" there have been.
The solution to our problem is structural sharing.
… efficient implementation exploiting structural sharing
Aside: CONS lists
The simplest example of a data structure exploiting structural sharing is the CONS list. The CONS list is built like this:

We have special object which is the empty list (often called () or NIL).
A list is either the empty list or a pair of references  where head points to the first element of the list and tail points to the rest of the list.

Here is a (runnable) example in Scala:
sealed trait MyList[+A]:
  @throws(classOf[NotImplementedError]) def head: A
  def tail: MyList[A]
  override def toString: String

case object MyNil extends MyList[Nothing]:
  @throws(classOf[NotImplementedError]) override def head = ???
  override val tail = this
  override val toString = "()"

case class MyCons[+A](val head: A, override val tail: MyList[A]) extends MyList[A]:
  override val toString = s"($head, $tail)"

val list1 = MyCons(4, MyCons(5, MyNil))
val list2 = MyCons(1, MyCons(2, MyCons(3, list1)))

val second = list2.tail.head

list1.toString //=>             (4, (5, ()))
list2.toString //=> (1, (2, (3, (4, (5, ())))))
second         //=> 2

As you can see, we were able to efficiently construct a new list (list2) based on an existing list (list1), by the fact that list1 and list2 share their structure. (In fact, in this simple example, list1 is fully contained within list2.) Even though the two lists share their structure, everybody who has a reference to the head of list1 can still independently use it without even having to know about list2 and vice versa.
This gives us θ(1) prepending of a new element at the head. If we want to insert, delete, or "mutate" an element in the middle, we only need to copy the path from the head up to the mutation point, but we can still share the tail with the previous version.
This kind of structural sharing is used by many widely-used persistent data structure implementations.
Generalizing lists to trees
In particular, many persistent data structure implementations are based on trees. I started with the list first, because I found it easier to understand how structural sharing and path copying works in lists and then generalize this understanding to trees. After all, you can think of a list as a degenerate tree where each node has only a single child. Conversely, once you understand how CONS lists work, how they share their structure, and how path copying works, you can sort-of imagine a persistent tree as a CONS list with more than one head value and more than one tail.
And that is exactly how typical persistent dynamic arrays are implemented: as trees.
You probably have learned about trees already. The vast majority of trees that are taught are binary trees where each node has two children. And you can build a dynamic array this way. You can think of a dynamic array as a binary search tree where the array index is the search key.
Just like with the CONS list example, a newer version and an older version can share a sub-structure. In the case of the CONS list, that was a list tail, in the case of a tree, that will be a sub-tree. And just like with the CONS list, when you make a change in the middle of the tree, you only need to copy the path from the root to the location of the change, but not the entire tree. The rest of the tree structure can still be shared!
In reality, persistent dynamic arrays don't use binary trees, they use much wider, much shallower trees where the tree nodes are themselves immutable arrays. This improves cache locality, i.e. multiple consecutive elements of the "tree array" are actually implemented as multiple consecutive elements of the internal immutable array (which is an actual array, i.e. a contiguous region of memory). It also reduces the number of "steps" you need to take until you find the value.
For example, Clojure's Vector uses a 32-ary tree, i.e. each node is an array of size 32. That means it takes at most log32(n) steps to go from the root to the leaf of an array with n values. While this is still, technically θ(log n) for indexing, in reality, log32 grows very slow and is almost indistinguishable from O(1) even for billions of elements.
This is an example where constants do matter! Rich Hickey, the creator of Clojure, likes to say that Clojure's Vectors are "θ(log32 n)", where the 32 is not irrelevant. Just as an example: Arrays and mutable ArrayLists in Java can only have a maximum of 2,147,483,647 elements. log32(2147483647) is slightly less than 6.2, meaning it will take a maximum of 7 pointer dereferences plus one array access to find a value in a Clojure Vector as opposed to one pointer dereference and one array access in a Java ArrayList.
Which means that not only is our dynamic array immutable and persistent with preservation of old versions and efficient operations regardless of number of versions, but it is even almost as efficient as its mutable counterpart.
Conclusion
So, in conclusion, the difference between an immutable data structure and a persistent data structure in functional programming is that persistent data structures are immutable with additional restrictions, namely:

Efficient operations AND
Preservation of old versions

Where efficient operations applies both to new and old versions.
Often, they are also "efficient" compared to their mutable counterparts.
Further reading
As mentioned in Karl Bielefeldt's answer, Prof. Chris Okasaki, Ph.D.'s book Purely Functional Data Structures, based on his Ph.D. thesis of the same name is the definitive work on immutable and persistent data structures.
However, there have been some improvements since then. Phil Bagwell created the Hash Array Mapped Trie (HAMT). Rich Hickey used HAMTs in Clojure with some tweaks. Phil Bagwell then improved on those tweaks when he helped re-write the collections library in Scala 2.8.
There is a big list question on the sister site cstheory.se titled What's new in purely functional data structures since Okasaki? which has links to a lot of interesting stuff.
Functional programming
Please note that everything I wrote above only applies within the context of functional-programming.
In the Wikipedia article, a distinction is made between fully persistent and partially persistent:

In the partial persistence model, a programmer may query any previous version of a data structure, but may only update the latest version. This implies a linear ordering among each version of the data structure. In the fully persistent model, both updates and queries are allowed on any version of the data structure.

In functional programming, we only care about the fully persistent model. The distinction between versions that can be updated and ones which can't doesn't make sense for the way we use persistent data structures in FP. In FP, we use persistent data structures as immutable data structures with efficient operations. We don't consider them to be "versions", they are all just values that we want to manipulate.
The Wikipedia article continues:

In some cases the performance characteristics of querying or updating older versions of a data structure may be allowed to degrade […]

Again, when we talk about persistent data structures in the context of functional programming, we do not allow this.
Regarding this statement in your question:

My current understandings are that every persistent data structure is immutable and vice versa.

Neither of those two things is true in general.
Only within the context of functional programming is the first statement true: every persistent data structure is an immutable data structure. (Although that is trivially true in functional programming, as everything is immutable.)
In functional programming, a persistent data structure is a special kind of immutable data structure with efficient operations.
The general concept of persistent data structures, though, is broader then immutable data structures.
Red herring: persistent storage
To add to the confusion, the term persistence can have another, very different, meaning.
The term persistence is also used in the context of persistent storage. Here, it means data storage which outlives the lifetime of the process which created the data.
A typical example of persistent storage would be a hard disk. The antonym here would be something like "volatile".
While there is some relationship in the etymology of those two terms, the concepts are completely different and from distinct sub-fields.
Note that the tag persistence you used on your question refers to this kind of persistence, not the functional-programming kind, which seems to have caused some confusion among answerers and commenters.


------------------------------------------------------------

ID: cstheory.stackexchange.com:1539
Title: What's new in purely functional data structures since Okasaki?
Body: Since Chris Okasaki's 1998 book "Purely functional data structures", I haven't seen too many new exciting purely functional data structures appear; I can name just a few:


IntMap (also invented by Okasaki in 1998, but not present in that book)
Finger trees (and their generalization over monoids)


There are also some interesting ways of implementing already known datastructures, such as using "nested types" or "generalized algebraic datatypes" to ensure tree invariants.

Which other new ideas have appeared since 1998 in this area?


[Answer]: Conchon, Filliatre, A Persistent UNION-FIND Data Structure and Semi-persistent Data Structures.


[Answer]: New purely functional data structures published since 1998:


2001: Ideal Hash Trees, and its 2000 predecessor, Fast And Space Efficient Trie Searches, by Phil Bagwell: Apparently used as a fundamental building block in Clojure's standard library.
2001: A Simple Implementation Technique for Priority Search Queues, by Ralf Hinze: a really simple and beautiful technique for implementing this important datastructure (useful, say, in the Dijkstra algorithm). The implementation is particularly beautiful and readable due to heavy use of "view patterns".
2002: Bootstrapping one-sided flexible arrays, by Ralf Hinze: Similar to Okasaki's random-access lists, but they can be tuned to alter the time tradeoff between cons and indexing.
2003: New catenable and non-catenable deques, by Radu Mihaescu and Robert Tarjan: A new take on some older work (by Kaplan and Tarjan) that Okasaki cites (The most recent version of Kaplan & Tarjan's work was published in 2000). This version is simpler in some ways.
2005: Maxiphobic heaps (paper and code), by Chris Okasaki: Presented not as a new, more efficient structure, but as a way to teach priority queues.
2006: Purely Functional Worst Case Constant Time Catenable Sorted Lists, by Gerth Stølting Brodal, Christos Makris, and Kostas Tsichlas: Answers an outstanding question of Kaplan and Tarjan by demonstrating a structure with O(lg n) insert, search, and delete and O(1) concat.
2008: Confluently Persistent Tries for Efficient Version Control, by Erik D. Demaine, Stefan Langerman, and Eric Price: Presents several data structures for tries that have efficient navigation and modification near the leaves. Some are purely functional. Others actually improve a long-standing data structure by Dietz et al. for fully persistent (but not confluently persistent or purely functional) arrays. This paper also presente purely functional link-cut trees, sometimes called "dynamic trees".
2010: A new purely functional delete algorithm for red-black trees, by Matt Might: Like Okasaki's red-black tree insertion algorithm, this is not a new data structure or a new operation on a data structure, but a new, simpler way to write a known operation.
2012: RRB-Trees: Efficient Immutable Vectors, by Phil Bagwell and Tiark Rompf: An extension to Hash Array Mapped Tries, supporting immutable vector concatenation, insert-at, and split in O(lg n) time, while maintaining the index, update, and insertion speeds of the original immutable vector.


Known in 1997, but not discussed in Okasaki's book:


Many other styles of balanced search tree. AVL, brother, rank-balanced, bounded-balance, and many other balanced search trees can be (and have been) implemented purely functionally by path copying. Perhaps deserving special mention are:


Biased Search Trees, by Samuel W. Bent, Daniel D. Sleator, and Robert E. Tarjan: A key element in Brodal et al.'s 2006 paper and Demaine et al.'s 2008 paper.

Infinite sets that admit fast exhaustive search, by Martín Escardó: Perhaps not a data structure per se.
Three algorithms on Braun Trees, by Chris Okasaki: Braun trees offer many stack operations in worst-case O(lg n). This bound is surpassed by many other data structures, but Braun trees have a cons operation lazy in its second argument, and so can be used as infinite stacks in some ways that other structures cannot.
The relaxed min-max heap: A mergeable double-ended priority queue and The KD heap: An efficient multi-dimensional priority queue, by Yuzheng Ding and Mark Allen Weiss: These happen to be purely functional, though this is not discussed in the papers. I do not think the time bounds achieved are any better than those that can be achieved by using finger trees (of Hinze & Paterson or Kaplan & Tarjan) as k-dimensional priority queues, but I think the structures of Ding & Weiss uses less space.
The Zipper, by Gérard Huet: Used in many other data structures (such as Hinze & Paterson's finger trees), this is a way of turning a data structure inside-out.
Difference lists are O(1) catenable lists with an O(n) transformation to usual cons lists. They have apparently been known since antiquity in the Prolog community, where they have an O(1) transformation to usual cons lists. The O(1) transformation seems to be impossible in traditional functional programming, but Minamide's hole abstraction, from POPL '98, discusses a way of allowing O(1) append and O(1) transformation within pure functional programming. Unlike the usual functional programming implementations of difference lists, which are based on function closures, hole abstractions are essentially the same (in both their use and their implementation) as Prolog difference lists. However, it seems that for years the only person that noticed this was one of Minamide's reviewers.
Uniquely represented dictionaries support insert, update, and lookup with the restriction that no two structures holding the same elements can have distinct shapes. To give an example, sorted singly-linked lists are uniquely represented, but traditional AVL trees are not. Tries are also uniquely represented. Tarjan and Sundar, in "Unique binary search tree representations and equality-testing of sets and sequences", showed a purely functional uniquely represented dictionary that supports searches in logarithmic time and updates in $O(\sqrt{n})$ time. However, it uses $\Theta(n \lg n)$ space. There is a simple representation using Braun trees that uses only linear space but has update time of $\Theta(\sqrt{n \lg n})$ and search time of $\Theta(\lg^2 n)$


Mostly functional data structures, before, during, and after Okasaki's book:


Many procedures for making data structures persistent, fully persistent, or confluently persistent: Haim Kaplan wrote an excellent survey on the topic. See also above the work of Demaine et al., who demonstrate a fully persistent array in $O(m)$ space (where $m$ is the number of operations ever performed on the array) and $O(\lg \lg n)$ expected access time.
1989: Randomized Search Trees by Cecilia R. Aragon and Raimund Seidel: These were discussed in a purely functional setting by Guy E. Blelloch and Margaret Reid-Miller in Fast Set Operations Using Treaps
and by Dan Blandford and Guy Blelloch in Functional Set Operations with Treaps (code). They provide all of the operations of purely functional fingertrees and biased search trees, but require a source of randomness, making them not purely functional. This may also invalidate the time complexity of the operations on treaps, assuming an adversary who can time operations and repeat the long ones. (This is the same reason why imperative amortization arguments aren't valid in a persistent setting, but it requires an adversary with a stopwatch)
1997: Skip-trees, an alternative data structure to Skip-lists in a concurrent approach, by Xavier Messeguer and Exploring the Duality Between Skip Lists and Binary Search Trees, by Brian C. Dean and Zachary H. Jones: Skip lists are not purely functional, but they can be implemented functionally as trees. Like treaps, they require a source of random bits. (It is possible to make skip lists deterministic, but, after translating them to a tree, I think they are just another way of looking at 2-3 trees.)
1998: All of the amortized structures in Okasaki's book! Okasaki invented this new method for mixing amortization and functional data structures, which were previously thought to be incompatible. It depends upon memoization, which, as Kaplan and Tarjan have sometimes mentioned, is actually a side effect. In some cases (such as PFDS on SSDs for performance reasons), this may be inappropriate.
1998: Simple Confluently Persistent Catenable Lists, by Haim Kaplan, Chris Okasaki, and Robert E. Tarjan: Uses modification under the hood to give amortized O(1) catenable deques, presenting the same interface as an earlier (purely functional, but with memoization) version appearing in Okasaki's book. Kaplan and Tarjan had earlier created a purely functional O(1) worst-case structure, but it is substantially more complicated.
2007: As mentioned in another answer on this page, semi-persistent data structures and persistent union-find by Sylvain Conchon and Jean-Christophe Filliâtre


Techniques for verifying functional data structures, before, during, and after Okasaki's book:


Phantom types are an old method for creating an API that does not allow certain ill-formed operations. A sophisticated use of them can be found in Oleg Kiselyov and Chung-chieh Shan's Lightweight Static Capabilities.
Nested types are not actually more recent than 1998 - Okasaki even uses them in his book. There are many other examples that are not in Okasaki's book; some are new, and some are old. They include:


Stefan Kahrs's Red-black trees with types (code)
Ross Paterson's AVL trees (mirror)
Chris Okasaki's From fast exponentiation to square matrices: an adventure in types
Richard S. Bird and Ross Peterson's de Bruijn notation as a nested datatype
Ralf Hinze's Numerical Representations as Higher-Order Nested Datatypes.

GADTs are not all that new, either. They are a recent addition to Haskell and some MLs, but they have been present, I think, in various typed lambda calculi since the 1970s.
2004-2010: Coq and Isabelle for correctness. Several people have used theorem provers to verify the correctness of purely functional data structures. Coq can extract these verifications to working code in Haskell, OCaml, and Scheme; Isabelle can extract to Haskell, ML, and OCaml.


Coq:


Pierre Letouzey and Jean-Christophe Filliâtre formalized red-black and AVL(ish) trees, finding a bug in the OCaml standard library in the process.
I formalized Brodal and Okasaki's asymptotically optimal priority queues.
Arthur Charguéraud formalized 825 of the 1,700 lines of ML in Okasaki's book.

Isabelle:


Tobias Nipkow and Cornelia Pusch formalized AVL trees.
Viktor Kuncak formalized unbalanced binary search trees.
Peter Lammich published The Isabelle Collections framework, which includes formalizations of efficient purely functional data structures like red-black trees and tries, as well as data structures that are less efficient when used persistently, such as two-stack-queues (without Okasaki's laziness trick) and hash tables.
Peter Lammich also published formalizations of tree automata, Hinze & Patterson's finger trees (with Benedikt Nordhoff and Stefan Körner), and Brodal and Okasaki's purely functional priority queues (with Rene Meis and Finn Nielsen).
René Neumann formalized binomial priority queues.


2007: Refined Typechecking with Stardust, by Joshua Dunfield: This paper uses refinement types for ML to find errors in SMLNJ's red-black tree delete function.
2008: Lightweight Semiformal Time Complexity Analysis for Purely Functional Data Structures by Nils Anders Danielsson: Uses Agda with manual annotation to prove time bounds for some PFDS.


Imperative data structures or analyses not discussed in Okasaki's book, but related to purely functional data structures:


The Soft Heap: An Approximate Priority Queue with Optimal Error Rate, by Bernard Chazelle: This data structure does not use arrays, and so has tempted first the #haskell IRC channel and later Stack Overflow users, but it includes delete in o(lg n), which is usually not possible in a functional setting, and imperative amortized analysis, which is not valid in a purely functional setting.
Balanced binary search trees with O(1) finger updates. In Making Data Structures Persistent, James R Driscoll, Neil Sarnak, Daniel D. Sleator, and Robert E. Tarjan present a method for grouping the nodes in a red-black tree so that persistent updates require only O(1) space. The purely functional deques and finger trees designed by Tarjan, Kaplan, and Mihaescu all use a very similar grouping technique to allow O(1) updates at both ends. AVL-trees for localized search by Athanasios K. Tsakalidis works similarly.
Faster pairing heaps or better bounds for pairing heaps: Since Okasaki's book was published, several new analyses of imperative pairing heaps have appeared, including Pairing heaps with O(log log n) decrease Cost by Amr Elmasry and Towards a Final Analysis of Pairing Heaps by Seth Pettie. It may be possible to apply some of this work to Okasaki's lazy pairing heaps.
Deterministic biased finger trees: In Biased Skip Lists, by Amitabha Bagchi, Adam L. Buchsbaum, and Michael T. Goodrich, a design is presented for deterministic biased skip lists. Through the skip list/tree transformation mentioned above, it may be possible to make deterministic biased search trees. The finger biased skip lists described by John Iacono and Özgür Özkan in Mergeable Dictionaries might then be possible on biased skip trees. A biased finger tree is suggested by Demaine et al. in their paper on purely functional tries (see above) as a way to reduce the time-and space bounds on finger update in tries.
The String B-Tree: A New Data Structure for String Search in External Memory and its Applications by Paolo Ferragina and Roberto Grossi is a well studied data structure combining the benefits of tries and B-trees.



[Answer]: To the excellent notes already made, I’ll add Zippers.

Huet, Gerard. “Functional Pearl: The Zipper” Journal of Functional Programming 7 (5): 549-554, September 1997.

Wikipedia: Zipper (data structure)


[Answer]: I'd add McBride's version of zippers as derivatives of data types.


[Answer]: Rangemaps

It is a specialized data structure, but it can be used as a substitute for Martin Erwig's DIET, with slightly different properties, so at least there is one existing data structure to compare it to. The DIET itself was described in an article in JFP in 1998, so perhaps it is not included in Purely Functional Data Structures.


[Answer]: Following up on the 2012 paper linked above, the work on RRB vectors has since been extended and published in ICFP'15.

RRB vector: a practical general purpose immutable sequence
http://dl.acm.org/citation.cfm?id=2784739


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:333734
Title: Why do we need stacks and queues?
Body: I don't see the reason to have classes for stacks, queues and deques if we have the data structure linked list, since a linked list can act as both a stack and a queue (and always has the functions of both, if not just named differently). 

So this brings forth my question,

Is there a specific reason I should use stacks and queues over a linked list besides readability? 

last tidbit, I program in Java and .Net


[Answer]: Stacks and queues are ways of working with contiguous memory.  Linked lists are not.  Now sure, any problem that you can solve with one structure you could solve with another and slap enough abstraction on it that you couldn't tell the difference.  So what's the real difference? Performance.  Choosing between these structures is about what you DON'T do with them because they don't do that well.

Linked lists make insertions into their middle easy.  They make traversing hard.

Stacks and queues prefer to do insertions and removal at an end.

None make it impossible to do anything since you can rebuild the entire structure.  The issue is the cost that comes at.

One thing that's helped me along the way is this little guy:



Here's one that includes the less popular structures:



Under the hood it's all one big array called random access memory.  Using these structures doesn't change that.  But if you can predict what you don't need you can choose the right structure that will help you use that memory very well.


[Answer]: I love CandiedOrange's answer though a queue or stack could be implemented using a linked list, or an unrolled linked list, or a fully contiguous array representation, or something else.

But I want to wholeheartedly agree and echo and hone in on a part which is really about designs doing less. Minimalism is very beneficial to a design in all kinds of ways, from having fewer ways to misuse a design to providing it more room to be implemented efficiently, to even expressing your requirements better in code that uses a particular design (one that does less tells the readers of your code all the things you aren't going to be doing).

LIFO stacks and FIFO queues typically do less than, say, a doubly-linked list, and that doesn't make their designs inferior whatsoever. In a number of cases, it could make them superior. And that applies to tangible objects as well in the real world. For example, someone might ask: "why use one of these"?



... "when you have one of these?"



And it should be pretty obvious in this case. But that also applies to designs in software. Doing more isn't necessarily equivalent to superior. When it comes to daily efficiency, it's also not always about using cutting-edge algorithms and parallelized, vectorized code. Sometimes it's just about making sure you aren't paying for things you don't need. Using designs which provide way more than what you need will often extract that kind of cost, like that multitool above when all you really need is a sharp knife to cut sashimi.

And naturally when you state your design requirements clearly and say, "All I need is a sharp knife to cut sashimi", then the implementers can come up with far more efficient, more reliable implementations in a short period of time for that narrow set of requirements than if you said, "Uhh, I don't know what I need. Here's a list of all the possibilities of things I may or may not need," at which point they might take ages and then hand you that analogical multitool. And naturally if I see you carrying a sushi knife around, I can more easily deduce that you're probably going to be dicing up fish and figure out your intentions than if you were carrying a multitool around.


[Answer]: Echoing @rwong comment, using a Queue or Stack


Documents the contract
Is a better name - might avoid the need for a comment
Makes your intentions clear.



------------------------------------------------------------

ID: cs.stackexchange.com:28496
Title: Using singly linked list instead of a doubly linked list?
Body: Are there advantages of implementing a singly instead of a doubly linked list other than space?


[Answer]: Other than saving space, the first advantage I can see for singly
linked lists over doubly linked lists is to avoid having to update two
links instead of one when you modify the list, when adding an element
for example.

Of course, one might say that you can always ignore the second link,
but in that case, it is no longer doubly linked, but only an unused
field.

What might also be seen as an advantage is that singly linked list are
necessarily consistent, though possibly wrong when there are bugs in
the program.  Doubly linked lists can end-up having inconsistent links
forward and backward. But it is debatable which of the two cases, if
any, is an advantage.

update:

I had tried to see some impact on garbage collection, but found
none. Actually, there is one when storage reclamation is done by
reference counting, as reference counting is defeated by loops, and
doubly linked lists contain loops. Singly linked list escape the
problem. Of course, there are ways around it for doubly linked lists,
such as the use of weak pointers, or programmable reference counting
as part of data abstraction.


[Answer]: Yes. Singly linked lists are easier to work with in highly concurrent situations, because there's less data to keep consistent.

For example, suppose you want to append a lot of items to a linked list in a wait-free way. It's okay if consuming the items is not wait free, but producers absolutely must not block no matter what the other threads are doing.

With a singly linked list you can just do:

var newNode = new Node(someValue);
var prevNode = Interlocked.Exchange(ref _insertionNode, newNode);
prevNode.next = newNode; //note: next is volatile; this is a write-release


Easy! Guaranteed progress for every producer. Produced  nodes may not be immediately visible from the head of the list (a previous producer may have yet to update the next pointer of the node it got), but they eventually will be.

With a doubly-linked list, you need CAS loops and other try-retry-retry-fix-up sorts of constructs. It's a lot easier to make a mistake, so I won't try to just zip off an example.


[Answer]: Single-link lists are easier to work with, and in many cases are adequate for the problem, but any application requiring moving both ways on the list requires a double-linked list. Presuming you were writing a text editor where each line was a dynamically generated string (so you don't waste tons of memory allocating strings much larger than any specific line normally is), in order to move, say, up to the previous line, or up or down a screen, you'd have to be able to quickly traverse the pointers to the next/previous line (or page/screen) in memory.
Also, a double-linked list does not have to be a loop. Each entry has a "prev" and "next" pointer, prev pointing to the prior (previous) entry, next pointing to the next one. The top of the list has a null prev pointer, the last one has a null next pointer.


------------------------------------------------------------

ID: cs.stackexchange.com:141315
Title: Return Linked List elements as an array
Body: Out of pure curiosity, if I have a Doubly Linked List, what would be the fastest way to return the elemenmts of the List in the correct order as an array? For example, would it be more efficient to create an empty list an append the elements or create a list with the length of the Linked List and insert each element?


[Answer]: Since you haven't mentioned if the elements have to be sorted into an array, I am assuming you just want to copy the elements of the list into an array.
Well, this can be done by pointing to the head and copying the data of the head (head->data) into the array one by one and deallocating the node and moving on to the next node, and repeat.
Pseudo-code:
 i = 0
 ptr = &head
 arr[i] = head->data
 head = head->next
 free(ptr)
 i++ until i = length of list
 



[Answer]: In terms of computational complexity, both approaches will be $O(n)$. Also, its impossible to do better than $O(n)$, since you must always go through all elements in the linked-list anyways.
So in those terms, the running times are equivalent. What about the space (memory) complexity? Well, in both cases we ultimately create an array of length $n$ and nothing more than it. So, both approaches will have also $O(n)$ memory complexity.

What is the difference then? The key difference is not in any complexity measure, but it has to do with how memory is allocated. In the second approach, you have to create an array with some constant size, and you wont change that size. Hence, you will allocate memory only once. In the first approach, you create a (dynamic) list, which uses a less-efficient data structure (when compared to an array with a constant size) that can increase in size. Additionally, since this array increases in size it means that it will allocate memory more than once.

So ultimately, what do you want to use? the second implementation would be a better option - even though it doesn't improve in asymptotic complexity measures.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:370125
Title: Linked-list iteration patterns
Body: If you have a linked-list, where the items are not necessarily close to each other in memory, wondering if it is (in general) better/worse/no difference to do the following.

Say you want to iterate through the items 2 or 3 times. One solution is to just iterate through them each time, finding the pointers one at a time with right or next. Another solution is to create a local temporary array filled with pointers to the items, and iterate through that the second/third times. The values are still in their normal spot. A third solution is like the second but you also copy the values (say they are numbers not arbitrary strings). Perhaps there are other better alternatives. The thinking is that you would somehow take advantage of memory locality for caching. The lists can be as small as 1 item to as large as a few thousand. I am new to the memory stuff.


[Answer]: Caching effects are difficult to predict. In general, contiguous memory data structures like arrays of values are more cache friendly, but does this matter? Not for most code.

For the purpose of iteration over the pointed-to values, an array of pointers is very similar to a linked list which you traverse by pointer chasing. Note that arrays of objects in most OOP programming languages are arrays of pointers (e.g. Java, C#, Python, …), and their performance is generally fine.

While a linked list does not require that the list nodes are adjacent in memory, this can still often be the case. E.g. when using an arena allocator and/or when the list nodes were allocated in their iteration order at roughly the same time, they might have array-like cache behaviour. Any clever optimizations would then have all the overhead of many small copies, without noticeable gains.

So whether any clever optimization would make a noticeable difference can only be answered reliably by running a realistic benchmark. I once encountered a case where simplifying a collection for repeated iteration did make a big measurable difference, but that was in the absolute hot spot of a very computationally expensive program. Most programs do not have such hot spots where nanosecond-scale savings are multiplied into noticeable speedups.

Do prefer cache friendly data structures where easily possible, but quite often that is not possible. E.g. in C++, vector is often “better” than a list, but adding a new element to a vector can invalidate any pointers to elements. So if I need stable pointers, then memory locality be damned – I need a vector>or a list. Also, lists can do many things that vectors cannot, e.g. O(1) removal or O(1) insertion at the front. 

Correctness trumps performance. At scale, algorithmic complexity trumps cache effects.


[Answer]: In response to genuine hotspots, it can be a useful optimization at times to especially apply the third solution you proposed which creates a contiguous array of elements that are stored by value (ex: numbers, not variable-sized sub-sequences like strings), and more so if the resulting array doesn't need to store all the data of the original list (some fields might be irrelevant to subsequent usage).

For variable-sized subsequences it can be helpful sometimes too to transfer to a flat container, like std::vector, with null terminators to terminate one string from the next if the subsequent and repetitive access patterns are sequential. That might not be as helpful these days now that strings use SBOs, but there was a time long ago at least where that was enormously helpful in my cases that involved repetitive UTF8 string processing to just flatten it all out to one giant array of characters.

That said, the cases where I benefitted from such optimizations did not traverse the linked structure (tree in our case) 2 or 3 times. It was more like 100+ times a second with the original data in the linked structure not changing very frequently at all (not invalidating the data cached into a contiguous array), and on top of that it enabled us to do things like SIMD processing with the resulting array as we repeatedly looped through it.

One the biggest examples I can recall where that sort of optimization was useful was a motion hierarchy evaluation. To compute the resulting matrices of child matrices down a hierarchy, we had to traverse the motion hierarchy (tree) in breadth/level-first order to transfer motions from parent to child. There, it was very helpful to frame rates to just grab an array of elements out of that level-first traversal and use that array sequentially instead of traversing the tree over and over every single frame. In that case the motion of the items was dynamic and frequently changing, but not the structure of the tree (very infrequently changed). And we did only do that after seeing hotspots and cache misses there in VTune.

Also as pointed out linked structures don't necessarily have to result in a loss of spatial locality. You can even write linked lists using arrays where the array elements just store like a next 32-bit index to skip to the next part of the array storing the next element, and use like -1 in place of a nullptr. Of course the next index for element 40 might might skip to the 35,000th position in the array if you start inserting and removing all over the place in the middle of the list, but then you can restore the spatial locality with a simple copying pass which traverses the original list using those links and then inserts to the new list in order (in which case all neighboring nodes would be right next to each other in memory/array in the new copy). I often use that rep these days as I find it far less cumbersome than having to deal with separate allocators like free lists; in general I don't like to deal with memory allocator and data structure as two separate things to deal with in usage if I can help it.

Finally I could imagine even a case where traversing the list one time to copy to array only to traverse the array one time might be beneficial if the work done per iteration is hefty and capable of being done in parallel. The array, with its random access, allows that loop to be parallelized, whereas the linked list traversal is serial in nature.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:344354
Title: Designing an efficient implementation of a random access queue based on a linkedlist
Body: I'm currently working through Robert Sedgewicks "Algorithms in java" (3rd edition, german) on my own and am currently at one of the more complicated questions there. I think I may have the starting point for a solution, but I'm not sure if its really the best way to go. 

The exercise (translated):

Create an abstract datatype of a random-access-queue: Write an interface and an implementation that uses a linked list as basic data structure. Be as efficient as possible in the implementations of "put" and "get" methods and analyze their cost in a worst case scenario. 

For clarity: The put method inserts an element at the end of the queue. Only the get method has a random part, in which it accesses a random part of the linked list, returns its value and removes the element from the linked list.

My ideas so far:

1. Implement linked list using node objects

This is the more obvious implementation, implementing a linked list using Node objects that contain a value (may be primitive data type or an Object)  and a reference to the next Node. Then have a "head" and a "tail" reference stored in the class of the queue implementation. 
The get method in this case would first calculate a number between 0 and "queueSize" (the number of elements in the queue) and then go from node to node (starting with head) using the reference in "next" "queueSize" times, return the value of the node you land on and then remove it from the list. 

However, I doubt that is the most efficient method and don't see a way to make it efficient.

2. Implement linked list using an array of node objects

Implementing the linked list of the queue by using an array of nodes, node objects again containing a value (may be primitive datatype or an Object) and a reference to either another node or null. One could then access a random node by randomly generating an index between 0 and the array length and accessing the Node object in that cell. Problem here obviously lies in the fact that not every Node in the array is going to be part of the linked-list at all times. And I don't see an efficient/quick way to get that information.

Therefore my question is, what implementation of the linked list is the way to go here? Is there a way to make sure a randomly generated number in an array-based linked-list points only at array cells that are part of the linked list? Or is there a way to write an efficient get-method for the first approach?


[Answer]: Because a put operation only adds elements at the end of the queue, you can easily and efficiently implement it by maintaining a reference to the last node of the list.

Using arrays can help you to decrease the time needed to access a random element, but they will only divide this time by a ~ constant factor. So it doesn't reduce the complexity.

I would try an approach based on skewed binary random access lists. Technically, these are not lists anymore, but trees that represent lists. Given the exercise, I would introduce them as "lists represented by linked nodes".

The structure of the traversed tree mirrors the positions of the contained values, in such a way you can convert an arbitrary value position into a binary representation, and traverse the tree according to this representation to reach/set the value corresponding to the specified position.

Inserting and retrieved the value at a particular position are O(log n) operations. Even better, insertion and retrieving at the end (or the beginning) of the list are O(1) operations. 


[Answer]: This approach might only be more efficient for very large linked-lists that are only moderately dynamic. If we implement the linked-list using an array of Nodes (called nodeArray), it is possible to keep track of all indices of the nodeArray using a second array int[] of the length nodeArray.length*2 (called indexArray) together with the int 'start' and int 'end'. The nodeArray also needs an int 'listSize' that keeps track of the length of the linkedList (initialized at 0, incremented if put() is called, decremented if get() is called.

start is always the index of the cell containing the head of the linked-list. end is always the index of the cell after the tail of the linked-list. A cell with a value of indexArray.length between 'start' and 'end' indicates a cell that used to contain an index to a cell in nodeArray, but that Node got removed by the get method.  All cells between start and end (including index start, excluding index end and cells with a value of indexArray.length) are all indices of the nodeArray that make up the linked-list in correct order.

This array needs the following methods accessing it:


The put method, besides putting a new Node containing a value at the end of the linked-list in nodeArray, now also needs to put the index in which the new Node is put in the nodeArray in indexArray[end] and increment 'end' by 1. 
The get method generates a random number rng between 0 and listSize. It then iterates through the indexArray, starting from 'start' and going through rng iterations, not counting cells it encounters with a value of indexArray.length - this should be faster than iterating through a normal node-based linked list. 


This has the following problems: 


The put method always increases the amount of cells between 'start' and 'end' by 1 every time it gets invoked while no method reduces them (get only sets a cell between the two to indexArray.length). This eventually will cause in indexOutOfBoundsException, forcing the insertion of additional code that creates a new array of double the size and then copying over the content of all cells in indexArray into this new array, ignoring all cells with a value of indexArray.length of course. 
Since you still iterate through an array, the complexity stays the same. So this only gains some speed by the fact that iterating through indexArray is faster than through a normal linkedList but you also lose some through the more clunky put and get methods in addition to the memory you need to sacrifice to make the indexArray. 


Therefore this solution might only be marginally better in the case of huge linked lists and definitely not if the initiation size of indexArray was chosen badly. 


[Answer]: Warning: this is not something that you'd ever want to do. But I'm guessing it's in the "Creative Problems" section (looking at the fourth edition non-Java-specific version of Algorithms, it seems like it might be question 1.2.38 -- which refers the reader to chapter 3 for possible solutions).

OK, with that out of the way, what are some basic constraints on the problem?


To find an element M in a linked list of size N, you must linearly search each element in the list. While M 
In order to reduce the big-O runtime, we must find another way to access the nodes in the list.
A divide-and-conquer algorithm is O(logN)
An array is O(1), but if you had that, you wouldn't need the linked list.


So, how do you divide-and-conquer a linked list? The answer: each non-leaf element in the list contains another list.

L0
+--L1
|  +--E0
|  +--E1
+--L2
   +--E2
   +--E3


So, the top-level list holds two child lists, and those child lists hold the actual elements. We need a couple more constraints:


Any list L knows (1) whether it contains sublists or elements, and (2) the total number of elements held by all descendents.
Any list L has a limited number of children. In this case 2, but it could be any number. This is critical to the big-O evaluation: any set of operations that has a fixed size not dependent on N is equivalent to O(1).


So, let's look at how this works to find element E2:


Start with node L1, which we know is a list that contains 2 elements.
Since E2 is the third element, move on to node L2.
We know that L2 has two children, and they're elements, so we can return element E2.


It may not be immediately obvious that search is a logN operation. If that's the case, then take a piece of paper and draw out a 14-element list.

Adding elements to this list is also an O(logN) operation, unlike a normal linked-list (which is either O(1) or O(N) depending on whether it's a singly- or doubly-linked list). Here's how it works:


Find the last descendent sublist. This is again a recursive operation (so logN), which for every list element follows its right child.
If that descendent sublist has only 1 element, add the new element, and recompute the sizes of all parents.
If the sublist is full, you need to add another child to its parent; this is also a recursive operation, up the tree of lists.
You might have to add a new root element, where the left child is the previous root and the right element is the new element.


Confused? Here's a walk-through of adding the elements to the list shown above. It starts with a single element, which is the left child of the root (I'm keeping the numbering the same as in the 4-element list).

L1
+-- E0


When we add the next element, we can store it in the right child of the root:

L1
+-- E0
+-- E1


At this point our list is full, so we have to add another level. The original root of the list is the left child of the new root, and we add another list as the right child. This new list (L2) is where we put element E2.

L
+-- L1
|   +-- E0
|   +-- E1
+-- L2
    +-- E2



[Answer]: Sorry for the late answer but I think this will give O(1) insertion, removal and random access.

Store the linked list nodes in a pre-allocated array.

Addition involves adding the new node to the first free slot in the array.

Removal is the complicated part. It may leave an empty slot in the middle of the array. However, this free slot can be immediately filled with the node that is currently at the end of the array. So the node at the end is moved into the free slot, thus ensuring that nodes are always at sequential locations in the array. For this to work when given a node as input, each node will need to store its index in the array.

So o(1) random access is given because the nodes are always at sequential locations in the array.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:419681
Title: Why the names Omega and Theta in Big Omega and Big Theta?
Body: There was a question asking what the "O" stands for in "Big O", the answer to which seems to be "Ordnung von"/"order of".
I wonder what's the origin of the names "Omega" and "Theta" in the "Big Omega" and "Big Theta". Or are they somewhat randomly chosen without an inherent meaning.


[Answer]: 
O was introduced in Bachmann, Paul (1894). Analytische Zahlentheorie. Bachmann is using the term "Ordnung" (order of) multiple times to refer to the growth rate of functions, and introduces the symbol O, but there is no explanation given why O was chosen, and he never explicitly or implicitly draws a link between the term "Ordnung" and the symbol "O". It is also, I believe, not 100% clear whether that is meant to be a Latin O or a Greek Omicron.

o was introduced in Landau, Edmund (1909). Handbuch der Lehre von der Verteilung der Primzahlen. I didn't find the specific place where it is defined in the book, unfortunately.

Ω was introduced in Hardy, G. H.; Littlewood, J. E. (1914). Some problems of diophantine approximation: Part II. The trigonometrical series associated with the elliptic ϑ-functions. It is simply introduced without explanation for the letter chosen.

Ω was redefined in Knuth, Donald (April–June 1976). Big Omicron and big Omega and big Theta. Knuth thought the definition of Ω used by Hardy and Littlewood was inconvenient for Computer Science and that the symbol was not used much anyway (on this point he was wrong unfortunately, so that there are now two mutually incompatible definitions of this symbol), so he chose to redefine it. All he has to say about the choice of symbol is:

I like the mnemonic appearance of Ω by analogy with Ο, and it is easy to typeset.

By the way, note that Knuth refers to "Big Οmicron" in the title.

Θ was defined in Knuth, Donald (April–June 1976). Big Omicron and big Omega and big Theta. (The same article that redefined Ω.) All he has to say about the choice of symbol is:

Furthermore, these two notations as defined above are nicely complemented by the Θ-notation which was suggested to me independently by Bob Tarjan and by Mike Paterson.

However, it is not clear whether he is referring to the symbol or the definition. Since he does not cite any sources, we may assume this was not in a published paper but rather private communication. We may assume that Θ was chosen because it also kinda sorta looks like Ο, but there is nothing in the article to support that theory.


So, in short, other than Knuth's "Ω looks like Ο", we don't know for any of those symbols.


------------------------------------------------------------

ID: cs.stackexchange.com:91228
Title: What is the relationship/difference between best/worse/expected case and big O/omega/theta?
Body: In the big O section of Cracking the Coding Interview 6th edition, I read the following.

"Best, worse, and expected case describe the big O for expected inputs and scenarios."

"Big O, big omega, big theta describe the upper, lower and tighter bounds for the run-time."

I understand what best, worse,  and the expected case is describing.

I know what bounds are but still don't understand the decoupling of the two concepts. Why would an algorithm have different bounds if the inputs/scenarios are the same?

Thanks in advance!


[Answer]: 
  Why would an algorithm have different bounds if the inputs/scenarios are the same?


A deterministic algorithm has a single running time on each input. However, this is not what best case, worst case and average case are about. Let us consider a deterministic sorting algorithm. Its running time on an array of length $n$ depends on the array (and on the algorithm). 
The best case running time is the minimal running time of the algorithm on an array of length $n$. The worst case running time is the maximal running time of the algorithm on an array of length $n$. The average case running time with respect to a distribution on arrays of length $n$ is the average running time of the algorithm on an array chosen at random from the given distribution.

For a randomized algorithm, it makes sense to consider the best case, worst case and average case for a single input, but this is not what these terms usually refer to.


  "Best, worse, and expected case describe the big O for expected inputs and scenarios."


This is simply wrong. I have explained above the meaning of best case, worst case and average case running time for the particular case of sorting algorithms, and the general case is similar.

How is running time measured? One way to measure running time is via an experiment – choose an actual machine, code the algorithm in a particular way, and measure the running time. However, this approach is problematic – which machine should we run the code on? How should we implement the algorithm? Another problem that the exact formula for the running time might be very complicated, and so not very helpful.

The way out is to state the running time up to a constant factor. This makes sense since under reasonable assumptions, the exact machine used and the exact implementation only affect the running time by a constant factor. Moreover, this allows us to hide the complexities of the exact running time by sweeping them under the rug of asymptotic notation.
(As an aside, we measure the running time on an abstract machine known as the RAM machine, which has unbounded memory. This allows us to make sense of arbitrary input sizes.)

Consequently, we always quote the running time in asymptotic notation. It is common to use big O notation, but this is in fact only an upper bound on the running time. Merge sort, for example, runs in time $O(n^5)$. It also runs in time $O(n\log n)$, and this upper bound is tight, in the sense that the worst case running time is $\Theta(n\log n)$.
When quoting the running time as $O(f(n))$, what one usually means is that the worst case running time is $\Theta(f(n))$. The reason we use big O rather than big $\Theta$ is that on some inputs the algorithm may run faster, and so for arbitrary inputs it may not be correct that the running time is $\Theta(f(n))$, but it is always correct that the running time is upper-bounded by $O(f(n))$.


  "Big O, big omega, big theta describe the upper, lower and tighter bounds for the run-time."


Big O, big $\Omega$ and big $\Theta$ are general-purpose asymptotic notation. They can be used to express asymptotic estimates on the rate of growth of arbitrary functions. Even in computer science, it is common to use asymptotic notation not only to measure running time, but also to measure memory consumption; and in other fields asymptotic notation is used for functions not related to algorithms at all.


------------------------------------------------------------

ID: cs.stackexchange.com:103533
Title: Sorting elements of linked list
Body: I've been looking over sorting algorithms for linked list and here's what I came up with.

As we know, for element access arrays will work better than linked list. Also, there is the benefit of elements being stored in consecutive memory locations: due to spatial locality of cache, arrays give better performance.

Thus for sorting linked list:


Extract all elements from linked list to a array: $O(n)$
Sort the array using quicksort: $\Theta(n \log n)$
Store the elements in linked list: $O(n)$ 


Is this approach better than other approaches for sorting linked lists such as using merge sort directly, as described on geeksforgeeks.


[Answer]: The downside of your algorithm is that you need to allocate enough space to hold pointers to all the elements in the list. This requires knowing the length of the list (which would need traversing it once to count) and you may not have enough memory to get a contiguous space for it.



A variation of classic merge sort will work a lot better for linked lists:

Have a array of $\lceil \log_2 n \rceil$ linked lists initially empty. (or just fix to size 64 because when are you ever going to need to sort anywhere near $2^{64}$ elements).

For each element in the original list you extract it into a single element list and do the following:

for the first list in the array if a list is already there merge the list into that list and remove the list from the array and repeat for the next list in the array. 

otherwise move on to the next element in the original list.

Once you've done the last element of the list you merge the lists in the array starting from the lowest index.



This is the algorithm used for linked lists in the linux kernel. And has a few handy features:


no allocations needed, no recursion, so no chance of an out of memory situation. 
you only walk the original list once.
the merge passes except the final ones are all with lists of equal length. This means there are few unbalanced passes which would have more traversing of the longer list than actually merging the two lists together in.



------------------------------------------------------------

ID: softwareengineering.stackexchange.com:373387
Title: Adjacency List list of linked list or can be repesented in other ways?
Body: I am new to graph data structure. Everywhere on google  it is said to be list(or array ) of linked list. 
My question is can not it be represented as list of list(in java array list of array list) or map of list(in java HashMap with key as node and value as array list of connected node ) ?

In all three mentioned approach I see time complexity  

To find if two nodes are connected - O(v)
To find all connected nodes - O(v)

Also space complexity will also be more or less same .

So why  Adjacency List  is said to be list(or array ) of linked list not as list of list or map of list ?


[Answer]: A list of list or a map of list or a map of map are just fine for implementing an adjacency list. No real downside to any of them. I personally use a list of lists in Java whenever I need an unweighted graph and a list of hashmaps if I need a weighted one.

As for why Adjacency list is said to be array of linked lists, I haven't really seen this claimed as a golden standard anywhere. The wiki article mentions three common implementations and it includes both linked lists and dynamic arrays.


[Answer]: A List of List will be dynamic(in terms of number of Vertices) in nature and hence can be costly in terms of reallocation(as the default size of ArrayList in JAVA is 10).
Since we already know the size of a graph while constructing it. So it is better to use an array of List rather than using a List of List or a Map  of List. You can read more about the List in JAVA here. 


------------------------------------------------------------

ID: cs.stackexchange.com:56219
Title: How to find middle element of linked list in one pass?
Body: One of the most popular question from data structures and algorithm, mostly asked on telephonic interview.


[Answer]: By cheating, and doing two passes at the same time, in parallel. But I do not know whether the recruiters will like this.

Can be done on a single linked list, with a nice trick. Two pointers travel over the list, one with double speed. When the fast one reaches the end, the other one is half-way. 


[Answer]: Create a structure with a pointer capable of pointing to nodes of linked list and with an integer variable which keeps count of the number of nodes in the list.

struct LL{
    struct node *ptr;
    int         count;
}start;


Now store the address of the first node of linked list in $start.ptr$ and initialize $start.count = 1$.
Ensure that the value of $start.count$ is incremented by one after the successfull creation of a new node in linked list. Similarly, decrement it by one whenever a node is deleted from the linked list.

Use $start.count$ to find the middle element in one single pass.


[Answer]: Elaborating on Hendrik's answer

If it's a doubly linked list, iterate from both ends

function middle(start, end) {
  do_advance_start = false;
  while(start !== end && start && end) {
     if (do_advance_start) {
        start = start.next
     }
     else {
        end = end.prev
     }
     do_advance_start = !do_advance_start
  }
  return (start === end) ? start : null;
}


Given [1, 2, 3] => 2

1, 3
1, 2
2, 2


Given [1, 2] => 1

1, 2
1, 1


Given [1] => 1

Given [] => null


[Answer]: Create a dynamic array, where each element of the array is a pointer to each node in the list in traversing order, starting from the beginning. Create an integer, initialized to 1, that keeps track of how many nodes you have visited (which increments each time you go to a new node). When you get to the end, you know how big the list is, and you have an ordered array of pointers to each node. Finally, divide the size of the list by 2 (and subtract 1 for 0-based indexing) and fetch the pointer held in that index of the array; if the size of the list is odd, you can choose which element to return (I will still return the first one).

Here is some Java code which gets the point across (even though the idea of a dynamic array will be a bit wonky). I would provide C/C++ but I am very rusty in that area.

public Node getMiddleNode(List nodes){

    int size = 1;
    //add code to dynamically increase size if at capacity after adding
    Node[] pointers = new Node[10];

    for (int i = 0; i 


[Answer]: If it's not a doubly linked list, you could just count and use a list, but that requires doubling your memory in the worst case, and it simply won't work if the list is too large to store in memory.

A simple, almost silly solution, is just increment the middle node every two nodes

function middle(start) {
    var middle = start
    var nextnode = start
    var do_increment = false;
    while (nextnode.next != null) {
        if (do_increment) {
             middle = middle.next;
        }
        do_increment = !do_increment;
        nextnode = nextnode.next;
    }
    return middle;
}



[Answer]: By using 2 pointers . Increment one at each iteration and other at every second iteration. When 1st pointer points to end of linked list, went 2nd pointer will point to the middle mode of linked list .


[Answer]: Is recursion considered more than one pass?  

Traverse the list to the end, passing an integer count by reference.  Make a local copy of that value at each level for later reference, and increment the ref count going into the next call.

On the last node, divide the count by two and truncate/floor() the result (if you want the first node to be the "middle" when there are only two elements) or round up (if you want the second node to be the "middle").  Use a zero- or one-based index appropriately.

Unwinding, match the ref count to the local copy (which is the node's number).  If equal, return that node; else return the node returned from the recursive call.
.

There are other ways to do this; some of them may be less cumbersome (I thought I saw someone say read it into an array and use the array length to determine the middle - kudos).  But frankly, there are no good answers, because it's a stupid interview question.  Number one, who still uses linked lists (supporting opinion); Two, finding the middle node is an arbitrary, academic exercise with no value in real-life scenarios; Three, if I really needed to know the middle node, my linked list would expose a node count.  It's hella easier to maintain that property than to waste time traversing the entire list every time I want the middle node.  And finally, four, every interviewer is going to like or reject different answers - what one interviewer thinks is slick, another will call ridiculous.

I almost always answer interview questions with more questions.  If I get a question like this (I never have), I would ask (1) What are you storing in this linked list, and is there a more appropriate structure to efficiently access the middle node if there is truly a need to do so; (2) What are my constraints?  I can make it faster if memory is not an issue (e.g. the array answer), but if the interviewer thinks bulking up memory is wasteful, I'll get dinged. (3) What language will I be developing in?  Nearly every modern language I know of has built-in classes to deal with linked lists that make traversing the list unnecessary - why reinvent something that has been tuned for efficiency by the developers of the language?


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:276761
Title: Is a linked list an actual data structure?
Body: I hear conflicting sides from this topic. I was under the impression that a linked list was a way to implement a data structure (stack, queue), not actually a data structure.


[Answer]: Alas! java.util.LinkedList is not a data structure by itself.

Does this mean that an array isn't a data structure either? or binary tree (because that's used to implement a SortedMap or a Heap)?

Ask yourself: "Is it data? is it structured?" if the answer to both of these is "yes", then, well, you have a data structure of some sort. Give the data structure Wikipedia page a read.

That a given structure is used to implement other things doesn't mean that it isn't a structure itself.

Arrays are used to implement flexible arrays.  Flexible arrays are used to implement the hash table part of a map (or dictionary if your language uses that jargon instead).  Linked lists may be thought of (kind of wrong, but bear with me) as specialized form of a tree (one branch only). Trees are used to implement... well, heaps, sorted maps, b trees (which are often found in indexes in databases and file systems).  The list can can go on and on and on (and that isn't a complete list).

Some of these things that I mentioned are fuzzy in their implementation. One can implement a Map with a hash table (backed by an array) - a HashMap, a red-black tree (a particular implementation of a balanced binary tree - just be glad its not an avl tree) as a SortedMap, or even linked lists as a ConcurrentSkipListMap. The key thing is how they behave - all of these implementations implement a set of the same methods guaranteeing they all have a get(K) and put(K, V) (along with some other things).

The linked list (concrete implementation) is a particular way of implementing a List (an abstract data type). You could also do it with a flexible array (another concrete implementation). 

That one structure is used as the foundation for another structure does not make it less of a structure.  The key point in the data structure is that there is data, and it is structured in some way so you know where to look for its components.


------------------------------------------------------------

ID: cs.stackexchange.com:154228
Title: Are linked lists homogenious or heterogenious containers?
Body: I have always assumed that "Linked List" is something in the lines of
struct Node
{
    SomeClass payload;
    Node * next = nullptr;
    Node * prev = nullptr;
}

This implies the restriction of working with objects of only one type. Considering lists are horrible in access time I doubt anyone really constructs them on the stack. Arrive the conclusion that each node could be of different size. This probably possible with std::any. But in pure C one would need an additional redirect void * payload; or to loose type safety by converting the whole Node * to void *. In other words: possible but difficult.
Is there some inherent characteristic of lists which mandates they be as restricted as arrays or am I observing pure coincidence?


[Answer]: You say "linked list" without mentioning a computer language.
First, a "Node" is not a linked list. It is a helper struct that may be useful to create a linked list, but it is just a wrapper around some thing with additions so it can be made part of a linked list.
In Objective C or in Swift, you'd declare Node as something based on SomeClass, say in Swift struct  Node { ... }. An important change that you would make: You'd put the two Node* first. That's because this means the Node* are always at the same offset; there will be many situations where the generated code will be identical for different classes, and Swift guarantees that code is not duplicated in such a situation.
In both languages, you can also declare the payload to be of type "id" in Objective-C, or "kindof whatEverClass" in Objective-C, or "Any" in Swift. In that case the payload can be anything (including different types) in Swift, and any object pointer, or any subclass of whatEverClass in Objective C.
In general, if you want a class "linked list", you just make sure it implements what is needed, like first, last, next, previous, insert, remove, lookup. The implementation is up to you. If I had to implement it for general use, I'd probably implement it using a series of arrays. So if you start with an empty list, one array would be allocated if you only add elements and remove items at the start or the end of the list. Operations in the middle of the list could allocate a second array. For the user, it would behave like a linked list, just more efficient then a naive linked list.


------------------------------------------------------------

ID: cs.stackexchange.com:74122
Title: Why does radix sort work?
Body: I understand how radix sort works and how to implement it, but I don't understand why it works. 

Are there any underlying mathematical or logical principles that it relies on? 


[Answer]: Radix sort sorts numbers by sorting on the least significant digit first. This is somewhat counterintuitive compared to the rather straightforward method by sorting on the most significant digit first.

The key point to radix sort is that the digit sorts used in each iteration of radix sort are stable: numbers (digit here) with the same value appear in the output array in the same order as they do in the input array. Given this, you can convince yourself of the correctness of radix sort by simply observing how a set of two-digit numbers are sorted. (Actually, we are using mathematical induction on the number of digits. However, a simple example with a set of two-digit numbers suffices here.)

In the first iteration, the numbers are sorted by their least significant digits. Now consider the second iteration which then sorts these numbers by the leftmost digits. Given two number $a = a_1 a_0$ and $b = b_1 b_0$, if $a_1 \neq b_1$, then after the second iteration, they are in the right order. If $a_1 = b_1$, then we know that the order between $a$ and $b$ should be consistent with $a_0$ and $b_0$. This is the case because the digit sort used by the second iteration is stable.


------------------------------------------------------------

ID: cs.stackexchange.com:100223
Title: Why does Radix sort require stable digit sorts?
Body: I'm reading the CLRS book and have a question about the following quote from the book. 


  In order for radix sort to work correctly, the digit sorts must be stable.


Why is stability required? Wouldn't Radix sort still produce correctly sorted output even if there was no consistency in ordering of identical values, since Radix sort sorts on every digit position?


[Answer]: Integers that are equal in some digit aren't always equal (e.g., 12 and 13).

Let's sort [13, 12] using a LSD-first radix sort (base 10). But we'll use an unstable sort for each digit.

First we sort by the 1's place, so the array becomes [12, 13].

Now we sort by the 10's places, but 12 and 13 have the same digit. Since the sort is unstable, the resulting array could be [12, 13] or [13, 12]. We don't know.

If the sort were stable we would be guaranteed to get a correctly sorted array.


------------------------------------------------------------

ID: cs.stackexchange.com:5030
Title: Why is Radix Sort $O(n)$?
Body: In radix sort we first sort by least significant digit then we sort by second least significant digit and so on and end up with sorted list. 

Now if we have list of $n$ numbers we need $\log n$ bits to distinguish between those number. So number of radix sort passes we make will be $\log n$. Each pass takes $O(n)$ time and hence running time of radix sort is $O(n \log n)$ 

But it is well known that it is linear time algorithm. Why? 


[Answer]: Be careful with your analysis: what do you assume to make sorting run in $O(n)$ time? This is because each of your digits are in a range from $0$ to $k-1$, meaning your digits can take on $k$ possible values. You need a stable sorting algorithm, so you can for example choose counting sort. Counting sort runs in $\Theta(n+k)$ time. If $k=O(n)$, counting sort runs in linear time.

Each one of your strings or numbers have $d$-digits. As you say, you make $d$ passes over them. Hence, radix sort clearly runs in $\Theta(d(n+k))$ time. But if we consider $d$ to be constant and $k=O(n)$, we see that radix sort runs in linear time.


[Answer]: 
if we have a list of $n$ numbers we need $\log n$ bits

No: if we have a list of numbers between $0$ and $2^k - 1$, we need $k$ bits. There is no relationship between $k$ and $\log n$ in general.
If the numbers are all distinct, then $\log n \le k$, and radix sort on distinct numbers therefore has a time complexity of $\Omega(n \log n)$. In general, the complexity of radix sort is $\Theta(n \, k)$ where $n$ is the number of elements to sort and $k$ is the number of bits in each element.
To say that the complexity of radix sort is $O(n)$ means taking a fixed bit size for the numbers. This implies that for large enough $n$, there will be many duplicate values.

There is a general theorem that an array or list sorting method that works by comparing two elements at a time cannot run faster than $\Theta(n \log n)$ in the worst case. Radix sort doesn't work by comparing elements, but the same proof method works. Radix sort is a decision process to determine which permutation to apply to the array; there are $n!$ permutations of the array, and radix sort takes binary decisions, i.e. it decides whether to swap two elements or not at each stage. After $m$ binary decisions, radix sort can decide between $2^m$ permutations. To reach all $n!$ possible permutations, it is necessary that $m \ge \log (n!) = \Theta(n \log n)$.
An assumption in the proof that I did not write out above is that the algorithm must work in the case when the elements are distinct. If it is known a priori that the elements are not all distinct, then the number of potential permutations is less than the full $n!$. When sorting $k$-bit numbers, it is only possible to have $n$ distinct elements when $n \le 2^k$; in that case, the complexity of radix sort is indeed $\Omega(n \log n)$. For larger values of $n$, there must be collisions, which explains how radix sort can have a complexity that's less than $\Theta(n \log n)$ when $n \gt 2^k$.


[Answer]: I think the assumption $k = \log_2(n)$ is wrong. You can perform radix sort with numbers in, e.g., hex. Thus, at each step you split you array of numbers into $16$ buckets.


------------------------------------------------------------

ID: cs.stackexchange.com:49427
Title: Which pass do you look at for Radix Sort stability?
Body: I know this is a fairly poorly worded question, but I can't think of a better way to phrase it in the title. 

So in Radix Sort, you go digit by digit from least significant to most significant, and along the way you are swapping elements around. When considering the case where two digits are the same, do you look at the original array to decide which should come first, or do you look at the previous iteration?

In one case you would have to store the original to look back on, so my guess is it's not that, but I can't tell which provides the correct result by my own examples. Let me know if I need to describe my question better and I will try. 


[Answer]: Radix sort always uses the results of the previous pass, not the original array.  It does not attempt to store the original value of the array.  You can find a description of radix sort at Wikipedia or in standard algorithms textbooks, and you'll see that it has this property.

As Wikipedia describes, radix sort is stable if you sort the digits starting from least significant and moving on to most significant.


------------------------------------------------------------

ID: cs.stackexchange.com:6871
Title: how does the parallel radix sort work?
Body: I'm currently learning computer science, and there is a slide of notes brief described the parallel radix sort under data parallelism.

number 101 110 011 001 111 (1st bit)
order    2   1   3   4   5 (new order)
number 110 101 011 001 111 (2nd bit)
order    3   1   4   2   5 (new order)
number 101 001 110 011 111 (3rd bit)
order    3   1   4   2   5 (new order)
number 001 011 101 110 111


I roughly know how to sort it from lecturer's explanation, but how is it related to parallel computing to increase the performance?


[Answer]: It turns out that within each round of radix sort, we can take advantage of parallelism. We need to reorder the keys (in a stable manner) according to the relevant bit. The simplest to do this in parallel would be as follows:

/* perform one round of radix-sort on the given input
 * sequence, returning a new sequence reordered according
 * to the kth bit */
function radixRound(input, k):
  front = filter(input, kth bit is zero)
  back = filter(input, kth bit is one)
  return concatenate(front, back)


In this approach, on each round, we "filter" the sequence twice. The first filter selects the subsequence of elements whose kth bit is zero. The second filter similarly selects the elements whose kth bit is one. We then complete the round by returning the concatenation of these two sequences. Here's a trace of your small example:

round 0:
input = [101, 110, 011, 001, 111]
front = [110]
back = [101, 011, 001, 111]

round 1:
input = [110, 101, 011, 001, 111]
front = [101, 001]
back = [110, 011, 111]

round 2:
input = [101, 001, 110, 011, 111]
front = [001, 011]
back = [101, 110, 111]

round 3:
input = [001, 011, 101, 110, 111]
(done)


Now all we have to do is explain how to do filter and concatenate in parallel. Assuming sequences are just implemented as arrays, concatenate is pretty simple. All we have to do is allocate an output array of the appropriate size and then, in parallel, write all the elements out to this new array:

function concatenate(a, b):
   n = length(a)
   m = length(b)
   result = allocate(n+m)
   for i from 0 to n+m do in parallel:
     if i 

Implementing filter in parallel is not so immediately obvious. The basic idea which is typically used in practice is to do a parallel prefix sum to count, at each position, how many elements satisfy the predicate preceding that position. This gives you the index of each surviving element of the output, so that in parallel you can write them into an output array.

With this implementation, you can do each round of radix sort in $O(n)$ work and $O(\log n)$ parallel time. This gives you a work-efficient radix sort with total work $O(wn)$ and parallel time $O(w \log(n))$, assuming a bit width of $w$. In practice, you would want to optimize this implementation to not allocate too many intermediate arrays, and to hopefully only make a single "pass" over the input on each round.


[Answer]: There are many ways to do it. The following approach allows to fairly split work between many cores. I believe that it's used even in GPU implementations of radix sort, such as ones provided by Boost.Compute and CUDA Thrust.

I describe here one pass of LSD radix sort that distributes data into R buckets:


First stage: split input block into K parts, where K is number of cores sharing the work. Each core builds histogram for one part of data, counting how many elements from this part of data should go into each bucket: Cnt[part][bucket]++
Second stage: Wait till all cores finished the stage one, and then compute partial sums over counts, thus revealing an initial index of each bucket for every part of data. This is sequential algorithm.
Third stage: each core again process its own part of data, sending each element into position determined by Cnt[part][bucket]


In pseudocode the entire pass looks like:

parallel_for part in 0..K-1
  for i in indexes(part)
    bucket = compute_bucket(a[i])
    Cnt[part][bucket]++

base = 0
for bucket in 0..R-1
  for part in 0..K-1
    Cnt[part][bucket] += base
    base = Cnt[part][bucket]

parallel_for part in 0..K-1
  for i in indexes(part)
    bucket = compute_bucket(a[i])
    out[Cnt[part][bucket]++] = a[i]



------------------------------------------------------------

ID: cs.stackexchange.com:122116
Title: MSD vs LSD Radix sort
Body: I read the following in CRLS:



I don't understand the text in yellow. Why would radix sort not work so well if we sort by their most significant digit? What extra "piles of cards" is it referring to?  

Perhaps I'm not able to follow the example with cards, and an example with actual numbers would be best. 



In case it helps, sorting naively by MSD doe not seem to work, even if all values are d-digit numbers:

Input   (1)   (2)   (3)
        *      *      *
 321    132   321   321
 522    321   426   522
 132    426   522   132
 426    522   132   426


where (i) is the result of sorting the previous column by the ith-highest digit.


[Answer]: It would work, the only problem is that it will generate a lot of extra piles for the intermediate results that are difficult to track.
if the sorting algorithm sorts the $d$-digit numbers starting from their most significant digit it would create 10 piles at first (one pile for the numbers starting with 0, another one for those starting with 1 and so on) and then sort each pile recursively as explained in the book.
The problem is that to sort the pile of numbers starting with 0 the algorithm needs to create ten more piles (one for the numbers starting with 00, the second for the numbers starting with 01 and so on, the last pile is for the numbers starting with 09).
Thus we have created 19 piles until now. To sort the pile of the numbers starting with 00 we need to create 10 more piles. If this process continues until the $d$ digit are sorted you can imagine that a huge number of piles are created (how many?).
This are the extra piles that the book was referring to.
If you use LSD radix sort you don't have to split the numbers in piles at all. You can just sort the input pile by last digit, then the same pile by the penultimate digit and so on, after $d$ steps you will end up with the expected result.
The intuition is the following: let $d$ be $2$ and let $83$, $19$ and $17$ be the input.
The first step on MSD radix sort will place $17$ and $19$ before $83$. Then if you sort the same whole pile by the second digit you will end up with $83$ before $17$ which is wrong. You need to split the pile because you have to "remember" the previous sorting made by the algorithm, which is more "important".
Conversely, the first step of LSD radix sort will place $83$ before $17$, then $19$. If you take the whole pile and sort it by the first digit you get $17$, $19$, $83$ which is correct. You don't need to split the pile because the current step of the algorithm is more "important" of the previous ones and it is allowed to "mess up" the previous ordering in any way (for example by placing $83$ after $17$ and $19$). This would work as long as the sorting algorithm used for each digit is stable.


------------------------------------------------------------

ID: cs.stackexchange.com:104622
Title: For sorting 10^9 unique 9-digit numbers, would radix sort or counting sort be faster, and why?
Body: 
  For sorting $10^9$ unique 9-digit numbers, would radix sort or counting sort be faster, and why?


I know that radix sort is $O(nk)$ and counting sort is $O(n+k)$, but can’t understand how to apply this.


[Answer]: We can fill in the numbers for both:
Radix Sort
The complexity of radix sort is $O(nk)$ with $k$ being the size of the numbers. Therefore we get approximately $9 \cdot 10^9 = 9.000.000.000$ iterations for Radix sort.
Count Sort
The complexity of radix sort is $O(n+k)$ with $k$ being the range of the numbers. In the worst case the range is $10^{9} -1$. So we get approximately  $10^{9}-1 + 10^9 = 2.000.000.000$ iterations  for count sort.
So count sort would be faster.


[Answer]: There are exactly 10^9 unique nine digit numbers, so the sorted array is just 0, 1, 2, 3, ..., 999,999,999. 


------------------------------------------------------------

ID: cs.stackexchange.com:123962
Title: Quick Sort vs Radix Sort
Body: In an coding exam, I was once asked this question:


  Assuming that you are only sorting Integers in ascending order, which algorithm do you use when you want to prioritize speed the most, but you also want to save space?


The options were:


  
  LSD Radix Sort
  Merge Sort
  Quick Sort
  


I think an argument can be made between LSD Radix Sort and Quicksort.

Radix sort would function in $O(k n)$ and Quicksort would function in $O(n \log n)$ and worst case, $O(n^2)$

I chose Radix Sort as the answer, but I'm curious as to what others had to say.


[Answer]: This depends... If the number of input numbers is $n$, assuming the word-ram model of computation, a word size of $\Theta(\log n)$ bits, and worst-case inputs:


Radix sort would require $O(n \log n)$  time and $O(n)$ auxiliary space.
Quicksort with fixed or random pivot selection would require $O(n^2)$ worst-case time and $O(\log n)$ auxiliary space.
Mergesort requires $O(n \log n)$ time and $O(n)$ auxiliary space.


Radix sort and Merge sort are then equivalent in this setting.

Notice that using quicksort with random pivoit selection results in a time complexity of $O(n \log n)$ with high probability. Selecting the pivot deterministically in a way that partitions the elements into two sets whose cardinalities are at most a constant factor apart gives your an algorithm with worst-case time complexity of $O(n \log n)$ and $O(\log n)$ auxiliary space.


------------------------------------------------------------

ID: cs.stackexchange.com:21385
Title: Choosing the optimal radix/number-of-buckets when sorting n-bit integers using radix sort
Body: This is a popular question: 


  What is the most efficient (in time complexity) way to sort 1 million 32-bit integers? 


Most answers seem to agree that one of the best ways would be to use radix sort since the number of bits in those numbers is assumed to be constant. This is also a very common thought exercise when CS students are first learning non-comparison based sorts. However, what I haven't seen described in detail (or at least clearly) is how to optimally choose the radix (or number of buckets) for the algorithm.

In this observed answer, the selection of the radix/number of buckets was done empirically and it turned out to be $2^8$ for 1 million 32-bit integers. I'm wondering if there is a better way to choose that number? In "Introduction to Algorithms" (p.198-199) it explains Radix's run-time should be $\Theta(d(n+k))$ (d=digits/passes, n=number of items, k=possible values). It then goes further and says that given n b-bit numbers, and any positive integer $r \leq b$, radix-sort sorts the number in $\Theta((b/r)(n+2^r))$ time. It then says: 


  If $b \geq \lfloor \lg(n) \rfloor$, choosing $r = \lfloor \lg(n) \rfloor$ gives the best time to within a constant factor.


But, if we choose $r = \lg(10^6) =20$, which is not $8$ as the observed answer suggests.

This tells me that I'm very likely misinterpreting the "choosing of $r$" approach the book is suggesting and missing something (very likely) or the observed answer didn't choose the optimal value.

Could anyone clear this up for me?


[Answer]: The derivative of (b/r)(n+2^r) = (b (2^r (r ln(2) - 1) - n))/r^2. A minimum occurs when the derivative == 0, which occurs when 2^r (r ln(2) - 1) - n = 0. For n == 2^20 (about 1 million), r ~= 16.606232 results in O() ~= 2212837. Some example values and O():

 r   O
18   2330169
17   2220514
16   2228224
15   2306867
12   2807125


So r = 16 would be the best actual number based on the formula. However since L1 cache is typically 32 KB, the optimal values for r are less. On my system (Intel 2600K 3.4 ghz), for n = 2^20, then 4 fields of 8, 8, 8, 8 (r = 8) is fastest. At around n = 2^24 3 fields of 10, 11, 11 (r = 10.67) is fastest. At around n = 2^26, 2 fields of 16, 16 (r = 16) is fastest. There's not a lot of difference though, less than 10%.


[Answer]: Minimizing run time is a very different question from minimizing operation counts. Sorting large data sets tends to be memory bound, so the goal of optimization is not to reduce the number of operations, but to try and do make efficient use of memory — for example,


do as much work as possible in L1 cache
use bandwidth to main memory efficiently by writing full cache lines
avoid overflowing the TLB by too much


Increasing the number of buckets works against all three points; it does no good to complete a radix sort in two passes if each pass takes three times as long as each pass of a four pass implementation!


------------------------------------------------------------

ID: cs.stackexchange.com:12223
Title: Practical Applications of Radix Sort
Body: Radix sort is theoretically very fast when you know that the keys are in a certain limited range, say $n$ values in the range $[0\dots n^k -1]$ for example.  If $k

However, I've read that in practice radix sort is typically much slower than doing for example a randomized quicksort:


  For large arrays, radix sort has the lowest instruction count, but
  because of its relatively poor cache performance, its overall
  performance is worse than the memory optimized versions of mergesort
  and quicksort.


Is radix sort just a nice theoretical algorithm, or does it have common practical uses?


[Answer]: Radix sorts are often, in practice, the fastest and most useful sorts on parallel machines.


Zagha and Blelloch: Radix sort for vector multiprocessors. Supercomputing, 1991: 712-721.
Blelloch, Leiserson, Maggs, Plaxton, Smith, and Zagha: A Comparison of Sorting Algorithms for the Connection Machine CM-2. Symp Parallel Algorithms and Arch (SPAA-3):3-16, 1991.
Arpaci-Dusseau, Arpaci-Dusseau, Culler, Hellerstein, and Patterson: High-performance sorting on networks of workstations. Conf on Mgt of Data, (SIGMOD-1997):243-254.
Arpaci-Dusseau, Arpaci-Dusseau, Culler, Hellerstein, and Patterson. Searching for the sorting record: experiences in tuning NOW-Sort. ACM Sigmetrics Symp Parallel and Distributed Tools, (SPDT-2):124-133, 1998.


On each node of the multiprocessor you probably do something like a quicksort, but radix sort allows multiple nodes to work together with less synchronization than the various recursive sorts.

There are other situations too.  If you need a stable sort (a sort where whenever two keys are equal they stay in the same order rather than getting rearranged) then I'm not aware of any version of quicksort that will be of use.  Mergesort is also stable (if implemented correctly).  Your link is the first time I've ever heard anyone say that mergesort could be made to have better cache behavior than radix sort.


[Answer]: Radix sort is also a natural way to sort fixed-length words over a fixed alphabet, e.g. in Kärkkäinen & Sanders algorithm (http://www.cs.cmu.edu/~guyb/realworld/papersS04/KaSa03.pdf)


[Answer]: @Robert: Your link is quite surprising (actually I could not find the quoted sentence). My personal experience is for random input, radix sort is much faster than the STL std::sort(), which uses a variant of quicksort. I used to make an algorithm 50% faster by replacing std::sort() with an unstable radix sort. I am not sure what is the "memory optimized version" of quicksort, but I doubt it can be twice as fast as the STL version.

This blog post evaluated radix sort along with several other sorting algorithms. Briefly, in this evaluation, std::sort() takes 5.1 sec to sort 50 million integers, while in-place/unstable radix sort takes 2.0 sec. Stable radix sort should be even faster.

Radix sort is also widely used for stably sorting strings. Variants of radix sort are at times seen for constructing suffix arrays, BWT, etc.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:77529
Title: Why isn't Radix Sort used more often?
Body: It's stable and has a time complexity of O(n). It should be faster than algorithms like Quicksort and Mergesort, yet I hardly ever see it used.


[Answer]: That O(f(n)) really means in order of K*f(n), where K is some arbitrary constant. For radix sort this K happens to be quite big (at least order of number of bits in the integers sorted), on the other hand quicksort has one of the lowest K among all sorting algorithms and average complexity of n*log(n). Thus in real life scenario quicksort will be very often faster than radix sort.


[Answer]: It's quite rare that the keys you sort by are actually integers in a known, sparse range. Usually you have alphabetic fields, which look like they would support non-comparative sorting, but since real-world strings aren't distributed evenly across the alphabet, this doesn't work as well as it should in theory. 

Other times, the criterion is defined only operationally (given two records, you can decide which comes first, but you cannot assess how a 'far' down the scale an isolated record is). So the method is often not applicable, less applicable than you might believe, or just not any faster than O(n * log (n)).


[Answer]: Most sorting algorithms are general-purpose.  Given a comparison function, they work on anything, and algorithms like Quicksort and Heapsort will sort with O(1) extra memory.

Radix sorting is more specialized.  You need a specific key that's in lexicographic order.  You need one bucket for each possible symbol in the key, and the buckets need to hold a lot of records.  (Alternately, you need one big array of buckets that will hold every possible key value.)  You're likely to require a lot more memory to do radix sort, and you're going to use it randomly.  Neither of this is good for modern computers, since you're likely to get page faults like Quicksort will get cache misses.

Finally, people don't in general write their own sort algorithms any more.  Most languages have library facilities to sort, and the right thing to do is normally to use them.  Since radix sort isn't universally applicable, typically has to be tailored to the actual use, and uses lots of extra memory, it's hard to put it into a library function or template.  


[Answer]: I use it all the time, actually more than comparison-based sorts, but I'm admittedly an oddball that works more with numbers than anything else (I barely ever work with strings, and they're generally interned if so at which point radix sorting can be useful again to filter out duplicates and compute set intersections; I practically never do lexicographical comparisons).

A basic example is radix sorting points by a given dimension as part of a search or median split or a quick way to detect coincident points, depth sorting fragments, or radix sorting an array of indices used in multiple loops to provide more cache-friendly access patterns (not going back and forth in memory only to go back again and reload the same memory into a cache line). There's a very wide application at least in my domain (computer graphics) just for sorting on fixed-sized 32-bit and 64-bit numeric keys.

One thing I wanted to pitch in and say is that radix sort can work on floating-point numbers and negatives, though it's difficult to write an FP version that is as portable as possible. Also while it's O(n*K), K just has to be the number of bytes of the key size (ex: a million 32-bit integers would generally take 4 byte-sized passes if there are 2^8 entries in the bucket). The memory access pattern also tends to be a lot more cache-friendly than quicksorts even though it needs a parallel array and a small bucket array typically (the second can usually fit just fine on the stack). QS might do 50 million swaps to sort an array of a million integers with sporadic random-access patterns. The radix sort can do that in 4 linear, cache-friendly passes over the data.

However, the lack of awareness of being able to do this with a small K, on negative numbers along with floating-point, might very well contribute significantly to the lack of popularity of radix sorts.

As for my opinion on why people don't use it more often, it might have to do with many domains not generally having the need to sort numbers or use them as search keys. However, just based on my personal experience, a lot of my former colleagues also didn't use it in cases where it was perfectly suited, and partially because they weren't aware that it could be made to work on FP and negatives. So aside from it only working on numeric types, it's often thought to be even less generally applicable than it actually is. I wouldn't have nearly as much use for it either if I thought it didn't work on floating-point numbers and negative integers.

Some benchmarks:

Sorting 10000000 elements 3 times...

mt_sort_int: {0.135 secs}
-- small result: [ 12 17 17 22 52 55 67 73 75 87 ]

mt_radix_sort: {0.228 secs}
-- small result: [ 12 17 17 22 52 55 67 73 75 87 ]

std::sort: {1.697 secs}
-- small result: [ 12 17 17 22 52 55 67 73 75 87 ]

qsort: {2.610 secs}
-- small result: [ 12 17 17 22 52 55 67 73 75 87 ]


And that's just with my naive implementation (mt_sort_int is also radix sorting but with a faster branch of code given that it can assume the key is an integer). Imagine how fast a standard implementation written by experts might be. 

The only case where I found the radix sort to fare worse than C++'s really fast comparison-based std::sort was for a really small number of elements, say 32, at which point I believe std::sort starts using sorts better suited for the smallest number of elements like heapsorts or insertion sorts, though at that point my implementation just uses std::sort.


[Answer]: If all your parameters are all integers and if you have over 1024 input parameters, then radix sort is always faster.

Why?

Complexity of radix sort = max number of digits x number of input parameters.

Complexity of quick sort = log(number of input parameters) x   number of input parameters


So radix sort is faster when 

log(n)> max num of digits


The max integer in Java is 2147483647. Which is 10 digits long

So radix sort is always faster when 

log(n)> 10


Therefore radix sort is always faster when 
n>1024


[Answer]: One more reason:  These days sorting is usually implemented with a user-supplied sorting routine attached to compiler-supplied sort logic.  With a radix sort this would be considerably more complex and gets even worse when the sort routine acts upon multiple keys of variable length.  (Say, name and birthdate.)

In the real world I have actually implemented a radix sort once.  This was in the old days when memory was limited, I couldn't bring all my data into memory at once.  That meant that the number of accesses to the data was far more important than O(n) vs O(n log n).  I made one pass across the data allocating each record to a bin (by a list of which records were in which bins, not actually moving anything.)  For each non-empty bin (my sort key was text, there would be a lot of empty bins) I checked if I could actually bring the data into memory--if yes, bring it in and use quicksort.  If no, build a temp file containing only the items in the bin and call the routine recursively.  (In practice few bins would overflow.)  This caused two complete reads and one complete write to network storage and something like 10% of this to local storage.  Simply quicksorting the whole file would I believe cause about 2 * n log n reads and about half as many writes--considerably slower.

These days such big-data issues are far harder to run into, I will probably never write anything like that again.  (If I was faced with the same data these days I would simply specify 64-bit OS, add RAM if you get thrashing in that editor.)


------------------------------------------------------------

ID: cs.stackexchange.com:9547
Title: Can someone help me understand cache conscience radix sort? (excerpt from journal article attached)
Body: Article:  

CC-Radix: a Cache Conscious Sorting Based on Radix sort

(IEEE 2003)

I'm trying to figure out what the author means by this section:


  Explanation of CC-Radix For clarity reasons, we explain the recursive
  version of CC-Radix sort as shown in Figure 3. However, we use the
  iterative implementation of CC-Radix for the evaluations in this paper
  because it is more efficient. The parameters of CC-Radix are bucket, which
  is the data set to be sorted, and b, which is the number of bits of the
  key that still have to be sorted. Constant b is explained below.

CC-Radix(bucket, b)

￼if fits in cache (bucket) then
   Radix sort(bucket, b )
￼￼￼￼￼￼else
   sub-buckets = Reverse sorting(bucket, ) 
   for each sub-bucket in sub-buckets
      CC-Radix(sub-bucket, ) 
endfor
￼￼￼￼￼endif 
end

  
  Figure 3. Pseudocode of CC-Radix The algorithm starts by checking
  whether the data struc- tures to sort bucket fit in cache level Li.
  Those data struc- tures are vectors S and D and the counter vectors C, one
  for each of the digits of the key. If so, CC-Radix calls Radix sort.
  If the data structures do not fit in cache level , the algorithm
  partitions that bucket into sub-buckets by sorting the bucket by the
  most significant digit using the counting algorithm (explained above).
  Here, we call this process of partitioning, Reverse Sorting, and it
  takes into account the computer architecture of the machine. For each
  sub-bucket, the CC-Radix sort is called again. Note that the first
  call to the routine processes the com- plete data set. Further calls
  to the routine process sub- buckets. Note also that certain subsets of
  the data may need more Reverse sorting calls than others depending on
  the data skew.
  
  Now, we explain the details of Reverse sorting and the sizing of the
  parameters of Radix sort. Reverse sorting. After each call of Reverse
  sorting, the number of digits that remain to be sorted for a specific
  sub- bucket is decremented by one.


I don't understand the "Reverse Sorting" part of this - how is that supposed to work?

I know this is a long shot, but I'm really stuck on what they mean.  Any help is appreciated!


[Answer]: The naming in the pseudo-code seems to be making things more confusing than they need to be, and it also seems to be glossing over some detail, but here's what I think is going on. 

Let's just jump right in on a small example.
Suppose we have a data set, the integers 0-7 in binary:
$000, 001, 010, 011, 100, 101, 110, 111$. But obviously they're not necessarily given to you in sorted order. Let's say the input is actually:
$110, 101, 000, 010, 100, 111, 001, 011$. 

If all this data fits in the cache, then you just run normal radix sort. That's the first branch of the if statement. For illustration, lets say that the cache can only hold two elements. Therefore, we end up in the else branch. The "reverse sorting" step splits the input into two sub-buckets based on the most significant bit. 
So we get two buckets:

$1: (10, 01, 00, 11)$ and $0: (00, 10, 01, 11)$. We recursively call our function on each of these smaller buckets. Since they are smaller, they might now fit inside the cache, and we could run normal radix sort. 

So now we're in the recursive call on the first sub-bucket. The input is 
$1:(10, 01, 00, 11)$. However, our cache size is only 2. Therefore we move into the else branch again. We split the input into
$11:(0,1)$ and $10:(1, 0)$. and call the function recursively. Now finally on each of these recursive calls, the input is small enough to fit in the cache, and we run normal radix sort. 

The other recursive calls function similarly.
Their pseudo-code seems to be missing how the partitions are pieced back together in a cache conscious way. Maybe this is obvious or covered elsewhere in the paper.


------------------------------------------------------------

ID: cs.stackexchange.com:40364
Title: I can not see why MSD radix sort is theoretically as efficient as LSD radix sort
Body: Here is the pseudocode for LSD radix sort from http://www.cs.princeton.edu/~rs/AlgsDS07/18RadixSort.pdf

   public static void lsd(String[] a)
   {
        int N = a.length;
        int W = a[0].length;
        for (int d = W-1; d >=0; d--)
        {
             int[] count = new int[R];
             for(int i = 0; i 

I guess that by 256 they mean $R$. We have a simple for loop, so the time is $\Theta(W(R+N))$ The reason why I use $\Theta$ is because the bound is tight, this is how many operations you will be doing no matter what, so it's worst case and best case performance at the same time. The space requirements are $\Theta(N+R)$

The following is the pseudocode of MSD:

    public static void msd(String[] a)
    { 
        msd(a, 0, a.length, 0); 
    }

    private static void msd(String[] a, int lo, int hi, int d)
    {
        if (hi 

Since the recursion we will stop recursing after $W$ levels in the worst case, the space complexity becomes $\Theta(N + WR)$

however what about time? Just from space itself, it seems like MSD is actually pretty bad. What can you say about the time? To me it looks like you will have to spend $R$ operations for every node in your recursion tree and also $N$ operations per every level. So the time is $\Theta(NW + R*amountOfNodes)$.

I do not see how and why MSD would ever be preferred in practice given these time bounds. They are pretty horrible, but I am still not sure whether I get the time bounds in a correct way, for instance I have no clue how big is amountOfNodes..

Keep in mind that I would like the analysis to be in terms of R and N. I know that in practice R is usually small so it's just a constant, but I am not interested in this scenario, since I would like to see how much does R affect the performance as well.

I asked a kind of similar question yesterday but I did receive any replies, I think the reason for that was because my question was not well structured, so I could remove this question if the admins wanted. Thank you in advance!


[Answer]: If I understand OP's question correctly - the question is what is the time-complexity estimate for MSD radix sort, taking into account what OP calls AmountofNodes or recursive frames for an MSD sort, which must fill the count[] array ($R$) per each recursive frame and than iterate through the count[] array to create subfiles. I couldn't find this in the literature and I presume jsguy hasn't found a satisfactory explanation either. 

I'm going to assume we are talking about the Naive MSD sort as described in the OP's pseudo code. (otherwise if you have 20 items in a subfile and $R = 256$, a hash-table would be significantly cheaper than the count[] array). 

From the looks of it worst case time-complexity for that should be $\Theta(NW + NR^{(W-1)})$, where each item is unique. Specifically a worst-case number of $R$ calls will happen if none of the recursive calls terminate early (before $W_{i-1}$) and if a subfile is broken up as much as possible from the root (the maximum number of recursive calls started from any subfile is $R$).

Considering a set of subfiles $N$ - if the set is already dense duplicates only cost $W$ ($N > R^W$ and all combinations appear), if the set is sparse ($N 

If the set $N$ is sparse, either some combinations will not diverge at level $W_0$ (which is always more expensive than diverging at some further level $W_x$) or as many as possible will diverge at each level but some will terminate before $W_{i-1}$ because the branch will have only a single item remaining - therefore for sparse sets the worse-case is to have maximal divergence at each level $W$ until there are two duplicates in each subfile, for which the above estimate of $R(W-1)$ per item should apply. 



edit: As to why people use MSD Radix sort, I hope someone that works with an application that utilizes Radix sort can answer that, I suspect there are cases like long RNA strings where there are very long strings with a very small $R$, and the ability to parallelize is an advantage (just a guess though). My worst case estimate definitely sounds pretty bad, but I think in practice no one tries to split a file of two duplicate items into $R$ buckets at every single $W_i$.


------------------------------------------------------------

ID: cs.stackexchange.com:18889
Title: Is radix sort a greedy algorithm?
Body: I was thinking of radix sort, and at a sudden thought that it uses de facto the paradigm  of dynamic programming, but I soon changed my mind to greedy algorithm. Is it really a greedy algorithm? 


[Answer]: Personally, I would not call it a greedy algorithm. I'm not sure that's a useful way to think about it.

A greedy algorithm optimizes some global objective function by making small choices, where at each step we make a locally optimal choice (among all the choices at each step, we choose the one that best improves our objective function).  In a greedy algorithm, the objective function monotonically increases at each step of the algorithm.  I don't see any sense in which radix sort has this form.

This becomes clearest if we look at the version of radix sort that starts by sorting on the least significant digit, then the next-least significant digit, etc.  Here the objective function does not monotonically increase.  The first step can take you further away from being sorted, but then later steps bring you back.

For instance, consider the list $[0101, 0010, 0001]$.  Let's look at what happens if we apply radix sort to this list.  The first pass will transform this to $[0010, 0101, 0001]$: it got further away from being sorted.  After the second pass, we get $[0101, 0001, 0010]$, then after the third pass we get $[0001, 0010, 0101]$: it is finally sorted.  So, we started out being close to sorted, then we got farther away from being sorted, then we got closer -- not exactly the characteristic of a greedy algorithm.

For an even better example, consider the list $[0001, 0010, 0101]$.  Let's look at what happens if we apply radix sort to this list, even though it happens to already be sorted.  The first pass will transform this to $[0010, 0001, 0101]$.  I don't care what your global objective function is; this has certainly made the global objective function worse, because it changed the list from being completely-sorted to being not-sorted.  So, this shows that radix sort doesn't seem to be a greedy algorithm (at least if you start by sorting on the least significant bit).

There's an alternative version of radix sort where we sort first by sorting on the most significant digit, then the next-most significant digit, etc.  This does feel closer to a greedy algorithm, though I'm not sure that viewpoint is really useful in any practical way.

It is definitely not a dynamic programming algorithm.

I'm not sure that trying to classify it in this way is useful or helpful.  I don't think it'll help you, e.g., understand how greedy algorithms work, or help you design better greedy algorithms in the future in other settings.  I'd suggest you think about radix sort on its own terms: it just is what it is.


------------------------------------------------------------

ID: cs.stackexchange.com:124946
Title: Radix sort slower than Quick sort?
Body: I would like to demonstrate that sometime radix-sort is better than quick-sort. In this example I am using the program below: 

#include 
#include 
#include 
#include 
#include 
#include 

int cmpfunc (const void * a, const void * b) {
   return ( *(int*)a - *(int*)b );
}

void bin_radix_sort(int *a, const long long size, int digits) {
    assert(digits % 2 == 0);

    long long count[2];
    int *b = malloc(size * sizeof(int));
    int exp = 0;

    while(digits--) {
        // Count elements
        count[0] = count[1] = 0;
        for (int i = 0; i > exp) & 0x01]++;

        // Cumulative sum
        count[1] += count[0];

        // Build output array
        for (int i = size - 1; i >= 0; i--)
            b[--count[(a[i] >> exp) & 0x01]] = a[i];

        exp++;
        int *p = a; a = b; b = p;
    };

    free(b);
}

struct timespec start;

void tic() {
    timespec_get(&start, TIME_UTC);
}

double toc() {
    struct timespec stop;
    timespec_get(&stop, TIME_UTC);
    return stop.tv_sec - start.tv_sec + (
        stop.tv_nsec - start.tv_nsec
    ) * 1e-9;
}

int main(void)
{
    const long long n = 1024 * 1024 * 50;
    printf("Init memory (%lld MB)...\n", n / 1024 / 1024 * sizeof(int));

    int *data = calloc(n, sizeof(int));

    printf("Sorting n = %lld data elements...\n", n);

    long long O;
    tic();
    O = n * log(n);
    qsort(data, n, sizeof(data[0]), cmpfunc);
    printf("%lld %lf s\n", O, toc());

    int d = 6;
    tic();
    O = d * (n + 2);
    bin_radix_sort(data, n, d);
    printf("%lld %lf s\n", O, toc());
}


It performs as follow:

$ gcc bench.c -lm
$ ./a.out 
Init memory (200 MB)...
Sorting n = 52428800 data elements...
931920169 1.858300 s
314572812 1.541998 s


I know that Quick Sort will perform in O(n log n) while Radix Sort will be in O(d (n + r)) ~= O(6 * n). For n = 52428800, log(n) = 17. I am then expecting Radix Sort to be 3 times faster than Quick Sort...

This is not what I observe. 

What am I missing?


[Answer]: 
qsort is not Quicksort. qsort is whatever the implementor of the standard library decided. 
Sorting 50 million identical values is highly unrepresentative. There are qsort implementations that will sort 50 million identical, or 50 million sorted or reverse sorted elements in linear time. 
Radix sort of 50 million numbers with six digits is obviously nonsense. Six digits means you expect only 64 different values and therefore would use counting sort. With less than 26 digits it doesn't make sense. 
Your radix sort does an awful lot of copying, and worse, uncached copying. 
Your radix sort doesn't produce a correctly sorted result if digits is an odd number. 
Your qsort call doesn't produce a correctly sorted result if the array contains very large and very small numbers. 



------------------------------------------------------------

ID: cs.stackexchange.com:151505
Title: Advantages of linked lists over arrays?
Body: Are there other advantages to linked lists over arrays other than constant time removals of the first element?


[Answer]: Linked lists can work well even if memory is fragmented. Arrays usually require a continuous piece of memory. For large piece of data finding this large continuous piece of memory might be hard.
Most of benefits of linked lists requires to remember more links than just the head. For example. If you are doing some sequential checks and deletions or insertions, you can keep this link of the last checked element. Then all of this insertions or deletions are O(1), you have all the data needed, and memory required for insert is small. In other words linked lists are good for local operations. Like text. If you would try to implement a text editor and rewrite the whole array when user adds another letter in the middle, it will be slower than with a linked list, if linked list remembers last edit points and most edits are a bit further than it. Upgrade to this idea are skip list and rope.
Another useful feature of links is atomicity of small edits. With an array you could achieve atomicity by replacing the link to the array itself, but only after making a whole new copy. But with linked list you can build another subchain, and then replace the link to it, changing the small portion of the linked list this way, keeping all the data consistent and not spending lots of memory on another copy.


[Answer]: This isn't really about "advantages", but appropriateness. Sometimes it's more appropriate to use an array, and sometimes to use a linked list.
If it helps, take a look at this large table of the complexity of various operations of a bunch of data structures from the C++ standard library so you can compare them. If you're not familiar with C++, vector is a growable array, deque is a double-ended queue (essentially a chunked array), and list is a linked list. What you can see is that the main tradeoff is:

Dereferencing an element of an array, from a position index, is $O(1)$ operation, and linked lists don't really support that.
Insertion and deletion of an arbitrary element of a linked list is $O(1)$, and $O(n)$ for an array.



[Answer]: I don't have an exhaustive list (pun intended), but off the top of my head:

You can also perform constant time operation at the tail (for a circular doubly linked list).
You can implement the move-to-front heuristic (MTF)  efficiently.
No need to perform costly expansion/contraction (there is a way to prevent this in an array but requires extra memory)



[Answer]: If you make the elements stored in the list already contain the next/prev pointers (called an intrinsic or intrusive linked list) than the list doesn't need to do allocations and risk fragmenting memory further just to store 3-pointer sized structs or an array of pointer that needs reshuffling every time you pull out an element.
This is very handy if you expect the elements to move lists frequently and/or a possibly failing allocation means a panic. Like for example the thread descriptor object moving between various wait queues in the kernel depending on why the thread was interrupted.
With an intrinsic list which is doubly linked then if you can acquire a pointer to the object directly then you can pull the object out of the list without needing to find it in the list or array.


[Answer]: A linked list is appropriate for many use cases independent of performance considerations. In addition to examples given by other answers here, consider that a given object (an instance) can be in more than one linked list (internally linked as well as externally linked) but can only be in one array (it can be pointed to by multiple arrays, but that's not the same, for many use cases).
Consider these diagrams from the book The Design Of An Optimizing Compiler (Wulf, Johnsson, Weinstock, Hobbs) (1973) - first is the multiply-internally-linked symbol table, second is the multiply-internally-linked "graph table" used for recognizing common subexpressions - the "threads" are multiple semantically different, semantically important traversals of the same data structure implemented by separate linked lists into the same objects each representing a symbol or a syntax tree node.


BTW, IMO it is a design mistake to consider performance of a data structure before considering appropriateness.  But I can see that it frequently happens, probably because of the way CS/programming is taught, in many cases by teachers and textbooks, in which performance characteristics are prioritized over appropriateness, and in fact, appropriateness is given short shrift.  See for example many industry "interview questions" which are considered properly answered by discussions of O-bounded performance, where the data structure to use is just assumed to be "obvious".  Or in many many examples in textbooks, websites, and language libraries that have good support for external data structures (linked lists, arrays, dictionaries, hash tables) and never mention at all much less support internally linked data structures as used above. (Note that as in the diagrams above hash tables and trees can be represented using internal links if that is appropriate - it's just never done anymore not because it is wrong by mainly because there's no language or library support.)


[Answer]: An important consideration missing from the other answers: It's easier and more performant to use immutable linked lists than immutable arrays, which is especially important in functional programming languages.
For example, to add an element to an array in an immutable language, you actually create a new array with the extra element added to it, which entails copying the entire array. The compiler might be able to optimize this out (it might detect that the original array is never read again, and just append in-place), but not in general.
Adding an element to an immutable linked list is easy, as long as you're willing to add to the front: Create the new head node, and link it to the old head. The old head still represents the old list, and the new head represents the new list!
This implies another benefit of linked lists: Two lists that differ only in the first few elements can share most of their memory.


[Answer]: Donald Knuth’s “dancing links” algorithm is a well-known one that uses the constant-time insertion and deletion of items in a linked list for performance.  You can hear him discuss it, twenty-four years later, here.
An important optimization for linked lists, in a lazy functional language with immutable linked lists, is to treat them just as a sequence and avoid ever actually creating the list itself, compiling to something much like yield return in C#.


[Answer]: If you mean fixed arrays (whose lengths are fixed in advance) versus linked lists, then linked lists have the advantage that they use up memory only on an as needed basis - a new cell is allocated each time a new insert operation is done.  Whereas, arrays would require you to specify the maximum length of the list in advance, and any unused space is wasted, and you can’t insert more elements than this maximum length even if you encounter more data. So, if you don’t know in advance how many elements will be inserted into the list, a linked list would be a better data structure to use.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:128520
Title: What are concrete rules for using a linked list instead of an array?
Body: A linked list can be used when you want cheap insertion and deletion of elements and when it doesn't matter that the elements aren't next to each other in memory.

This is very abstract and I would like a concrete explanation of why a linked list should be used rather than an array. I'm not very experienced with programming, so I haven't got much (if any) real-world experience.


[Answer]: A linked list can be used to implement a message queue.

A message queue is a structure in which we store information about events for later processing. For example, when the user presses a key or moves the mouse, this is an event. An application might be busy at the moment when the event occurs, so it cannot be expected to process the event at the precise moment when it occurs. So, the event is placed in a message queue, (information about which key was pressed, or where the mouse has moved,) and when the application has some time to spare, it checks its message queue, fetches events from it, and process them. (This happens within a time frame of milliseconds, so it is not noticeable.) 

From the usage scenario that I just described, it should be obvious that we never care to have random access to events stored in the message queue; we only care to be able to store messages in it, and retrieve them. So, it makes sense to use a linked list, which provides optimal insertion/removal time.

(Please do not butt in to point out that a message queue is likely, or more likely, or almost as likely, to be implemented using a circular array-list; that's a technical detail, and it has a limitation: you can only store a limited number of messages in it.)


[Answer]: 
Hashtables that use chaining to resolve hash collisions typically have one linked list per bucket for the elements in that bucket.
Simple memory allocators use a free list of unused memory regions, basically a linked list with the list pointer inside the free memory itself
In a FAT filesystem, the metadata of a large file is organized as a linked list of FAT entries.



[Answer]: The C-language "call stack" is implemented as a linked list in the x86 (and most other) binary APIs.

That is, C-language procedure calling follows a first-in, last-out discipline. The result of executing (possibly recursive) function calls gets referred to as a "call stack", or sometimes even just "the stack".

The CALL x86 instruction ends up implmenting a linked list using the "call stack". A CALL instruction pushes the contents of %EIP register, the address of the instruction after the CALL onto stack memory. The called-fuction prolog pushes the contents of the %EBP register, the lowest address of local variables in the calling functio, onto stack memory. Then the called function prolog sets %EBP to the current function's stack base.

That means that %EBP is a pointer to a memory location that holds the address of the calling function's %EBP value.  That's nothing more than a linked list, implemented partially in hardware via CALL. 

As far as what this is good for, it's how x86 CPUs implement function calls, particularly function calls where the function has it's own copy of arguments, and variables local to the function. Every function call pushes some information on the "call stack" that allows the CPU to pick up where it left off in the calling function, without interference from the called function or the calling function.


[Answer]: Here is something part way between an example and an analogy. You have some errands to do, so you grab a piece of paper and write:


bank
groceries
drop off drycleaning


Then you remember that you also need to buy stamps. Because of the geography of your town, you need to do that after the bank. You could copy your whole list onto a new piece of paper:


bank
stamps
groceries
drop off drycleaning


or you could scribble on the one you had:


bank    .......         STAMPS
groceries
drop off drycleaning


As you thought of other errands, you might write them at the bottom of the list, but with arrows reminding yourself what order to do them in. This is a linked list. It's quicker and easier than copying the whole list around every time you add something.

Then your cell phone rings while you're at the bank "hey, I got the stamps, don't pick up any more". You just cross STAMPS off the list, you don't rewrite a whole new one without STAMPS in it.

Now you could actually implement an errands list in code (maybe an app that puts your errands in order based on your geography) and there's a reasonable chance you would actually use a linked list for that in code. You want to add and remove lots of items, order matters, but you don't want to recopy the whole list after each insertion or deletion. 


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:77772
Title: Which sorting algorithms have a different worst case complexity than their average case?
Body: I'v been working on sorting for a while now but I can't figure these two questions apart,  I'm kind of getting mixed up somewhere ... Somebody help

a) Which sorting algorithms have a different worst case complexity than their average case? 

b) Which sorting algorithms have a different best case complexity than their average case? 


[Answer]: The most obvious example would be Quicksort -- its average case is O(N log N), and worst case is O(N2). As its normally written, the best case is O(N log N), but some versions have included a pre-scan to exit early when data was sorted, which gives a best-base of O(N), though I suppose it's open to argument that it's no longer purely a Quicksort at that point.

I don't know whether it does any more, but the implementation of qsort in Microsoft's C standard library used to do that -- mostly to make up for a poor implementation (always used the first element as the pivot) that would otherwise have been O(N2) for sorted data.


[Answer]: Insertion sort is an example of a sort algorithm with a better best case than average case. Best case is O(n), average case and worst case are O(n²)


[Answer]: Here's a complete overview of the complexities.

Some of the most popular ones:


Quick sort: O(n²) in the worst case, O(n lg(n)) on average and in the best case.
Insertion sort: O(n²) in the worst case & average case, O(n) in the best case.



[Answer]: The crucial thing here is recognizing that the algorithm needs to take shortcuts to avoid having to compare each element to every other element (which would result in a O(N^2) algorithm) but only to some of the other elements and to do it again when sorting those elements.  This is the reason why "divide work in piles and handle each of those piles by dividing it in piles and handle each of those piles etc" ends up with O(N log N) complexity.

The hard part is picking those elements, and to handle working with those elements.  Picking is hard because you must choose well to get the subpiles equally sized.  Managing is hard because you cannot spend too much time moving things around if you want a fast algorithm.

For instance, QuickSort works by choosing an element (traditionally called "pivot") and divide the work in two piles.  One having elements smaller than the pivot, and the other having larger.  Then quicksort each pile, and merge the two sorted piles back.  If the pivot is the smallest element each time, you end up with an empty sub-pile and a big subpile with all the elements but the pivot.  This will result in a worst case of O(N^2).  If you choose well, you get half-sized sub-piles and a running time of O(N log N).

So a typical way to choose the pivot is to look at three randomly chosen values from the pile and choose the one in the middle.  This at least avoids the empty sub-pile.


[Answer]: Bubble Sort, O(N) in the best case, O(N ** 2) in the average and worst cases.

To get the O(N) behaviour, set a swapped boolean to false at the beginning of each pass, set it to true whenever you swap two elements. If you have swapped elements, you need to do another pass. This also works as a general termination criterion, as long as comparisons are deterministic, which means you can skip an O(N) counting pass before starting.


[Answer]: There are implementations that first check if the array starts or ends with a sequence of elements in ascending or descending order, and takes advantage if many such elements are found. So best case is linear (if the array is mostly sorted in ascending or descending order). What the average case would be can be highly debatable. 


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:234871
Title: Correct way to undo the past few git commits including a merge?
Body: My team is working with git for the first time and has really messed up our git remote due to our inexperience. I'm trying to help educate them on git to solve the problem in the long term, but in the meantime, the commits that have been made are totally wrong. We haven't made very much progress at all, and I don't see the value in retaining these commits when they could cause confusion/additional problems down the line. 

The most recent commit was a merge between the master branch and a second branch that one of the team members unintentionally created. If I try to revert, git states that I must be using the -m flag, but I'm not sure whether I should be reverting or trying to rebase in this case, or how I would do it given the structure of the past couple of commits.

What is the best way to resolve this issue and how should I go about doing it?


[Answer]: I believe you're looking for the git reset command. If you run git log it will show you a commit history for your repo. Then run git reset --hard HEAD-number of commits you'd like to go back.

Documentation here: http://git-scm.com/docs/git-reset


[Answer]: To undo the merge, you can do

git checkout master
git revert -m 1 [SHA of the merge commit]
git push


This will create and push a new commit which reverts all changes that were merged in.

It's also possible to alter master's history so that it appears as if the merge never happened, but I wouldn't recommend it. Your teammates would have trouble when they try to pull master and the history is different. Also it's easy to revert a revert commit if you reverted the wrong changes, but if you alter mater's history, it's harder to undo that.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:388153
Title: Running python script in a node server?
Body: My younger brother has made a small rock, scissor and paper python game running in console.
I want to host it on my own domain, running on a digitalOcean droplet. I have no more experience with python, than knowing some syntax and use-cases. 

I guess it is almost impossible to run python code in the browser, so my guess is that I have to set up a server. However, due to me not knowing python whatsoever I won't throw my self into learning django or flask for the sake of this. 

Can I create a java backend, or even better a node server running with express that can somehow communicate with the python script? If so, how should this be achieved in the easiest way? 


[Answer]: There are Python implementations available that run in the browser. That is likely the best approach to your problem. Turning a Python script into a basic web app with Flask wouldn't be too hard either, the difficulty is that a console app with its stream-based I/O model is fundamentally different from a web app with a request-based I/O model.

You can of course create a Java or Node backend that runs the Python script. The general approach goes a little something like this:


start the Python script as a separate process. You capture the stdin, stdout, stderr streams.
when the backend receives a request, it writes input to the captured stdin, and responds with captured data from stdout/stderr


The difficulty is managing a Python process that persists over multiple requests, because the Python script is stateful. You  can't share one Python process across all requests because different users will have different program states. So you need to start one Python process per user/session, and shut down old processes if the session is inactive. You may also want to limit the maximum number of concurrent processes, because the memory and CPU requirements can add up.

So managing these processes safely can involve a lot of complexity. Purely client-side solutions like running Python in the browser might be more difficult to set up, but they avoid these complications entirely.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:175655
Title: Python Forgiveness vs. Permission and Duck Typing
Body: In Python, I often hear that it is better to "beg forgiveness" (exception catching) instead of "ask permission" (type/condition checking).  In regards to enforcing duck typing in Python, is this

try:
    x = foo.bar
except AttributeError:
    pass
else:
    do(x)


better or worse than

if hasattr(foo, "bar"):
    do(foo.bar)
else:
    pass


in terms of performance, readability, "pythonic", or some other important factor?


[Answer]: In python you often get better performance doing things the Python way. With other languages, using exceptions for flow-control is generally regarded as a terrible idea because exceptions typically impose an extraordinary overhead. But because this technique is explicitly endorsed in Python, the interpreter is optimized for this type of code.

As with all performance questions, the only way to be certain is to profile your code. Write both versions and see which one runs faster. Though in my experience, the "Python way" is typically the fastest way.


[Answer]: It really depends on how often you think the exception is going to be thrown.

Both approaches are, in my opinion, equally valid, at least in terms of readability and pythonic-ness. But if 90% of your objects do not have the attribute bar you'll notice a distinct performance difference between the two approaches:

>>> import timeit
>>> def askforgiveness(foo=object()):
...     try:
...         x = foo.bar
...     except AttributeError:
...         pass
... 
>>> def askpermission(foo=object()):
...     if hasattr(foo, 'bar'):
...         x = foo.bar
... 
>>> timeit.timeit('testfunc()', 'from __main__ import askforgiveness as testfunc')
2.9459929466247559
>>> timeit.timeit('testfunc()', 'from __main__ import askpermission as testfunc')
1.0396890640258789


But if 90% of your objects do have the attribute, the tables have been turned:

>>> class Foo(object):
...     bar = None
... 
>>> foo = Foo()
>>> timeit.timeit('testfunc(foo)', 'from __main__ import askforgiveness as testfunc, foo')
0.31336188316345215
>>> timeit.timeit('testfunc(foo)', 'from __main__ import askpermission as testfunc, foo')
0.4864199161529541


So, from a performance point of view, you need to pick the approach that works best for your circumstances.

In the end, some strategic use of the timeit module may be the most Pythonic thing you can do.


[Answer]: Performance, I feel, is a secondary concern. If it arises, a profiler would help you focus on the real bottlenecks, which may or may not be how you treat possible illegal arguments. 

Readability and simplicity, on the other hand, are always a prime concern. There are no hard rules here, just use your judgment.

This is a universal issue, but environment- or language-specific conventions are relevant. For example, in Python it's usually fine to simply use the attribute you expect and let a possible AttributeError reach the caller. 


[Answer]: In terms of correctness, I think exception handling is the way to go (I sometimes use the hasattr() approach myself, though).  The basic problem with relying on hasattr() is that it turns violations of code contracts into silent failures (this is a big problem in JavaScript, which doesn't throw on non-existing properties).


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:276549
Title: Can I use in a non-free script a code example that doesn't mention license but which is used by its author in a GPL'ed script?
Body: After seeing the following question one of my problems came in mind: I want to couple two different scripts in two different languages.

I found a solution for that, and I copied that solution. Unfortunately the same author uses this code for a script which he put under the GPL, but he did not mention the GPL in his example.

Can I use the connection (pipe-construction) in a non-free script, even if the script he uses this code is under the GPL?

I want to combine python and ruby, and I used the code from this page. Later I found out that he wrote a script licensed under the GPL using the same technique.


[Answer]: Most likely, no you cannot use the other person's script without licensing your code under the GPL.

For you to legally use someone else's work, they need to provide a license to you for that code.  Some licenses, such as MIT and BSD, are permissive, and are the equivalent of giving the code away.  Other licenses, such as the GPL, impose restrictions upon end-users of their code.

You stated that the script you want to use is GPL'd, then you must release your code as GPL if you wish to use their script.  

The fact that an example was provided without explicitly stating the license is mot.  That example was effectively unlicensed code to which you did not have permission to use.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:323216
Title: Web app with a Node.js front end interefacing with a python script with sockets
Body: Right now I have a webpage with various buttons and options that I'd like to use to send commands to a python script running on the webserver. The python script is being used to interface with devices over a serial connection as well as performing computations with the data received from both the webpage and the serial connection.

I was recommended to use Node.js with socketIO to open up a socket from the front end webpage to the server. I was able to get socketio running in minutes and sending data to a javascript server. Now I tried using that server to replace my python script, but my computations were taking too long, and being single threaded was very limiting.

I'd like to revert back to my python script for handling computations(and possibly serial communications as well), but there are so many options available I don't have time to try them all and see which works best.

As of right now I'm looking at:


Javascript websockets talking directly to the python script over socket

The limitation here is that I won't be taking advantage of the scalability and power of Node.js or socket.io which would handle concurrent connections better without me having to reinvent the wheel.
Socket.io talking to a javascript socket.io server, which then relays the data over a net.Socket to the python script

Is it overkill to have that additional script just relaying information though? In my mind it would be taking advantage of all the socket.io features, but does add a layer to my web stack.
Socket.io talking directly to a python script using python-socketio

I think this makes the most sense, don't really know of any shortfalls.


Ideally the solution would have low latency, allow for data to be sent and received from the front end webpage, and allow for threading so long computations won't block processes. I do NOT need python to act as a websever, and many packages I see are being used that way. Curious as to any input you guys might have.


[Answer]: In the interest of making information available to anyone in the future, my current solution is having a socket.io server running, which connects over a socket to a python script. the socket.io server just relays information between the two and is capable of handling multiple clients whereas a python only solution would not handle multiple clients without me having to reinvent the wheel. So I'm leveraging the multithread power of python with the multi connections of socket.io, without having to recreate what has already been created by others. I didn't go with python-socketio because information was limited and all the documentation involved flask, which I'm not using (I wanna keep my python script as simple and lightweight so that others can easily use it).


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:78397
Title: linux script as a service within PHP & JS
Body: Is it possible to create a web application/service in PHP in which the backbone of the application/service is really a linux script and PHP is just relaying the data from the script? The linux script requires certain types of hardware and drivers in order to execute smoothly. I will be hosting the web application and linux script however not the hardware or drivers. I'm thinking of writing up some javascript code that will check if the end-user meet the hardware/driver requirements & have some sort of connection, if so the linux script will execute and utilize the user's hardware to process data that is then passed to PHP. YIKES!? ;o Is my approach for this correct or is there an easier way? Thanks!


[Answer]: Why even use PHP if its only function is as a conduit to a shell script? Why not just install them as CGI scripts?

You won't be able to check hardware and drivers with Javascript. Having that ability could lead to huge security issues. You might, however, be able to write a client-side Java applet that intefaces with the client machine to check out those things for you. However, I don't know enough about Java to know whether or not that's possible (in a client-side applet anyway).


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:197067
Title: Does change the license under which Linux is released need the permission of all the copyright holders?
Body: As far as I know Linus holds the copyright to Linux.

While looking around in the kernel sources, I see that almost every file has it's own copyright holders. For example the file module.c in the Linux kernel contains:

Copyright (C) 2002 Richard Henderson Copyright (C) 2001 Rusty Russell, 2002, 2010 Rusty


And the COPYING file in the root directory contains:

   NOTE! This copyright does *not* cover user programs that use kernel
 services by normal system calls - this is merely considered normal use
 of the kernel, and does *not* fall under the heading of "derived work".
 Also note that the GPL below is copyrighted by the Free Software
 Foundation, but the instance of code that it refers to (the Linux
 kernel) is copyrighted by me and others who actually wrote it.

 Also note that the only valid version of the GPL as far as the kernel
 is concerned is _this_ particular version of the license (ie v2, not
 v2.2 or v3.x or whatever), unless explicitly otherwise stated.

            Linus Torvalds


So if Linus would want to change the license under which Linux is released, wouldn't he need the permission of all the copyright holders, which are probably 1000's of people?


[Answer]: Yes, he would need permission from every person who has contributed code to the project. This is because those people have only given permission to redistribute their code under the GPL.

Some other projects circumvent this by requiring that contributors assign their rights to the project.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:246349
Title: Need advise for porting linux script to Win command line
Body: I am creating app(script) that is used similar to RSS reader for specific webpage that doesn't have RSS.

So my app downloads that html page in defined intervals, checks new items and parses them into messages that are sent over network.

But my app prototype is written in Bash (Linux is my primary OS), and I was asked that this app will be needed on Windows also - luckily WinXP 32bit only.

So I am deciding how rewrite it easily and without testing various scripting languages and technologies.

Currently my app is working with Bash, and calls for unix utilities like sed, wget and curl. I checked that these utilities are for win32 also.

So my question is about Windows commandline capabilities compared to linux bash. Does wincmd have similar capabilities like bash? In e.g. manipulating strings, cycles or something that might be a problem?

I also found Cygwin, but don't know if it works reliable and this is important in this case of app.


[Answer]: Windows cmd is far more limited than bash. For example, 'if', 'for' and 'goto' are the only flow of control functions, and the versions provided by cmd are fare more limited than the ones in bash. You might eventually be able to write a Windows cmd shell script that duplicates the functionality of a complex bash script, but  it will take you ages, and you won't enjoy it. If you need to port a complex script, try using Microsoft Windows Powershell instead. It programming model is very different from bash, so it will need a complete re-write, but it has quite good capabilities.

If you want to stick to bash, then MinGw and Cygwin are your only choices. Both have been around for years and are quite reliable, but of course will require a separate and fairly extensive install process for the folks using your script


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:349169
Title: Writing a Linux installation script. How do I test it in an automated fashion?
Body: With all the github services available nowadays, it has become quite easy to have automated unit tests for e.g. C++ or Python coding projects.

Together with colleagues I have written a basic install script for Arch Linux in Python that I would like to test automatically instead of manually creating a VM, letting the installer run, booting into the installed system and figuring out if everything works. However, I have no idea how I should approach this as the test would basically need to launch a virtual machine and work inside this virtual machine and I don't see how I could do this with Travis for example.

Questions:


Is it possible with currently available tools to perform at least some automated tests? (e.g. boots successfully)
How do OS developers test their installers? Manual Test Team?



[Answer]: This might not be what you're looking for, but have you considered snapshotting just before you do the install, and restoring to the snapshotted state?  Your answer said you don't want to do this with VMs, so apologies if this misses the mark.

I would set up a VM to the pre-installed state, including linking the installation script to a remote share/partition outside of the VM.  Take a snapshot, and then do the install.  Let the install reboot the instance or do whatever.  After inspecting the install, restore to the previous state, fix my script, and repeat except not take a snapshot the second time through.

If you want to automate some graphical repetitive tasks, you could look into SikuliX.  You can script it with conditionals based on graphical elements - if you see the "OK" button, click it, etc. 

Hope this helps.  It's inelegant, but sometimes I have to "cut my losses" and go with the most efficient system.  


[Answer]: You could use something like KitchenCI for this.
With KitchenCI you could have Vagrant spin up a bare VM, then run the script you have to install everything that is needed.
Finally, you can have KitchenCI run a test suite written with something like Chef InSpec or Serverspec. InsSpec and Serverspec are testing frameworks that are specifically designed to assert and verify server configuration.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:382265
Title: Making execution of Python script fool-proof beyond adding shebang
Body: Assume that a Python package (available via PyPI) is too difficult for novice users to utilize. Specifically, typing python2 path_to_package/start_GUI.py in the command line, which opens a TKinter GUI, is too hard for many users. Instead, users would like to simply double-click on the file start_GUI.py, irrespective of the operating system they are on.

Beyond placing a shebang (i.e., #!/usr/bin/env python2) at the very top of the Python script start_GUI.py, how else can I make it easier for users to execute it?

Linux/OSX:

Users on Linux/OSX could double-click start_GUI.py if they first changed its file permissions (i.e., chmod +X start_GUI.py). Is it possible to change the file permissions of start_GUI.py during the installation of the package via setup.py?

Windows:

Users on Windows could double-click a batch file (e.g., start_GUI.bat), which in turn calls start_GUI.py. Minimal example of start_GUI.bat:

ECHO ON
REM A batch script to execute the GUI
SET PATH=%PATH%;C:\path_to_python\Python27
python path_to_package\start_GUI.py
PAUSE


Can I have this batch file written during the installation of the package via setup.py?


[Answer]: Users should not have to write any launcher scripts, which is why setuptools can create them for you with the entry_points key. For example:

setup(
  ...,
  entry_points={
    'console_scripts': [
      'myprogram=mypackage:main',
     ],
  },
)


This would create an executable myprogram in a format appropriate for the target operating system that basically does:

import sys
from mypackage import main

sys.exit(main())


For further info, read Automatic Script Creation in the setuptools docs.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:207613
Title: Encoding a bash script for use in Python
Body: I am writing some code in Python which checks for the validity of a license key after polling a server. If it is valid then the Python program in turn runs some bash scripts. 

How to create a combined binary of the python script (to check the license) and the bash script (which performs the task)? 

Here are what I think could be the solutions but I am really not sure:


I use the encode function in python and encode the bash script file once and add combine that file and the license_checker.py file into a single binary. When the binary runs, the python code should decode the file into a temp file, run the script and delete the temp file.
Find a mechanism which wraps/embeds the bash code in the python script. I have heard of decorators but I haven't used them and do not know if they would serve the purpose.



[Answer]: I'd use the first option; embed the bash script in a multi-line string in the Python script, then write that out and use the subprocess module to run the script:

import tempfile
import subprocess

script_one = '''\
echo "Hello world!"
echo "This is a bash script. :-)"
'''

def run_script(script):
    with tempfile.NamedTemporaryFile() as scriptfile:
        scriptfile.write(script)
        scriptfile.flush()
        subprocess.call(['/bin/bash', scriptfile.name])

run_script(script_one)


The run_script() function takes a bash script source, writes it to a temporary file, executes it, and automatically cleans up the temporary file when done.

Demo:

$ python /tmp/scripttest.py 
Hello world!
This is a bash script. :-)


Python decorators are of no help here; there is no point in looking at those; decorators are simply Python functions that get to wrap other Python functions.

If the script is large, you could try compressing it with zlib, base64 encode it, then embed that into the Python script file; using .decode('base64') and passing the result to the zlib module first gives you a decompressed string again. The ZLIB compression reduces the size but results in binary data, always a little harder to embed without having to deal with proper escaping and line lengths. The Base64 encoding turns the binary data into multi-line ASCII-safe data again, albeit at the price of 25% more bytes:

>>> import zlib
>>> zlib.compress(script_one).encode('base64')
'eJxLTc7IV1DySM3JyVcozy/KSVFU4koFi4VkZBYrAFGiQlJicYZCcXJRZkGJnoKVrqYSFwDnexDe\n'
>>> print """script_one = '''\\\n{}'''""".format(_)
script_one = '''\
eJxLTc7IV1DySM3JyVcozy/KSVFU4koFi4VkZBYrAFGiQlJicYZCcXJRZkGJnoKVrqYSFwDnexDe
'''


Note that for this short script sample, this increased the size, from 54 characters to 77, but for longer scripts you should see a decent size reduction.

Decompression is then as simple as zlib.decompress(script_one.decode('base64'))


------------------------------------------------------------

ID: datascience.stackexchange.com:36457
Title: getting error while installing install_tensorflow()
Body: i am trying to install tensorflow in r library. when i try to install using
> library(tensorflow)
> install_tensorflow()
Preparing for installation (updating pip if necessary)
Requirement already up-to-date: pip in c:\users\ideapad\appdata\local\programs\python\python35\lib\site-packages (18.0)
Installing TensorFlow...
Collecting tensorflow
  Using cached https://files.pythonhosted.org/packages/55/02/e07e6ff277147aa52816215cc1b96d1acd1166411fc71356c6c3160760ca/tensorflow-1.9.0-cp35-cp35m-win_amd64.whl
Collecting h5py
  Downloading https://files.pythonhosted.org/packages/d0/2c/4572e2e495341e667c89b490ad18ea71a5f9e9fafca06109a9c7db22848b/h5py-2.8.0-cp35-cp35m-win_amd64.whl (2.3MB)
Collecting pyyaml
  Downloading https://files.pythonhosted.org/packages/ad/d4/d895fb7ac1b0828151b829a32cefc8a8b58b4499570520b91af20982b880/PyYAML-3.13-cp35-cp35m-win_amd64.whl (205kB)
Collecting requests
  Downloading https://files.pythonhosted.org/packages/65/47/7e02164a2a3db50ed6d8a6ab1d6d60b69c4c3fdf57a284257925dfc12bda/requests-2.19.1-py2.py3-none-any.whl (91kB)
Collecting Pillow
  Downloading https://files.pythonhosted.org/packages/2e/5f/2829276d720513a434f5bcbf61316d98369a5707f6128b34c03f2213feb1/Pillow-5.2.0-cp35-cp35m-win_amd64.whl (1.6MB)
Collecting termcolor>=1.1.0 (from tensorflow)
Collecting astor>=0.6.0 (from tensorflow)
  Using cached https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl
Collecting tensorboard=1.9.0 (from tensorflow)
  Using cached https://files.pythonhosted.org/packages/9e/1f/3da43860db614e294a034e42d4be5c8f7f0d2c75dc1c428c541116d8cdab/tensorboard-1.9.0-py3-none-any.whl
Collecting wheel>=0.26 (from tensorflow)
  Using cached https://files.pythonhosted.org/packages/81/30/e935244ca6165187ae8be876b6316ae201b71485538ffac1d718843025a9/wheel-0.31.1-py2.py3-none-any.whl
Collecting six>=1.10.0 (from tensorflow)
  Using cached https://files.pythonhosted.org/packages/67/4b/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a/six-1.11.0-py2.py3-none-any.whl
Collecting grpcio>=1.8.6 (from tensorflow)
  Using cached https://files.pythonhosted.org/packages/2e/1c/7451288f896ce972a2d2112ed1d8106d3950a02afc5bddf307a1a5de3d73/grpcio-1.14.0-cp35-cp35m-win_amd64.whl
Collecting numpy>=1.13.3 (from tensorflow)
  Using cached https://files.pythonhosted.org/packages/62/47/54baeff52b37be258dd97442f52d8a2a9c27c4af8fcbc5467827c5ae5eed/numpy-1.15.0-cp35-none-win_amd64.whl
Collecting absl-py>=0.1.6 (from tensorflow)
Collecting protobuf>=3.4.0 (from tensorflow)
  Using cached https://files.pythonhosted.org/packages/f0/7d/1145805ef3ac475074f8d14d1c0512a79ef709ddfd35ca89c5fa4fc94065/protobuf-3.6.0-cp35-cp35m-win_amd64.whl
Collecting setuptools=0.2.0 (from tensorflow)
Collecting idna=2.5 (from requests)
  Downloading https://files.pythonhosted.org/packages/4b/2a/0276479a4b3caeb8a8c1af2f8e4355746a97fab05a372e4a2c6a6b876165/idna-2.7-py2.py3-none-any.whl (58kB)
Collecting urllib3=1.21.1 (from requests)
  Downloading https://files.pythonhosted.org/packages/bd/c9/6fdd990019071a4a32a5e7cb78a1d92c53851ef4f56f62a3486e6a7d8ffb/urllib3-1.23-py2.py3-none-any.whl (133kB)
Collecting chardet=3.0.2 (from requests)
  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)
Collecting certifi>=2017.4.17 (from requests)
  Downloading https://files.pythonhosted.org/packages/7c/e6/92ad559b7192d846975fc916b65f667c7b8c3a32bea7372340bfe9a15fa5/certifi-2018.4.16-py2.py3-none-any.whl (150kB)
Collecting werkzeug>=0.11.10 (from tensorboard=1.9.0->tensorflow)
  Using cached https://files.pythonhosted.org/packages/20/c4/12e3e56473e52375aa29c4764e70d1b8f3efa6682bef8d0aae04fe335243/Werkzeug-0.14.1-py2.py3-none-any.whl
Collecting markdown>=2.6.8 (from tensorboard=1.9.0->tensorflow)
  Using cached https://files.pythonhosted.org/packages/6d/7d/488b90f470b96531a3f5788cf12a93332f543dbab13c423a5e7ce96a0493/Markdown-2.6.11-py2.py3-none-any.whl
Installing collected packages: termcolor, astor, werkzeug, numpy, markdown, six, setuptools, protobuf, wheel, tensorboard, grpcio, absl-py, gast, tensorflow, h5py, pyyaml, idna, urllib3, chardet, certifi, requests, Pillow
Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'c:\\users\\ideapad\\appdata\\local\\programs\\python\\python35\\Lib\\site-packages\\numpy\\.libs\\libopenblas.CSRRD7HKRKC3T3YXA7VY7TAZGLSWDKW6.gfortran-win_amd64.dll'
Consider using the `--user` option or check the permissions.

then i got the following error
Error: Error 1 occurred installing tensorflow package
In addition: Warning message:
running command '"C:\Users\Ideapad\AppData\Local\Programs\Python\Python35\/Scripts/pip.exe" install --upgrade --ignore-installed "tensorflow" "h5py" "pyyaml" "requests" "Pillow"' had status 1 

Can someone help me in this issue?


[Answer]: its just because you don't have permission to save the module as it is in C drive. So, just do the following

1.Open the app as an administrator and then install it

or

2.follow This link. this can help


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:185704
Title: What is the nicest (user-friendliest) way to tell a user about "Access Denied" error?
Body: Our software implements a layer of role-based security to secure data access, in the form of access control lists. Whenever a user tries to do something that isn't allowed, the software layer will receive an "access denied" error code or exception.

Having seen how our customers react to such "access denied" messages, and their negative impression to our software as a result, I've been thinking about how we should modify that message to make it less stressful.

I understand that the best way to reduce such frustration is to simply hide the UI actions that a user is not authorized to do. However, our UI is very feature-rich and it is not always possible to predict whether a UI action will result in access-denied error.

From a user's point of view, it would be nice if the user can be told "what went wrong", i.e. explained in the same way that a human IT administrator would explain.

My question


From a user-friendliness standpoint, what is the best way to present an "access denied" message without offending or frightening the user?
From a software design standpoint, what design methodologies can be used to reduce the chance of a user seeing an access-denied situation?




Some thoughts


If there is a security rule that says someone is forbidden from viewing a piece of data, then:


If the user has low trust (e.g. general public), no explanation for the "access denied" should be given because any partial information could be used against the security.
If the user has partial trust (e.g. an employee in the customer's organization whose rank is not high enough, or works in a different department), perhaps some explanation can be given to the user so that the user knows what security rule is being enforced?

However, occasionally some security setup is based on "denied by default", meaning that in the absence of a rule granting access, nobody can access anything.


In this case, it is much harder for a software system to explain to the user why the "access denied" occurred. Maybe it is a configuration error (a human error)? Maybe a security rule is indeed being enforced in the way of the lack of a "access granting rule"?





Related:

Methodologies for Managing Users and Access?


[Answer]: The best solution in this case (and it is mentioned in the question) is simply to remove, or even better, to disable the UI elements that lead to "Access denied" actions. This way, the user will know that they can't do this action. Additionally, some hints can be displayed on these elements that to explain: "Disabled, because this action requires more privileges." or something similar.

The statement


  "our UI is very feature-rich" 


is a stupid excuse not to use this method. Any other solution will cause annoyance to the user, regardless of how politely you phrase the reason in the error message.

Well, OK, I will write about another acceptable method:
Instead of error message, the program must give the user chance to raise their privileges on such actions. Some dialog box that says: "You need additional privileges in order to perform this action. Please, provide your username/password." and corresponding user/password edits (or whatever controls you are using for authentication).
Also, OK/Cancel buttons. If the user presses "Cancel" no additional action is provided. If the user presses OK without acceptable user/password - provide error message "Access denied for specified user" or "Wrong password" or other consistent message and go back to the dialog for second try.

This way, the user will cancel the action himself, so the user will be less annoyed.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:393303
Title: How to handle user permission changes in SPA?
Body: I have a SPA which at the beginning of application startup calls the backend API, sends a JWT and asks for the access permissions of the current user. SPA then caches the permissions in-memory and uses them to check what the user is permitted to do and clears the cache when user takes an action after which the cache might become invalid (out of sync with the backend), like buying something which gives more permissions.

This worked great but now there is a new functionality which allows someone else (not the user which is currently browsing SPA) to grant (or remove) permissions for other users. This is not an issue for the backend because when a new permission is granted for user A, the backend state is immediately updated and subsequent request would check if user A has the required permissions to execute that action. The issue is that the SPA does not know that user A has a new set of permissions since the old ones are cached. That means that even though from the backend's perspective the user is allowed to execute new actions, frontend thinks that the user does not have required permissions and will prevent user from doing that action and will show an error message. This mismatch between frontend and backend would persist until user reloads the page.


User A is browsing the SPA website User A asks, let's say an admin,
to grant him a permission to do X
Admin grants user A a permission to do X (now backend allows user A to do X) and informs user A of this
User A tries to do X but the frontend permission cache says that there is no permission for X and so an access denied error is shown


Any suggestions on how this can be handled? One idea is to have a list of users which need their frontend cache invalidated stored in the backend (which is a single machine). With every request, check if the user who is executing that request is in that list and if the answer is yes then "cancel" the request and return some HTTP status code which would indicate that SPA cache needs to be cleared to continue. SPA would know how to handle this response code and would clear the cache, reload the current page (or redirect to home page, or something else) and would repopulate the permission cache. This approach would seem to work but it feels somewhat hacky and complex so it would be great to hear some more insights.


[Answer]: Usually, when you work with JWT tokens, the permissions are baked into the token. So you invalidate the token as soon as the permissions change. 

Whether the user needs to log in again or you find some comfortable way to swap tokens when one gets invalidated is up to you and how much work you are willing to invest. 


[Answer]: You may be able to handle this closer to the network transport layer. A request to the server where permission should be denied should return a "403 Permission Denied" response.

If you have a way of tracking "handlers" for certain kinds of responses, you could have one global "handler" for the 403 response, that throws up a modal window saying something like "Permission denied." and give them a "Reload" button, which reloads the page.

If the user has any unsaved changes to data that are not persisted to the backend they will end up losing that work.

You could take it one UI widget at a time to limit this affect. Reload just that UI widget up to and including removing it from screen if the user no longer has any permissions for it, but that will require extra coding.

If you go with the global 403 "handler" you can also register a global 401 "handler" to opens a login modal over top of the page. That way an expired user session doesn't cause the entire page to reload, and have them lose their work. Then you need to handle logging back in and the requisite UI updates if permissions have changed between user sessions — still more coding.

Basically, being nice to the user after permissions have changed is not an easy task for a single page application (or GUI desktop application for that matter).


[Answer]: A solution I have been designing, but for this your SPA requires a BFF (backend for frontend) like asp.net core mvc as its host. 

The SPA Access token only consists of user and access claims and the Roles of said user. The SPA also on initial load performs a API call to fetch the current access policies for the roles the current user is using along with his current access token. Permissions check are then evaluated at browser side based on collection of roles which relates to a flat list of permissions.

When a role is updated by a different user only the current permission cache of the application needs to be refreshed since the policies on the API's would already be up to date.

In order to make this work SignalR is used to push changes to the browser. If a role is changed as a post delegate to the save, a method determines which users are linked to the role with a change, which of them have current active connections, and it broadcasts with Signalr that the permission cache has changed which the SPA will then re fetch its permission cache and the UI checks would be updated. 

For a user with a role change you only need to push the change to the one user if he has an active connection. And the user would perform a call to request an updated access token from the BFF.

Also note the SPA communicates to the API via a Bearer Token or access token but with the BFF it relies on Session cookies in my case which is generated after successful authentication with my Openid connect provider. My SPA does not handle the authentication and tokens on the browser but lets the BFF act as a confidential client to handle those movements between Identity Provider and Application.

Note if the user is currently on a route and his permission has been revoked the SPA would need to after updated the permission cache issue a reevaluate route check or method so it can handle revoking the user's access and rerouting him to component or default area which he has access to.


[Answer]: How about using push notifications?

I've created apps hosting a chat client coupled to a message hub. It's a perfect medium for  sending event notifications from server side asynchronous processes or client instances of the app. When the client side chat listener receives a message that is identified as a system message, that message triggers the app to make an api call, to perhaps, reload a store after refresh of the token.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:262692
Title: Designing a better performing total permissions setup for multiple permission levels
Body: We've got a global system that we are attempting to solve a permissions issue around.  Currently, our system serves a number of different applications out to our clients and each client has their own list of user types and users for our system.  However, all of these things can be independent of each other.  Such that Client A can have access to Application A and Application B.  Client A might also have give different User Types, all with different Users.  Each 'level' of usage would add a new level of complexity to how we give permissions out to everything.

For example: I might have three permissions, called "Grape", "Purple", and "Car".  At the global level (meaning all applications, clients, user types, and users), I deny the permission on Grape, but allow permission for Purple and Car.  That means that no application, no client, no user type, and no user can access Grape, but they can access Purple and Car.  So now, we have two applications, named "Fruit" and "Auto".  Currently, both Fruit and Auto would look down to the global permissions created for Grape, Purple, and Car and use those.  So, this is how things would look:


Fruit

Grape: Denied
Purple: Granted
Car: Granted

Auto

Grape: Denied
Purple: Granted
Car: Granted



But, for the applications, this might not make sense.  So, I would create an application specific permission to grant permissions for Grape and deny permissions for Car for the application of Fruit.  So now, the permissions would look like this.


Fruit

Grape: Granted (a)
Purple: Granted
Car: Denied (a)

Auto

Grape: Denied
Purple: Granted
Car: Granted



While I still have the global permissions telling Auto what permissions to use, I have specific permissions telling Fruit that it needs to grant and deny different permissions there.  Now, we'd add another level of complexity and add in a client.  Let's say I have a client called "Fox".  Fox uses both applications Fruit and Car.  Right now, without adding anything else, the permissions to the three objects (Grape, Purple, and Car) come from the application wide permission level.  But, maybe Fox wants things different.  So, instead of having Purple granted on application Fruit, it denies that permission specifically.  In terms of data, that would be a specific record you would add just for client Fox that overrides the permissions of Purple to be denied, regardless of what the application or global permissions said for Purple.  So, now you have something that looks like this.


Fruit

Fox

Grape: Granted (a)
Purple: Denied (c)
Car: Denied (a)


Auto

Fox

Grape: Denied
Purple: Granted
Car: Granted




Remember, too, that a lot of this can be pretty abstracted.  You could have clients that only use one application, three applications, or every application you have in your system.  And for those clients, you might have very specific user lists that can only use what the client can use, so they'd be inheriting permissions down from the client unless specifically overwritten.

Obviously, this can get increasingly complex when you add in a level for the user type and then for the user.  Keeping track of it all can be very cumbersome, but it's something we have to account for based on our system's model.  So, we came up with a scoring solution for permissions.

We would query the database and tables and pass in the current application, client, user type, and user to the database for whatever we have current at the time.  The database would then use a scoring solution to determine if we can see or not see a particular permission.  Application would be a score of 1, client would be a score of 2, user type would be 4, user would be 8, ect.  The more defined objects you put in, the higher the maximum score can get.  This gives us a very clear definition of what permissions are active for a given set of objects.

Our real problem now is performance.  We now have 5 levels of objects in our system and might be adding more, if the design of the system requires it.  However, querying all of these levels is a serious performance hit on SQL.  And each object will just make the problem exponentially worse.  My question is this: how can I achieve the same type of permission inheritance system here, and give the ability to add new levels when needed, without exponentially increasing the performance hit.  There has to be other companies and applications that have this same problem and have better solutions to it then we do.


[Answer]: Please take a look at the following thread.
Groups is usually the simplest way to go. Note, you don't necessary have to have 'Group' object and link it to your roles/users, simpler approach is tagging. Having specific set of tags on the objects itself identifies the set of groups it belongs to - hence the permissions.

Fox [tags: auto, fruit]
Auto [tags: purple, car]
Fruit [tags: grape]

something along these lines...


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:436841
Title: Permission Design - Allow/Deny vs Allow Only
Body: I've come onto a new a project and the permissions are done with an allow and a deny option for every permission. Until now i've only ever seen/build allow only permissions.
What is the advantage of this? It seems to introduce extra complexity for developer as well as enduser.
The only situation i could think of that an Allow/Deny design can handle, that a Allow-Only can not is the following.
Role has a permission: AllowX = true and DenyX = false.
User has an empty permission.
In an allow-Only design you cannot add an empty permission to the user as only AllowX exists, so the "empty"  user scoped permission (where it says AllowX = false) would override the role scoped permission.
But there are obviously many ways to handle this. Just dont add an empty user permission by default. Make the default user permission an aggregate of the role permissions.
However you see this Allow/Deny model in windows and probably a lot bigger apps. Whats the reason?


[Answer]: If you only ever deal with individual users, individual assets and individual permissions, you probably don't need a deny permission.
As soon as you deal with groups, of users, assets or permissions, deny permissions make things much easier to manage. Some slightly made up examples:
Allow (Users = *, Table = *, Permission = Read)
Deny (Users = *, Table = credit_card_numbers, Permission = Read)

or
Allow (Users = TeamA_Group, Table = teama_*, Permission = Write)
Deny (Users = TeamAProjectManager, Table = teama_*, Permission = Write)

or
Allow (Users = TeamA_Group, Table = teama_*, Permission = *)
Deny (Users = TeamA_Group, Table = teama_*, Permission = Delete)

without a deny permission in those cases, you'd have to individually give read permission on every other table, to each member of TeamA other than the project manager, or to every non-delete permission. That's both error prone and needs action every time a new table / team member / permission arrives as opposed to just doing the right thing by default.


[Answer]: Allow/Deny allows hierarchical permissions, commonly demonstrated by files and folders.
ie  UserA can read and write the contents of /folder but is denied write for folder/file1
In this way, where there are many files in the folder I can express the permissions more succinctly than an Allow for every file plus a deny of the file1.
However! It's a flawed system. As the number of groups and objects increase the overlapping permissions become impossible to manage. In order to work out whether a user has a permission or not you have to calculate the combination of groups. No problem for a computer, but difficult for a human.
You can achieve the same complexity with a single layer permission system and "Roles". Each role gets a combination of single layer permissions, which don't change  and users are assigned to roles.
Because the permissions are static and require no calculation they are easily understood, and because users are assigned to these roles rather than each having their own combination of permissions you can easily state who can do what.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:152920
Title: How to hide the python code from users?
Body: I am developing a paid application in python. I do not want the users to see the source code or decompile it. How can I accomplish this task to hiding the source code from the user but running the code perfectly with the same performance.


[Answer]: To a determined user, you can't.

From a practical standpoint, you can do some tricks, such as wrapping it into a binary executable or using obfuscation.

See here for full details: https://stackoverflow.com/questions/261638/how-do-i-protect-python-code


[Answer]: There's no use in doing that.

If your application is not incredibly small and simple, then even if the client (or other programmers they hired) were to work on that code, they'd still need a lot of your help with it.

If it is incredibly small and simple, don't even bother with it. The client could just get someone to rewrite it from scratch anyway.


[Answer]: I was in a similar situation to this not too long ago. But i managed to find a way to hide my source code, at least, enough to deter most from trying to hack it. 

Contrary to common belief, obfuscation and/or encryption of interpreted language scripts is possible. The question is, do you have the time (and motivation) to devote to it? You'll need to make sure whichever "obfuscation/encryption" method you use is very difficult to crack, and that it does not slow down the execution time of the script in any noticeable way.

If you wish to encrypt a python script, without having to do any work on your end, you can do so using this site.

I tested the following script using the aforementioned site and it produced a very intimidating output, which worked (at least for my purposes):

#!/usr/bin/env python2
# vim: ft=python

import sys, json, urllib2

URL = 'http://127.0.0.1:8080/optimization/resources/_harness/status?_fields=health.summary'

try:
    response = urllib2.urlopen(URL).read()
except:
    print('HEALTHCHECKS CRITICAL - Error fetching health check at %s' % URL)
    sys.exit(2)

parsed = json.loads(response)
resource = parsed["resource"] if "resource" in parsed else parsed
status = resource["health"]["summary"]["status"]
incidents = resource["health"]["summary"].get("incidents", [])
warning = len([incident for incident in incidents if incident["status"] == "WARNING"])
critical = len([incident for incident in incidents if incident["status"] == "CRITICAL"])

if warning or critical:
    print('HEALTHCHECKS %s - %d WARNING - %d CRITICAL' % (status, warning, critical))
else:
    print('HEALTHCHECKS OK - no incidents')

print(json.dumps(resource, indent=2))

sys.exit({"OK":0, "WARNING":1, "CRITICAL":2}.get(status, 3))


Can the obfuscated code produced be broken into? Maybe, maybe not. But it is sure to deter most people from even attempting it.

If you do have some time on your hands and wish to encrypt your script on your own using your own improvised method, you'll want to use the openssl command. Why? Because it appears to be the one encryption tool that is available across most Unix systems. I've verified it exists on Linux (ubuntu, centos, redhat, mac), and AIX.  SunOS i believe uses the "crypt" command instead.

The simplest way to use Openssl to encrypt a script is:

1. cat  | openssl aes-128-cbc -a -salt -k "specify-a-password" > thescript.enc

OR

2. openssl aes-128-cbc -a -salt -in  -k "yourpassword"


To decrypt a script using Openssl (notice the '-d'):

1. cat thescript.enc | openssl aes-128-cbc -a -d -salt -k "specify-a-password" > thescript.dec

OR

2. openssl aes-128-cbc -a -d -salt -in  -k "yourpassword" > thescript.dec


The trick here would be to automate the supply of password so your users need not specify a password each time they wanted to run the script.


------------------------------------------------------------

ID: cs.stackexchange.com:57551
Title: When to use DFS and when use BFS?
Body: Preparing for an interview.

I see two cases where each one is specially suited

BFS:  When you need to find shortest path between vertices (if one exists).

DFS:  If you need to find cycles in a directed graph:

Uses-case where both can be used:  Finding cycle in undirected graph, Is there a path from one vertex to another

Are there any other specialized or common cases?

In general DFS is considered to be simpler (though if graph is very deep, I am assuming we can have a stack overflow due to excessive recursion). So, if both can be used for a use-case, use DFS?


[Answer]: Bfs and Dfs both are good state space search algorithms. If search space is infinite then its good to use Bfs because dfs can be lost in infinite space and will not return any result. 
Also, Bfs searches result in neighbors and then go neighbor by neighbor on other hand dfs searches for answer branch by branch. If you search key is near starting point then go for bfs


[Answer]: Shortest cycle: BFS has the nice property that, if started from u, the first time the BFS finds u is when the shortest cycle that includes u has been completed.
Email infections: Determining whether a computer is infected from looking at the email metadata, BFS and DFS are about the same, since you don't know if that worker was infected early, late, or not at all. The trick in that problem is to set up the graph right so a search can be done with as few conditionals as possible.


------------------------------------------------------------

ID: cs.stackexchange.com:65276
Title: Why do so many 8-Puzzle solving algorithms use DFS instead of BFS?
Body: I see so many 8 Puzzle Solvers use a stack instead of a queue. Why is that? If you are looking for the solution with the fewest number of moves, wouldn't the solution be at a shallower point in the tree, meaning that BFS would find it much quicker? Is there some subtlety to the problem that I'm forgetting that makes DFS a better approach?

I've been comparing performance times myself and it seems like BFS is consistently and significantly faster.


[Answer]: If the goal is to find the shortest solution to the 8-puzzle, DFS is indeed a very poor method.  It's not a matter of faster or slower; DFS is not correct (it finds a solution, but it's not guaranteed to be the shortest solution).  You could use BFS and that will work fine.  Even better is to use A* or some related algorithm; it will find the shortest solution even faster than BFS.

If the goal is to find any solution to the 8-puzzle, without regard to how many moves the solution takes, DFS is a fine method.  Actually, you could use either DFS or BFS for that.  The 8-puzzle has only 181440 reachable states, so the data structures won't grow too large.


------------------------------------------------------------

ID: cs.stackexchange.com:70133
Title: DFS algorithm and how do I show/put it into practice?
Body: I'm new to DFS algorithm and I'm trying to figure it out.

The main concepts I get from it is that you have a stack of nodes, and they have a value, and can contain children-nodes (Even called branches or leaves) that can contains nodes with values.

So let's say that I have a Tree containing 15 nodes like this.



I would understand that the algorithm goes A-N-O-B-D-J-L-M-K-C-E-F-H-G-I, If you were to go in alphabetic order.

My confusion is this how to put this algorithm into practice (such as coding it). How should I see this tree like (Array, List?)

And if some of the children are connected to each other can it still be a DFS algorithm?



[Answer]: The first thing you need to do is pick a data structure - i.e a representation for your node graph.  For this I suggest you pick up any undergraduate-level book on data structures and read it carefully, and choose something appropriate.  For example, given the tree above, I would represent each node with a record containing whatever data is to be stored in that node (e.g the name - A, B, etc) plus a list of (pointers to) child nodes.  E.g your root would have the list {A, B, C}, node A would have the list {N}, node J would have the list {K, L, M}, and so on.  Each childless node ('leaf') would have an empty list.

Then you code your DFS procedure, which you may find easier to do if you use recursion to process children (although there are ways to make this non-recursive if you insist).  Regardless of implementation, the core behavior of DFS is: To process each node, you must do something with the node, then scan the list of children and process each child -and its entire subtree- before processing the next child.  You can get different effects according to whether you process each node before its children, or after [if each node has at most two children AND explicitly distinguishes 'left' child from 'right' child, then there is also the option of processing each node -between- left and right].

And yes, you can still perform DFS if there are cross-connections.  The key new thing to do here is to keep track of whether the node you are about to process has already been processed; generally what you will want to do in that case is skip that node and go back up to the parent.  This is particularly true if 'children' really means 'neighbors', so the edges between nodes aren't directed; then you really need to avoid running in cycles.


------------------------------------------------------------

ID: cs.stackexchange.com:139078
Title: DFS produces the correct Topologically ordered sequence
Body: 
Prove that DFS produces the correct topologically ordered sequence.

I am having a hard time understanding the question itself. Should I prove the correctness of DFS? Should I use the pseudocode?


[Answer]: Before I get to the proof, let me just clarify that the algorithm using DFS would be to process edges in decreasing order of finishing times while running DFS on the input graph.
Now to prove that the above algorithm returns a topological sorting, we can use some lemmas about DFS $-$ namely the Parenthesis Theorem as well as the White Path Theorem. I will not get into the proofs of each lemma but we will use the following:
A corollary of the parenthesis theorem, as follows:

Consider vertices $u$ and $v$ in $G$. Then $v$ is a descendant of $u$ in the DFS forest of $G$, if and only if $d(u), where $d(x')$ is the discovery time of vertex $x'$ and $f(x')$ is its finish time.

Also, we will use the white path theorem which states that

Vertext $v$ is a descendant of vertex $u$ in the DFS forest if and only if at time $d(u),$ there exists a white path from $u$ to $v.$

Now let $e = (u,v)$ be an edge in the input graph. We want to prove that in our output, $u$ comes before $v$. We can show that by proving that in our DFS traversal, $f(u) > f(v).$
CASE I: $d(u) 
By the White Path Theorem, $v$ is a descendant of $u$. This is because at time $d(u)$, there is a white path in $G$, which must be from $u$ to $v$ as there exists edge $e = (u,v)$. Hence, we can use the corollary of the parenthesis theorem to show that $f(u) > f(v).$
CASE II: $d(v) 
At time $d(v)$, there is no white path from $v$ to $u$, as that would imply that $G$ has a cycle (meaning that it is not a DAG), and hence $u$ cannot be a descendant of $v$ in the DFS forest of $G$. So by the parenthesis theorem, we have $d(v) . Again, we proved that $f(u) > f(v)$.
Hope that helped :)


------------------------------------------------------------

ID: cs.stackexchange.com:101141
Title: DFS & BFS Spanning Trees
Body: I want to construct a DFS and a BFS spanning trees for the graph below. The root is node a. At each step the next edge to be traversed should be the cheapest one. 



DFS:

My understanding that to the construct this DFS where each next is the cheapest one, the resultant spanning tree would be:


  {(a,c), (c,h), (h,g), (g,f), (f,b), (b,k), (k, j), (j,i), (i, l), (l,m), (m,e), (e,d)}




BFS:

My understanding that to the construct this BFS where each next is the cheapest one, the resultant spanning tree would be:


  Queue: a  c   d   b   e   f   h   g   k   m   i   j   k




Is my understanding of spanning trees correct when the next edge to be traversed should be the cheapest one? Is this a MCST?


[Answer]: You can construct a DFS spanning tree or a BFS spanning tree in many ways. You could require the next edge to be traversed should be the cheapest one available, if that helps in any way or if you just like to do that. As long as you are using DFS or BFS, you will end up with a spanning tree.

However, if you do want to always traverse using next cheapest edge among all possible choices, "Queue: a c d b e f h g k m i j k" for BFS should be "Queue: a c d b e f h g k m i j l", which is I assume a typo.

"Is this a MCST?" No, none of the two is a minimum cost spanning tree, which is more commonly called minimum spanning tree (MST). The DFS spanning tree, {(a,c), (c,h), (h,g), (g,f), (f,b), (b,k), (k, j), (j,i), (i, l), (l,m), (m,e), (e,d)} could have less cost if (g,f) is replaced by (c,f). The BFS spanning tree, {(a,c), (a,d), (a,b), (a,e), (c,f), (c,h), (b,g), (b,k), (e,m), (h,j), (k,l)} could have less cost if (b,g) is replaced by (h,g). As you can see, there is no guarantee that a MST can be obtained if you use a DFS or BFS. 

In fact, for this particular graph, no matter how you conduct BFS or DFS, you will not end up with a MST. 


------------------------------------------------------------

ID: cs.stackexchange.com:129192
Title: Does DFS have better constants/complexity than Backtracking on a Graph?
Body: I came to know through some examples that DFS and Backtracking aren't exactly the same ( A misconception I had since a long time). So now my question is, since Backtracking visits nodes backwards step by step while DFS on a graph may directly jump some nodes backwards, is DFS a faster algorithm? If so, only in terms of constants or complexity wise?
Edit:
Maybe the original writeup was a bit confusing. What I mean specifically is this.
DFS:

A normal implemetation of DFS should traverse the following tree in this manner A->B->C->D. This implies that the implementation for what we normally refer to as DFS visits each node exactly once, giving it a time complexity O(V) where V is the number of vertices.
By Backtracking, what I have in mind is this:

This I feel is a more bare bones approach where one step of backtracking actually goes to a parent node and explores its other children, instead of skipping over some parent nodes which had only one child (node B here) to begin with. This results in a traversal sequence of A->B->C->B->A->D. This particular example shows that this is always going to require more nodes to be visited than the standard DFS and hence the question.


[Answer]: When you explore a graph of $N$ vertices and $E$ edges, you basically need to store the visited vertices in a $N$ boolean array. But eventually (when you are interested in retrieving path and not only connection), you will use instead a $N$ bactrack array, containing for each vertex, the one that let you reach it (the parent vertex in the exploration tree).
Exploration methods BFS and DFS needs an extra storage for a queue of vertices to visit which size is $O(N)$. One can consider that the difference between BFS and DFS is just that they respectively use a FIFO and LIFO queue. Note that DFS algorithms often use a recursion hiding the additional storage.
Now, what you call "backtrack" is just a simpler way to do the DFS as you don't have a "todo" queue. When a vertex is fully explore, you just go back in the exploration tree. Of course, it is less efficient than DFS as you will run the exploration loop several times on some vertex. One can argue that it is slightly more efficient in space as you save the queue storage, but practically, this is negligible.


[Answer]: In DFS also the traversal sequence on the above graph will be A->B->C->B->A->D->A. It is just that it prints a node when it visits it first time and marks it as visited.
In this case, it'll print A B C but when it again visits B and A it ignores it as it is already visited.
Backtracking is a programming paradigm whereas DFS is an actual algorithm that uses backtracking to traverse the tree. Hope that helps.


------------------------------------------------------------

ID: cs.stackexchange.com:142646
Title: In every DFS run on $G$, in every step of DFS, the $G_{\pi}$ is a forest
Body: Studying for my finals. So I'm reading the "Introduction to Algorithms (Third Edition)" book. In the DFS section there is the following section:

Depth-first search yields valuable information about the structure of a graph. Perhaps
the most basic property of depth-first search is that the predecessor subgraph
$G_{\pi}$ does indeed form a forest of trees, since the structure of the depth-first
trees exactly mirrors the structure of recursive calls of DFS-VISIT. That is,
$u=v.\pi$  if and only if DFS-VISIT(G,v)  was called during a search of $u$’s adjacency
list. Additionally, vertex $v$ is a descendant of vertex $u$ in the depth-first
forest if and only if $v$ is discovered during the time in which $u$ is gray.

I'm trying to prove the following statement for myself:

In every DFS run on $G$, in every step of DFS, the $G_{\pi}$ is a forest.

This question is coming from a booklet published for studding for the finals (without solutions).
I understand the logic behind why it true (with the help of the statements from the book). But I struggle of writing a "formal proof" which shows the correctness of it. Do I need to use induction to prove it (since I need to show it for every step). How to prove this statement formally?
For the completeness of the question, the $G_\pi=(V,E_\pi)$ is the following graph:
$$
E_\pi=\{(\pi[v],v)\,:\,\pi[v]\neq NULL \wedge v\in V\}
$$


[Answer]: Yes use induction. You will assume that $G_\pi$ is a forest, and you want to prove that $G_{\pi'}$ is also a forest, where $\pi'$ is $\pi$ after one step of the DFS algorithm.
The key point, is that a forest is a graph without cycles. So basically, you want to show that no new cycles where created in the last step. To prove this, you will want to have a statement similar to this:
Assume towards contradiction that $G_{\pi'}$ contains a cycle. Hence, the new node $u'$ that was added to $G_{\pi}$ in order to create $G_{\pi'}$ must be a part of the new cycle (since $G_{\pi}$ was a forest). Therefore, there must be some node $u\in G_\pi$ such that $\pi(u)=u'$, but this is impossible since it would mean that $u'$ would have been already visited in $G_{\pi}$, but this is impossible since $u'$ was visited only after $G_{\pi}$ was constructed.


------------------------------------------------------------

ID: cs.stackexchange.com:93202
Title: DFS and BFS Time and Space complexities of 'Number of islands' on Leetcode
Body: Here is the question description. The first 2 suggested solutions involve DFS and BFS. This question refers to the 1st two approaches: DFS and BFS. Apparently, the grid can be viewed as a graph.

I have included the problem statement here for easier reading. 

Given a 2d grid map of '1's (land) and '0's (water), count the number of 
islands. An island is surrounded by water and is formed by connecting adjacent
lands horizontally or vertically. You may assume all four edges of the grid are 
all surrounded by water.

    Example 1:

    Input:
    11110
    11010
    11000
    00000

    Output: 1


    Example 2:

    Input:
    11000
    11000
    00100
    00011

    Output: 3


I am unclear as to why the time complexity for both DFS and BFS is O(rows * columns) for both. I see how this is the case where the grid is just full of 0's - we simply have to check each cell. However, doesn't the DFS approach add more time to the search? Even if we mark the cells we visited by changing them to 0 in the dfs methods, we still would revisit all the cells because of the two outer loops. If dfs could be have time complexity of O(n) in the case of a big grid with large row and column numbers, wouldn't the time complexity be O(rows * columns * max[rows, cols])? Moreover, isn't the same case with the BFS approach where it is O(rows * cols * possibleMaxSizeOfQueue) where possibleMaxSizeOfQueue could again be max[rows, cols]? 

for (int r = 0; r 

How is DFS's space complexity O(rows*cols)? Is it not possible/common to consider the call stack space as freed when a recursion branch returns? 
How is the space complexity for BFS O(min(rows, cols))? The way I see it, the queue could be full of all elements in the case of a grid with just 1's thereby giving O(rows*cols) for BFS space complexity. 

DFS Solution

class Solution {
  void dfs(char[][] grid, int r, int c) {
    int nr = grid.length;
    int nc = grid[0].length;

    if (r = nr || c >= nc || grid[r][c] == '0') {
      return;
    }

    grid[r][c] = '0';
    dfs(grid, r - 1, c);
    dfs(grid, r + 1, c);
    dfs(grid, r, c - 1);
    dfs(grid, r, c + 1);
  }

  public int numIslands(char[][] grid) {
    if (grid == null || grid.length == 0) {
      return 0;
    }

    int nr = grid.length;
    int nc = grid[0].length;
    int num_islands = 0;
    for (int r = 0; r 


  Time complexity : O(M×N) where M is the number of rows
  and N is the number of columns.
  
  Space complexity : worst case O(M×N) in case that the
  grid map is filled with lands where DFS goes by M×N deep.


BFS Solution

class Solution {
  public int numIslands(char[][] grid) {
    if (grid == null || grid.length == 0) {
      return 0;
    }

    int nr = grid.length;
    int nc = grid[0].length;
    int num_islands = 0;

    for (int r = 0; r  neighbors = new LinkedList<>();
          neighbors.add(r * nc + c);
          while (!neighbors.isEmpty()) {
            int id = neighbors.remove();
            int row = id / nc;
            int col = id % nc;
            if (row - 1 >= 0 && grid[row-1][col] == '1') {
              neighbors.add((row-1) * nc + col);
              grid[row-1][col] = '0';
            }
            if (row + 1 = 0 && grid[row][col-1] == '1') {
              neighbors.add(row * nc + col-1);
              grid[row][col-1] = '0';
            }
            if (col + 1 


  Time complexity : O(M×N) where M is the number of rows
  and N is the number of columns.
  
  Space complexity : O(min(M,N)) because in worst case where
  the grid is filled with lands, the size of queue can grow up to
  min(M,N).



[Answer]: DFS takes linear time.  It only visits each vertex once; it has an explicit check, before visiting a vertex, to see whether it has been visited before, and if so, avoid visiting it again.  This should be covered in standard algorithms textbooks, so there is little point in repeating the standard analysis of DFS here -- I suggest finding a good textbook and reading their chapter on depth-first search.  I like the discussion in Dasgupta, Papadimitriou, and Vazirani's textbook, but there are many others.

See also Counting islands in Boolean matrices for an even better algorithm that has the same asymptotic running time, but lower space complexity.


------------------------------------------------------------

ID: cs.stackexchange.com:80493
Title: Open question regarding the DFS
Body: It is an open question that whether $\mathsf{DFS\text{}}$ can be done in $O(m+n)$ time and $O(n)$ bits of space, where $n,m$ represent the number of vertices and edges in a undirected graph. see this for DFS

DFS for directed graph is known to be in RNC i.e. randomised NC. $\mathsf{DFS\text{}}$ also known to be $\mathsf{P\text{-}Complete}$ problem.

My question is what would happen if someone shows that  $\mathsf{DFS\text{}}$ can be done in $O(n+m)$ time and in $O(n)$ bits of space?Is it likely in that situation for DFS to admit polyloga-rithmically fast, parallel algorithm using only polynomial resources (see this research paper)?


[Answer]: It is conjectured that $\mathsf{P} \neq \mathsf{NC}$, and so no $\mathsf{P}$-complete problem has polylogarithmic parallel algorithms. This in no way contradicts the existence of a linear time and space algorithm for DFS.


------------------------------------------------------------

ID: cs.stackexchange.com:100440
Title: Proof for BFS and DFS equivalence
Body: I'm trying to prove (by induction) that BFS in equivalent to DFS, in the sense that they return the same set of visited nodes, but I'm stuck in the middle of some of the cases.

Let $G$ be a directed graph and $u \in V(G)$. 

We want to prove that $  BFS(G,u) = DFS(G,u)$.



$\text{BASE CASE}$ 

$V(G) = \{u\}$

$BFS(G, u) = \{u\} \quad DFS(G,u) = \{u\}$

$BFS(G,u) = DFS(G,u) \quad Qed.$



$\text{INDUCTIVE HYPOTHESIS}$

$BFS(G,u) = DFS(G,u) \;\;\forall G, u \in V(G)$.



$\text{INDUCTIVE STEP}$

$(i)\;G' = (V(G) \cup\{v\}, E(G))$ 

We know that $BFS(G',u) = BFS(G,u)$ if $u \ne v$ (and so does $DFS$), because $v$ is disjoint from the rest of the graph, and so the proof follows from the hypothesis.

But if $u = v$ ?

$(ii)\;G' = (V(G), E(G) \cup (v,w)) \quad v,w \in V(G)$

How can I use the hypothesis here?


[Answer]: There is not much hope in proving $BFS(G,u) = DFS(G,u)$ directly by mathematical induction on the number of nodes in $G$ or on the degree of $u$. The problem is that as an induction hypothesis that equality does not capture the "right" kind of information or cover the "right" cases that are useful for the induction step.
Approach one: the explicit description as reachable nodes
Instead, you can try proving separately that each side is equal to the set $R$ of reachable nodes from $u$, that is, $R=\{v\in V(G)\mid \mbox{ there is a directed path from } u \mbox{ to } v \}$. More specifically,
$$R=\{u\}\cup\left\{v\in V(G)\mid \mbox{ there exist } u_0, u_1, \cdots,u_n \text{ such that }u_0=u, u_n=v, u_i\in V(G) \text{ for } 0\le i\le n \text { and }(u_i,u_{i+1})\in E(G)\text{ for } 0\le i\lt n\right\}$$
You can prove the case of $DFS$ by mathematical induction on the total number of nodes of $G$. You can prove the case of $BFS$ by mathematical induction on the distance of $v$ to $u$. Or use whatever as you see fit.
Approach two: the characterization as nodes closed under neighbourhood
A set of nodes $S$ is said to be closed under neighbourhood if for any node $n$, $S$ contains the adjacent nodes of $n$. That is, if $n\in G$ and $(n, m)\in E(G)$, then $m\in S$. Here are the critical observations on both $BFS$ and $DFS$.
Lemma 1. $DFS(G,u)$ is closed under neighbourhood.
Proof: It becomes obvious once we check what $DFS$ does when it discovers a new node.
Lemma 2. $BFS(G,u)$ is closed under neighbourhood.
Proof: It becomes obvious once we check what $BFS$ does when it pops out a node from the queue.
Lemma 1 and 2 suggest us to consider the minimal set of nodes that contains $u$ and closed under neighbourhood. Name it $C(G,u)$. It is enough to prove that both $BFS(G,u)$ and $DFS(G,u)$ are equal to $C(G,u)$. We have shown both contain $C(G,u)$. It is easy to verify "the set of nodes visited so far are contained in $C(G,u)$" is an invariant of $BFS$ on $G$ starting from $u$. The same hold for $DFS$.


[Answer]: Let's try and fix your induction. I'll transform it into an induction over $\mathbb{N}$; while not necessary -- structural induction is a thing! -- it's often easier.

Attempt one: induction over the naturals


Base case: BFS and DFS visit the same set of nodes for all graphs $G = (V, E)$ with $|V| = 1$, when started on the same node $u \in V$.
Induction hypothesis: Assume BFS and DFS visit the same set of nodes for all graphs $G = (V, E)$ with $|V| = n$, when started on the same node $u \in V$.
Inductive step: Let $G = (V, E)$ and arbitrary graph with $|V| = n+1$.

Let $U \subseteq V$ the connected component with $u \in U$. There are two cases:


$U = V$: Let $v \in V \setminus \{u\}$ arbitrary. Applying the induction hypothesis to $V' = V \setminus \{v\}$, we know that BFS and DFS visit the same set of nodes when run on the subgraph induced by $V'$.
Problem 1: We don't know anything about $v$; we have nothing in hand to cross the border!
$U \neq V$: From the induction hypothesis...
Problem 2: We know nothing about $U$ or $V \setminus U$!



Attempt two: strong induction

Problem 2 is easy to fix: strengthen the induction hypothesis to cover all small graphs:


Induction hypothesis: Assume BFS and DFS visit the same set of nodes for all graphs $G = (V, E)$ with $|V| \leq n$, when started on the same node $u \in V$.


Assuming we have established that both BFS and DFS do not visit nodes not connected to $u$, the second case is simple now.

The fundamental issue

Problem 1 persists. The problem is that for either algorithm, knowing that some set of nodes is visited tells you little about "how" $v$ is reached. The two algorithms will visit it at different points in time, via different edges. And that's the problem here: graphs and therewith the behaviour of graph algorithms are not defined solely by nodes -- but we have ignored the edges completely! And how the algorithms work, for that matter.

What you would (and should!) try now is to strengthen the claim. That is a common technique; you set out to prove more than the original claim so that the inductive hypothesis is strong enough to make the step. Here, you'd have to include the edges in some clever way.

However, since DFS and BFS treat edges very differently -- the respective sets of visited nodes after $k$ steps are wildly different! -- it is indeed unlikely that an induction for your claim will work out (nicely). A more fruitful approach is to show independently that both algorithms work correctly; along observations of the form "if a node is visited, all it's neighbours will eventually be visited", inductive proofs are indeed not too hard -- you can (and have to!) use how the algorithms actually work.


------------------------------------------------------------

ID: cs.stackexchange.com:7749
Title: DFS - Proof of Correctness
Body: I'm currently studying the book "Introduction to Algorithms - Cormen". Although a proof of correctness for the BFS algorithm is given, there isn't one any for the DFS in the book. So I was courious about how it can be shown that DFS visits all the nodes.
I also googled for it. But it seems that every lecturer do some copy-paste work from this book in their pdf's, so I couldn't find anything useful. 
By DFS we had to show that it found the shortest path. But since DFS does not calculate something like that I have no idea how to prove it.



Off the topic, why are those proofs so important? Throughout the book there are so many lemmas and theorems which can be really boring sometimes. I understand how an algorithm works in 10 minutes, perhaps need another 5 to 10 minutes to understand how to analyse the running time, but then I'm loosing 1 hour just for some useless lemmas. Besides, and worse even, I studied almost 50 proofs/lemmas of different algorithms till now, I never managed to solve one of them by myself. How can I gain the "proving ability"? Is there a systematical way to learn that? I don't mean the Hoare logic way with invariants, rather the informal way described in the book "Introduction to Algorithms". Is there any book which focuses on "how to prove algorithms" and show that in a systematical, introductory way ?


[Answer]: I had the same problem when I was an undergrad. I can see you understand the algorithm. And, to prove that it works is to explain it formally that it works. 

Why we have many proofs ? because we may have algorithms that are not correct, and we need to show that our algorithm is correct and actually work always!. 

Same for the analysis. If we want to prove for example that our algorithm is better than others, then we prove it. You may tell me that we can implement our algorithm and and test it, but this does not consider all the possible cases of input. Therefore, what we do is to prove it. 

(We usually prove the correctness and complexity of an algorithm (which is a measure of its performance)).   

This is just an answer for your second question. For your first question, a hint is to say why we will visit every other vertex if the graph is connected. [Hint: because the graph is connected than for every pair of vertices there is a path.] ...(back to your second question).. if you note here that I am using one fact to prove my original fact (i.e., there is a path connecting every pair of nodes --> DFS is correct). The first fact is clear in our case given the definition of connectivity. In many other cases, it is not clear. So we will need to prove one fact by proving another, and perhaps do that again and again. This is why we have a lot of lemmas and theorems. 

They are boring now. but once you see their magic, you will love them ! As a fun excercise try to solve simple algorithmic problems and prove their correctness and analyses. To verify your answers, re-read your answers. If they convince you, then you are correct. If not, then re-do it again. 


[Answer]: From lecture notes by Prof. Danny Sleator:

Basic DFS algorithm: Each vertex has a binary field called
"visited(v)".  Here's the algorithm in pseudo-code:

 for all v in V do visited(v) = 0
 for all v in V do if (visited(v)==0) DFS(v)


   DFS(v) {
      visited(v) = 1
      for all w in adj(v) do if (visited(w)==0) DFS(w)
   }


Claim 1: DFS is called exactly once for each vertex in the graph.

Proof: Clearly DFS(x) is called for a vertex x only if
visited(x)==0.  The moment it's called, visited(x) is set to
1.  Therefore the DFS(x) cannot be called more than once for
any vertex x.  Furthermore, the loop "for all v...DFS(v)"
ensures that it will be called for every vertex at least
once.   QED.

Claim 2: The body of the "for all w" loop is executed
exactly once for each edge (v,w) in the graph.

Proof:  DFS(v) is called exactly once for each vertex
v (Claim 1).  And the body of the loop is executed once for
all the edges out of v.  QED.

Therefore the running time of the DFS algorithm is O(n+m).


[Answer]: AJed puts things very nicely, and ajmartin has provided you with a proof too. I would like to add the following to your second question:

If you really understood the algorithm, then you should also understand these properties of the algorithm, and be able to explain them to others informally - say to friends or colleagues (such as how and why a shortest path is found). If you don't realize these properties on your own, you should at least realize them once you are told (such as by being asked to prove them). If you can do that, from there it is just a matter of writing down things formally (which can be achieved with practice). If you can't explain that, or don't understand things like why it visits every node, then you have not really understood the algorithm - just understood how to implement it.

I know proofs are often written in a more complicated manner than necessary. And can be difficult to understand easily. But that should not undermine the importance of proofs - it just shows proofs should be as simple as possible without being ambiguous/loose/incorrect.

Proofs make things water-tight. It allows the algorithm to form the basis of more complex analysis/algorithms/theories without having to be re-assessed. You need to show that the algorithm is correct for all inputs that it claims to handle. Just intuition may not always be sufficient (or sometimes may also be misleading). You say that in software development, there are ways to test things for most cases. But if the algorithm is proven correct, all you need to check is that the algorithm has been implemented correctly.

Put another way, what if someday you come up with a brilliant algorithm for some problem in your work. The algorithm is complex and no-one understands it. How do you convince others that the algorithm works, and that they could use it too? Equally (or more?) importantly, how would you convince your boss that your brilliant idea actually works and won't cause cataclysmic failures for some cases which have not been tested - especially since no one else has used the algorithm before? The answer is you prove the correctness of your algorithm, and show that it has been correctly implemented. You need proof techniques for that, and so are taught these in university - unlike your claim in your comment as posted as answer - it is not something only useful for masters or phd.

A side-effect of proofs is that they often offer deeper insight into the algorithm. Taking the very trivial case of your question, if you read a proof of why every edge and vertex is visited exactly once, or why and how it finds a shortest path, you will be able to better understand and visualize the working of the algorithm.

As for why there are so many lemmas, it is the same thing as programming. You could write the whole block of proof at once (as you could write the whole code in one chunk), but that would be unwieldy, tough to understand and error-prone. If, however, you write modular code, it becomes easy to re-use the modules (functions/classes), easier to test, verify, understand and abstract out complications. The same is true for proofs. You break it down into simpler "modules" - they are just called lemmas/claims etc. You get more or less the same benefits from lemmas that you get from modular code.


------------------------------------------------------------

ID: cs.stackexchange.com:129700
Title: Bottom-up Mergesort vs Natural Merge Sort
Body: I ran into natural mergesort in this Wikipedia page: https://en.wikipedia.org/wiki/Merge_sort#Natural_merge_sort
However, I can't find much information regarding:

The algorithm(s) to achieve natural mergesort, and the pros and cons of each method
How natural mergesort compares to bottom-up mergesort (pros and cons of each w.r.t the other)

In short, I'm looking for resources that would teach me algorithms to do natural mergesort, and how to weigh it against the usual bottom-up mergesort


[Answer]: Natural Mergesort usually refers to Timsort. In terms of comparison to bottom-up mergesort, Timsort has $O(n)$ best-case running time where as bottom-up mergesort is at least $O(n\log n)$ on any input.
However, analysis of the worst-case running time of Timsort proves to be more tricky; an 2018 paper [1] establishes that its worst-case time complexity is $O(n\log \rho)$, where $\rho$ is the number of maximal monotonic sequences in the input array.
[1]: Nicolas Auger, Vincent Jugé, Cyril Nicaud, Carine Pivoteau, "On the Worst-Case Complexity of TimSort"


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:297160
Title: Why is mergesort O(log n)?
Body: Mergesort is a divide and conquer algorithm and is O(log n) because the input is repeatedly halved. But shouldn't it be O(n) because even though the input is halved each loop, each input item needs to be iterated on to do the swapping in each halved array? This is essentially asymptotically O(n) in my mind. If possible please provide examples and explain how to count the operations correctly! I haven't coded anything up yet but I've been looking at algorithms online. I've also attached a gif of what wikipedia is using to visually show how mergesort works.




[Answer]: It's O(n * log(n)), not O(log(n)). As you've accurately surmised, the entire input must be iterated through, and this must occur O(log(n)) times (the input can only be halved O(log(n)) times). n items iterated log(n) times gives O(n log(n)).

It's been proven that no comparison sort can operate faster than this. Only sorts that rely on a special property of the input such as radix sort can beat this complexity. The constant factors of mergesort are typically not that great though so algorithms with worse complexity can often take less time.


[Answer]: The complexity of merge sort is O(nlog(n)) and NOT O(log(n)).
Merge sort is a divide and conquer algorithm. Think of it in terms of 3 steps:

The divide step computes the midpoint of each of the sub-arrays. Each of this step just takes O(1) time.
The conquer step recursively sorts two subarrays of n/2 (for even n) elements each.
The merge step merges n elements which takes O(n) time.

Now, for steps 1 and 3 i.e. between O(1) and O(n), O(n) is higher. Let's consider steps 1 and 3 take O(n) time in total. Say it is cn for some constant c.
How many times are we subdividing the original array?
We subdivide the input until each sub-array has one item so there are exactly log(n) "subdivision stages". We undo each subdivision stage with a "merge stage".
For example, if n = 8, there is a total of 3 merge stages: one where each pair of n/8 sub-arrays are merged to form a single n/4 sub-array, one where pairs of n/4s are merged to form n/2s, and one where the pair of n/2 are merged to form n.
What is the time cost for merging all pairs at each merge stage?
For this, look at the tree below - for each level from top to bottom:

Level 2 calls merge method on 2 sub-arrays of length n/2 each. The complexity here is 2 * (cn/2) = cn.
Level 3 calls merge method on 4 sub-arrays of length n/4 each. The complexity here is 4 * (cn/4) = cn.
and so on ...

At each merge stage, the total cost for merging all pairs is O(cn). Since there are log(n) merge stages, the total complexity is: (cost per stage)*(number of stages) = (cn)*(log(n)) or O(nlog(n)).

Image credits: Khan Academy


[Answer]: Merge Sort is a recursive algorithm and time complexity can be expressed as following recurrence relation.


  T(n) = 2T(n/2) + ɵ(n)


The above recurrence can be solved either using Recurrence Tree method or Master method. It falls in case II of Master Method and solution of the recurrence is ɵ(n log n).

Time complexity of Merge Sort is ɵ(nLogn) in all 3 cases (worst, average and best) as merge sort always divides the array in two halves and take linear time to merge two halves.

It divides input array in two halves, calls itself for the two halves and then merges the two sorted halves. The merg() function is used for merging two halves. The merge(arr, l, m, r) is key process that assumes that arr[l..m] and arr[m+1..r] are sorted and merges the two sorted sub-arrays into one. See following C implementation for details.

MergeSort(arr[], l,  r)
If r > l
     1. Find the middle point to divide the array into two halves:  
             middle m = (l+r)/2
     2. Call mergeSort for first half:   
             Call mergeSort(arr, l, m)
     3. Call mergeSort for second half:
             Call mergeSort(arr, m+1, r)
     4. Merge the two halves sorted in step 2 and 3:
             Call merge(arr, l, m, r)




If we take a closer look at the diagram, we can see that the array is recursively divided in two halves till the size becomes 1. Once the size becomes 1, the merge processes comes into action and starts merging arrays back till the complete array is merged.


[Answer]: Comparison based sort algorithms have a lower bound (n*log(n)), which means it's not possible to have a comparison based sorting algorithm with O(log(n)) time complexity. 

By the way, merge sort is O(n*log(n)).
Think it this way.

[ a1,a2,         a3,a4,         a5,a6,          a7,a8     .... an-3,an-2,     an-1, an ] 
   \ /            \  /           \ /             \  /            \  /            \  /    
    a1'            a3'            a5'             a7'            an-3'           an-1'    
      \            /                \             /                 \             /
            a1''                          a5''                       an-3''
             \                             /                         /
                          a1'''                                     /
                           \
                                              a1''''


This looks a reversed binary tree.

Let the input size be n. 

Each a_n represents a list of elements. First line's a_n have only one element.

At each level, the sum of the merge cost on average is n(there exists corner cases which the cost is lower[1]). And the tree's height is log_2(n).

So, the time complexity of merge sort is O(n*log_2(n)).


  [1] if sorting on a list that is already sorted, which is called the best case. the cost reduced to n/2 + n/4 + n/8 + .... + 1 = 2^log_2(n) -1 ~ O(n). (assume the length n is power of two)



[Answer]: Sorting is a NP-Complete problem in computer science (Non Polynomial problem). This means that, unless mathematically proven, you can't go below O(n log n) when sorting an list of elements.

Check this article in Wikipedia (https://en.wikipedia.org/wiki/P_versus_NP_problem)

Basically so far nobody managed to prove that (P == NP) and if you do, you first become millionaire, second you start world war III due to the fact that you will be able to break all the pub/private key security mechanisms used everywhere nowadays :)


------------------------------------------------------------

ID: cs.stackexchange.com:68179
Title: Combining merge sort and insertion sort
Body: A cache-aware sorting algorithm sorts an array of size $ 2^{k} $  with each key of size 4 bytes. The size of the cache memory is 128 bytes and algorithm is the combinations of merge sort and insertion sort to exploit the locality of reference for the cache memory (i.e. will use insertion sort when problem size equals cache memory).


  What are the best case and worst case running time of the algorithm?



[Answer]: If each word is 4-byte long, then a 128-byte cache contains 32 words. Therefore, if the problem size is 32 or less, the instance will be immediately solved by Insertion Sort, otherwise it will be split in two sub-problems with the same size, which will then have to be merged.

Try drawing the recursion tree. What does each leaf do? How much does that contribute to the overall complexity? What do internal nodes do? I'll let you work out the details. 


[Answer]: I will only consider worst case, leaving best case to you. The running time $T(n)$ of merge sort on an instance of length $n$ (words) satisfies the recurrence relation
$$
T(n) = 2T(n/2) + O(n),
$$
with a base case of $T(1) = O(1)$. In your case, the base case is different: $T(32)$ is the running time of insertion sort on an array of length $32$. Since 32 is constant, we can write $T(32) = O(1)$, and then solve the recurrence relation as in pure merge sort. The result will be exactly the same - the difference between the algorithms (pure merge sort and modified merge sort as in your question) is hidden in the big O constant.


[Answer]: Basic Idea

For small values of n insertion sort runs faster than merge sort . Hence insertion sort can be used to Optimize merge sort

Basic idea is apply insertion sort on sublists obtained in merge sort and merge the sorted (using insertion sort) lists


Coming to Question
Base Condition
If each word is 4-byte long, then a 128-byte cache contains 32 words. Therefore, if the problem size is 32 or less, the instance will be immediately solved by Insertion Sort, otherwise it will be split in two sub-problems with the same size, which will then have to be merged
Time Complexity

Total Complexity = Complexity Insertion Sort + Complexity of Merge Sort

We have $n$ elements . We are dividing it to $\frac{n}{x}$ sublist each contain x elements each

Complexity Of Merge Sort

Total work done by merge sort in copying $n$ elements from each level to upper level is $O(n)$
Number of levels = number of sublists or small problem = $O(\log \frac{n}{x})$
Complexity of Merge sort : $O( n \log \frac{n}{x} )$

Complexity Of Insertion Sort

We are sorting $ \frac{n}{x} $ lists each of $x$ elements using insertion sort so complexity contributed by insertion sort is $\frac{n}{x} $ INSERTION ${x}$


Total Complexity =  $\Theta ( \frac{n}{x}  \ \text{INSERTION} \ (x)  +  n \log \frac{n}{x} )$

Best Case

is when Insertion sort perform in best case with $O(n)$ ie $\text{INSERTION} \ (x) = x$
Best Case Complexity=Total Complexity =  $\Theta ( n  +  n \log \frac{n}{x} )$

Worst Case

is when Insertion sort perform in best case with $O(n^{2})$ ie $\text{INSERTION} \ (x) = x^{2}$
Best Case Complexity=Total Complexity =  $\Theta ( nx  +  n \log \frac{n}{x} )$

More Information : Gate Overflow - Sorting Algorithm


------------------------------------------------------------

ID: cs.stackexchange.com:92369
Title: What is the time complexity of MergeSort without its penultimate merge?
Body: As in, with the two final sublists which are sorted arrays, let's say just for this example (yes it is no longer fully MergeSort but I am interested in the time complexity only), that we finish with a constant time operation involving the two sublists. MergeSort in worst case is $O(n\log(n))$, and not having the final merge would definitely lead to greater efficiency than this, but by how much? Again, I understand that MergeSort has not completed it's full run, I am only interested with its time complexity for my specified run only.


[Answer]: It will still take $O(n \log n)$.

There are four steps in mergesort.


Divide the list in half.
Mergesort the left half.
Mergesort the right half.
Merge the two together.


If I understand right, you're asking about removing the fourth step, in the final stage only (that is, not in any of the recursive calls).

In this case, you'll still have to do step 1 in $O(1)$, step 2 in $O(\frac{n}{2} \log \frac{n}{2})$, and step 3 in $O(\frac{n}{2} \log \frac{n}{2})$ (since steps 2 and 3 still call conventional mergesort, which is $O(n \log n)$).

So your total time complexity will be $O(1 + \frac{n}{2} \log \frac{n}{2} + \frac{n}{2} \log \frac{n}{2}) = O(n \log \frac{n}{2}) = O(n \log n)$.

The final merging only takes $O(n)$ time; it's the recursive part that's the real cost.


------------------------------------------------------------

ID: cs.stackexchange.com:124397
Title: How can divide by 2 blocksize bubblesort followed by a final mergesort be optimized in a particular environment?
Body: I am wondering if we had a large array to sort (let's say 1,048,576 random integers), chosen because it is a perfect power of 2, if we can just keep dividing those blocks into smaller and smaller half size blocks, how would someone know (on a particular computer using a particular language and complier) what the ideal blocksize would be to get the best actual runtime speed using mergesort to put them all back together?  For example, what if someone had 1024 sorted blocks of size 1024, but it could that be beaten by some other combination?  Is there anyway to predict this or someone has to just code them and try them all and pick the best?  Perhaps for simplicity they would want to use some simple bubblesort on the 1024 size blocks, then merge them all together at the end using mergesort. Of course the mergesort portion would only work on 2 sorted blocks at a time, merging them into 1 larger sorted block.

Also, what about the time complexity analysis on something like this?  Would all divide and conquer variations of this be of the same time complexity?  The 2 extremes would be 2 sorted blocks (of size 524,288) or 1,048,576 "sorted" blocks of size 1, handed over to a merge process at that point.


[Answer]: The normal way to tell is through measurement: we try different choices for the threshold, benchmark each one, and choose the best.  This can be tricky, as the best threshold might vary from computer to computer (depending on, e.g., the relative speeds of CPU vs RAM, the size of various caches, and so on).  So, a plausible approach might be to benchmark on multiple different platforms and then choose one threshold that seems to do ok on most of them.

Asymptotic running time analysis probably won't be very helpful, because the constants matter a lot; and because the cache/memory hierarchy probably matters a lot, and standard running time analysis doesn't model the memory hierarchy very well.


------------------------------------------------------------

ID: cs.stackexchange.com:54088
Title: What is the significance of a Θ-bound on the running time of Mergesort?
Body: While studying algorithm analysis I found that there is something called tight bound and there is some mathematical formula to support it.
Given:


  Mergesort takes $\Theta(n \log n)$ compares to sort $n$ elements.


What insight can we get from the above statement?


[Answer]: The insight you get from that statement is that the number of comparisons that merge sort uses behaves like a constant time $n\log n$. That's exactly what the statement means. In many cases the running time is captured by the number of comparisons, so the running time of merge sort behaves like a constant time $n\log n$. You can use this to estimate how long merge sort will take on a given number of elements (though you'll have to empirically estimate the constants involved). You can also use this estimate to compare merge sort to other algorithms, which might use asymptotically more comparisons.


[Answer]: 
  Mergesort takes $Θ(n \log n)$ compares to sort n elements [in the worst case].


Just unfold definitions. Step one:


  Mergesort takes $O(n \log n)$ and $\Omega(n \log n)$ compares to sort $n$ elements [in the worst case].


Step two (and three, I guess, since I recombine the unfolded definitions of $O$ and $\Omega$):


  For some $n_0 \in \mathbb{N}$ and $a,b \in \mathbb{R}_{>0}$, the number of comparisons $C_n$ Mergesort uses to sort $n$ elements [in the worst case] is bounded by $a \cdot n \log n \leq C_n \leq b \cdot n \log n$ for all $n \leq n_0$.


That's it. Would I call that insight? Hardly.

Adding the justification for why we ignore constant factors in the first place and that comparisons are a dominant operation in Mergesort, I guess the intended statement is something like this:


  If you implemented this Mergesort algorithm on a machine [that fits the RAM model and can compare all input numbers in constant time], its running-time cost would be roughly proportional to $n \log n$ for large enough $n$.


Note that neither the proportionality constant nor the sense of "roughly" and "large enough" are specified by the formal definitions. It may very well be that the algorithm is utterly unusable for all instances relevant to you. Hence, statements about algorithm performance using Landau notation are, for practical purposes, mostly vacuous -- formally speaking.

Implicitly, "we know" that "usually" constants are small and lower-order terms behave "nicely" (see e.g. here for a more precise result on Mergesort) so comparing Landau bounds is a somewhat meaningful criterion for choosing algorithms in practice. It has serious limitations, though, and it is important to keep those in mind.


------------------------------------------------------------

ID: cs.stackexchange.com:30772
Title: What is the runtime of Mergesort if we switch to Insertion Sort at logarithmic depth?
Body: 
  Consider the Mergesort algorithm on inputs of size $n = 2^k$. Normally, this algorithm would have a recursion depth of $k$. Suppose that we modify the algorithm so that after $k/2$ levels of recursion, it switches over to insertion sort. What is the running time of this modified algorithm?


I know that MergeSort has worst-case runtime of $O(n \log n)$ and Insertion Sort has worst-case runtime of $O(n^2)$, but I'm not sure how these bounds will be affected within the problem above.


[Answer]: You know that MergeSort has a complexity of $O(n\log n)$. Why? We can write a recurrence for the running time of MergeSort:
$$ T(n) = 2T(n/2) + Cn, \quad T(1) = D. $$
(Assume for simplicity that $n$ is a power of 2.) The solution is $T(n) = O(n\log n)$. Why? Since we can write
$$ T(n) = 2T(n/2) + Cn = 4T(n/4) + Cn + Cn = 8T(n/8) + Cn + Cn + Cn = \cdots = (\log n)Cn + Dn. $$
That is, at each internal level the total complexity is $Cn$, there are $\log n$ levels, and at the bottom the complexity is $Dn$. It is instructive to rewrite it in terms of $n = 2^k$:
$$ T(2^k) = 2^1T(2^{k-1}) + C2^k = 2^2T(2^{k-2}) + C2^k + C2^k = \cdots. $$
In your case, you want to stop the recursion at depth $k/2$, and instead of expanding $2^{k/2}T(2^{k/2})$ using the recursion, apply the $En^2$ insertion sort algorithm. What would you get? (that's a question for you)


------------------------------------------------------------

ID: cs.stackexchange.com:41479
Title: What is the worst case running time for an algorithm that combines insertionsort and mergesort?
Body: Suppose that we have an algorithm "combination" that uses insertionsort for $n 

I was thinking that it's simply $n^2$, since the algorithm "combination" allows inputs larger than 100, in which case the worst case running time is $n^2$.


[Answer]: In fact, the running time of mergesort for large $n$ doesn't depend on what happens for small $n$. Recall the recurrence for mergesort:

$$ T(n) = 2T(n/2) + O(n), \qquad T(1) = O(1). $$

This recurrence holds even if you run insertion sort for $n 


------------------------------------------------------------

ID: cs.stackexchange.com:37561
Title: Merge Sort proof
Body: I am trying to prove that merge sort is indeed $O(n \log n)$.

I was able to extract a pattern using constants, however now I am stuck. This is as far as I can get:


$T(n) = 2T(n/2) + cn$
$T(n/2) = 2T(n/4) + c(n/2)$


Now plug in 1. into 2.


$T(n) = 2(2T(n/4) + c(n/2)) + cn$
$T(n) = 4T(n/4) + 2cn$


Now the pattern I was able to find is the following:


$2^kT(n/2^k) + kcn$


Is there a way to use this pattern to prove that merge sort has complexity of $O(n\log n)$?


[Answer]: You correctly figured out that after unrolling the recursive equation
$$T(n) = 2 \cdot T(n/2) + c n$$
$k$-times you get
$$T(n) = 2^k \cdot T(n/2^k) + k c n.$$
To finish your proof, ask yourself when the unrolling process will stop. The answer: when we reach the base case, which is $T(1) = d$ where $d$ is a constant. For what value of $k$ do we reach $T(1)$? For this we need to solve the equation
$$n/2^k = 1$$
whose solution is $k = \log_2 n$. So now we plug in $k = \log_2 n$:
\begin{align*}
T(n) &= 2^{\log_2 n} \cdot T(n/2^{\log_2 n}) + c  \cdot n \cdot \log_2 n \\
     &= n \cdot T(1) + c  \cdot n \cdot \log_2 n \\
     &= d \cdot n + c  \cdot n \cdot \log_2 n \\
     &\in O(n \cdot log_2 n).
\end{align*}


------------------------------------------------------------

ID: cs.stackexchange.com:111963
Title: Worst Case Space Complexity of Merge Sort and Bubble Sort
Body: I understand that the worst space complexity of Bubble Sort is constant O(1), since all the space we need is the array where the elements were stored. But why is Merge Sort's worst space complexity O(n), linear? All space we need is the exact number of the elements, right? Please enlighten me.


[Answer]: Typical implementations of Merge sort use a new auxiliary array split into two parts, a left part and a right part.  This extra space is the reason for the O(n) space complexity.  

During the sort section of the algorithm we have the following two new auxiliary arrays created for additional space.  

int leftPartition[] = new int[ leftPartitionSize ];
int rightPartition[] = new int[ rightPartitionSize ];


So, merge sort uses auxiliary arrays to sort.  However, quick sort is an in-place sort that uses O(1) space as no auxiliary space is required to sort the data.  

Side note:  You might also see the space complexity of merge sort as O(nlog(n)).  Some would argue that because of the way the recursive calls to merge sort work, the space complexly at any given point is at most O(n).  Others might say that adding all the space for all recursive calls requiresO(nlog(n)).  The log(n) is because each repeated call to merge sort cuts the existing array in half.  

If you want true O(n) complexity, you can pass an auxiliary array as a parameter into the sort method.  Therefore, each call to merge sort will use the same array without the need to split the array in half with each call.   However, this implementation is quite a bit harder to implement.  


------------------------------------------------------------

ID: cs.stackexchange.com:101312
Title: Compute the general time complexity of a merge sort algorithm with specified complexity of the merge process
Body: The problem was from an exam, I spent much time wrapping my head up around this kind of problems, so I decided to ask for help ;(

Problem:

We implement a merge sort algorithm to sort $n$ items. The algorithm will divide the set into 2 roughly equal-size halves, and merge the 2 halves after each half set is recursively sorted. Because the item comparison is complicated, the merge process takes $\theta(m\sqrt{m})$ steps for input size $m$. What is the time complexity for this algorithm?

(a) $\theta{(n\log{n})}$ 
(b) $\theta{(n)}$ 
(c) $\theta{(n^2)}$ 
(d) $\theta{(n\sqrt{n})}$ 
(e) $\theta{(n\sqrt{n}\log{n})}$ 

What I tried:


I know the merge sort normally can be written $S(n) = 2S(\frac{n}{2}) + n, S(1) = 1$, where the recursive function $S$ is the step cost of the merge sort for the size $n$.
But the problem specifies the step costs to merge is $\theta(n\sqrt{n})$, I have to rewrite it as $S(n) = 2S(\frac{n}{2}) + \theta(n\sqrt{n}) = 2S(\frac{n}{2}) + n\sqrt{n}$. 
I have no idea how to transform it into the general solution...


Please help me, I will learn a lot from this problem! Thanks :)


[Answer]: You are on the right track. 

Now that we have the recurrence relation $$S(n) = 2S(\frac{n}{2}) + \Theta(n\sqrt{n}),$$ Applying the case three of the master theorem, where $a=2$, $b=2$, $\epsilon=\frac12$, we will have $S(n)= \Theta( n\sqrt n)$. 

I was stretching a bit when we were applying the master theorem since the regularity condition, the existence of such $c>0$ required for case three is not necessarily satisfied. What I really meant is that we can find two constant $0 such that $c_1n\sqrt n\le S(n)-2S(\frac n2) \le c_2n\sqrt n$ for all $n$. Define $S_1$ by $S_1(n) = 2S_1(\frac{n}{2}) + c_1n\sqrt{n}$ and $S_2$ by $S_2(n) = 2S_2(\frac{n}{2}) + c_2 n\sqrt{n}$ with the same initial condition as $S$. Now we can apply case three of the master theorem to $S_1$ and $S_2$ to get the same $\Theta$ bound, since the regularity condition is satisfied with $1>c=0.8>\sqrt2/2$. Since $S_1(n)\le S(n)\le S_2(n)$, S(n) has the same $\Theta$ bound.

In case you have not learned/are not allowed to use the master theorem, you may have to more or less prove case three of the master theorem by yourself. You could find a proof, for example, in CLRS.


------------------------------------------------------------

ID: cs.stackexchange.com:68113
Title: Time Complexity of Sort-Merge Join
Body: According to this German Wikipedia article, the time required to merge relations $R$ and $S$ is $\in \mathcal{O}(|R| + |S|)$ if both relations are already sorted. 
[Note: You don't really need to read the text and the link jumps right to where the time complexity is stated. But I added a translation of that section to this question for some clarity.]

Assume $R = S$ with $R$ having 2 columns. One is a random number and one is always 5 for every line. Join $R$ and $S$ on the column which is 5 for every line. The resulting output's space complexity is $\in \Theta(|R| \cdot |S|)$. Time complexity is always $\in \Omega(\text{spaceComplexity})$.

How can the time complexity stated by the Wikipedia article be true?



Here is the "Sort-Merge Join" section translated to English. Sorry, no markdown quote, just everything from here on is the quote. It's translated poorly (not how you would write it in English if you know what you're writing) in some parts to preserve the meaning as well as possible. I marked my own inline comments with ///.

Quote of https://de.wikipedia.org/wiki/Joinalgorithmen#Sort-Merge_Join

Sort-Merge Join

Both relations get sorted by their join attributes. The result can be determined via a single scan through both sorted relations.

The algorithm is only suited for natural join and equi-join.

Pseudocode

Implementation of $R\bowtie_{R.a=S.a} S$ in pseudocode:

p_r := first tuple in R
p_s := first tuple in S
while(p_r != endof_r && p_s != endof_s)
    // Collect all tuples in S with the same join attributes.
    M_s := set with contents p_s /// Yes, it says "with", not "of".

    foreach(t_s in S > p_s)
        if(t_s.a = p_s.a)
            M_s += set with contents t_s
        elseif // I think they mean "else".
            p_s := t_s
            break foreach
        endif
    endforeach

    // Seach suitable start tuple in R. /// "passend" can also be translated "fitting" or "matching", not just "suitable".
    foreach(t_r in R > p_r)
        p_r = t_r
        if(t_r.a >= t_s.a)
            break foreach
        endif
    endforeach

    // Output tuples.
    foreach(t_r in R > p_r)
        if(t_r.a > t_s.a)
            break foreach
        endif

        foreach(t_s in M_s)
            Write output: (t_r, t_s)
        endforeach
        p_r = t_r
    endforeach
endwhile


Evaluation

Sorting can be done with effort $\mathcal{O}(|R|\log|R|+|S|\log|S|)$. The number of block accesses to sort $S$ is $b_s\left(2\log\frac{b_s}{b_{free}}\right)+b_s$ in the worst case, analogous for $R$.

A merge of both relations after sorting them costs $\mathcal{O}(|R|+|S|)$. In the best case – i.e. the relations are already sorted –, the costs of merging are the only ones.

In the normal case, the total costs are $\mathcal{O}(n\log n)$.

Variants

[Not translated because it doesn't seem to be important for the question.]


[Answer]: You are absolutely correct.  Wikipedia has an error -- or perhaps, if we are feeling more charitable, we could call it an oversimplification.

It is not true that the running time is at most $O(|R|+|S|)$.  For instance, if we consider the case where the value of attribute $a$ is 42 for all elements of $R$ and $S$, we output $|R| \times |S|$ tuples.  It is also easy to see that the pseudocode does $|R| \times |S|$ iterations of the nested inner loop (i.e., that many iterations of the statement "Write output:"), so the running time is also $O(|R| \times |S|)$.

Here are some statements that are true:


If there are no repeated values of $a$ (i.e., each value for $a$ appears at most once in $R$ and at most once in $S$), then the running time for the merge is $O(|R|+|S|)$, and the size of the output is also $O(|R|+|S|)$.
If any value appears at most $c$ times in attribute $a$, where $c$ is a constant, then the running time for the merge is $O(|R|+|S|)$, and the size of the output is also $O(|R|+|S|)$.
If we count only the I/O complexity (the number of disk/cache transfer operations), and if memory is large enough to hold $|S|$ items, then the I/O complexity is $O(|R|+|S|)$... though the running time and the size of the output might be as large as $O(|R| \times |S|)$.  More generally, the same statement is true if any value for attribute $a$ appears at most $k$ times in $S$, and main memory is large enough to hold at least $k$ items (here $k$ does not need to be a constant).


So Wikipedia's statement is misleading or wrong or (at best) over-simplified.  Perhaps what they really meant was one of the bullet items above.  Your understanding is absolutely correct.  


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:389919
Title: When to use Quicksort instead of MergeSort?
Body: I have the following homework question: You develop a video surveillance software that must run on real-time data. In one of the steps of the algorithm executed with each image captured by the camera, you have to sort some numbers.
Considering that the average execution time of QuickSort is better than that of MergeSort, which
Sorting should you consider using? justify

I know that QuickSort isn't good with larger dataset as opposed to MergeSort.
I also think that because we are working with data that continuously updates speed is incredibly important. But considering that its real-time that means that the size of the data set is small initialy and then grows. 

It seems like a trap to go for QuickSort because the question is heavily promoting it. On the other hand MergeSort is more stable it always have a complexity of O(n log n). I don't know which to choose and how to justify it properly.


[Answer]: Likely there isn't necessarily a single correct answer, but your professor wants to see how well you know the properties of both algorithms and whether you can justify a set of constraints /  goals in the given setting that make you prefer one (for good reasons). 

That said, the hint that it's real-time processing strongly suggests one line of reasoning that you already seem to have come up with half-way. 


[Answer]: The point here is that average performance is not relevant in this case. You want an algorithm that, in a worst case scenario, will still fit in the time slot you have been granted. That time slot is one frame period. When you are done processing you have to wait for the next frame anyway. Then it is better to need 90% of the slot in a worst case scenario that occurs often then to do it in 25% of the slot for 99% of the cases but to fall short in the remaining 1%.


------------------------------------------------------------

ID: cseducators.stackexchange.com:5281
Title: Why and how is it efficient to process sorted arrays than unsorted arrays?
Body: It is well accepted that processing sorted arrays is easier and efficient. What would be the pedagogical approach to explain how and why it is more efficient to process sorted collections than unsorted collections.


[Answer]: First, it is not "well accepted" that processing sorted array is more efficient. It depends on what you do. If you spend your time inserting, for instance, then sorted arrays are not necessarily a good choice. You are better off inserting at the end (say if you have resizable arrays such as Vector or ArrayList in Java), and then sort afterwards (you'll pay $O(N*log(N))$ instead of $O(N²)$).

It is true that if you don't have insertions (or very few insertions with respect to searching or other operations for instance) then it is more efficient.
If you want to find an element in a sorted array, you can use binary search which is $O(log(N))$ while if your array is not sorted you have to scan the whole thing which is $O(N)$.

Likewise if you want to perform set operations on two sorted arrays (intersection, union, difference), then doing so on sorted Arrays is linear. If they are not sorted you will be either quadratic (naive algorithm) or have to use an auxiliary data structure (such as a hash table).

Sorted arrays have the same complexity for these operations than the more involved tree structures (AVL or red-black trees) and they have a very compact and simple representation. But you pay the price that insertion is more expensive, since it has to maintain the order of elements.


[Answer]: I would do it unplugged. Give then a sorted stack of cards, get them to find a number. Repeat with an unsorted list. 

Ensure that there are gaps, not a sequence, but sorted. I got caught out with this, pupils could find a card in a sequence in $O(1)$.


[Answer]: If an array is unsorted, and you want to check an item for membership in it, that is O(n) time procedure.  If it is sorted, you can use divide-and-conquer to do this task in O(log(n) time.  


------------------------------------------------------------

ID: cs.stackexchange.com:144059
Title: Why is my implementation of Dijkstra's Algorithm using min heap faster than using an unsorted array for a complete graph?
Body: Based on theory, the implementation using adjacency matrix has a time complexity of E+V^2 and the implementation using min heap has a time complexity of (E+V)logV where E is the number of edges and V is the number of vertices.
When E>>V, such as for a complete graph the time complexity would be V^2 and (V^2)logV. This would mean that the implementation using min heap should be slower.
However I tested both implementations and found that the runtime for min heap is faster. Why is this so?
Here is my implementation:

adjacency matrix and unsorted list

def dijkstraUnsortedArr(graph, start):
    distances = [math.inf for v in range(len(graph))]
    visited = [False for v in range(len(graph))]
    predecessors = [v for v in range(len(graph))]
    distances[start] = 0

    while True:                                                                               
        shortest_distance = math.inf
        shortest_vertex = -1
        for v in range(len(graph)):                                               
            if distances[v] 

adjacency list and min heap

def dijkstraMinHeap(graph, start):
    distances = [math.inf for v in range(len(graph))]
    visited = [False for v in range(len(graph))]
    predecessors = [v for v in range(len(graph))]
    heap = Heap()

    for v in range(len(graph)):
        heap.array.append([v, distances[v]])
        heap.pos.append(v)

    distances[start] = 0
    heap.decreaseKey(start, distances[start])
    heap.size = len(graph)

    while heap.isEmpty() == False:                                 
        min_node = heap.extractMin()                               
        min_vertex = min_node[0]

        for v, d in graph[min_vertex]:
            if not visited[v]:
                if (distances[min_vertex] + d)  0 and self.array[i][1] 
Here's the graph of the runtime against the number of vertices v:



[Answer]: It depends on the input graph also. Perhaps, heap.decreaseKey() operation is not happening as frequently as it should. For example, consider a complete graph: $G = (V,E)$ such that all its edge weights are $1$.
In this case, the heap implementation will work faster since distance[v] for every vertex will be set to $1$ in the first iteration. The heap.decreaseKey() operation will not happen more than once on any vertex. Therefore, the complexity of the heap based approach here is $O(|E| + |V| \log |V|)$.
On the other hand, in the case of unsorted list approach, you will be computing the shortest distance $|V|$ times and computing it every time takes $\Theta(|V|)$ time. Therefore, in such a graph the time complexity of the unsorted list approach is $O(|E| + |V|^2)$.
You should check with your input graph. Try with random weights on the edges and random source vertex, then you will surely see that unsorted array approach will be better than the heap approach in the case of complete graphs.


[Answer]: In short: Your test sample is too small or unvaried.
The $O$ notation allows for arbitrary multiplicative and additive constants everywhere, and for edge cases in the input data (i.e., a list being sorted vs. unsorted vs. sorted backwards etc. could introduce very unexpected results; and it does not need to be obvious either).
For things like this I would design some test program which runs for a long time (minutes at least), is pretty random, and varies the input data meaningfully. This may or may not be hard to achieve; sometimes it's best to grab your data from "out of the system" to avoid your test case generator algorithm to somehow mesh with the algorithm under test and thus always generating "special" data.
Finally; if you are not designing your algorithm in thin air - i.e., if it's not library code but some application solving a real problem - then by all means test with a large sample size of your actual domain data. That way, you can actually exploit those edge cases in the complexities and the algorithm. For example, a Dijkstra-style algorithm pathfinding in OpenStreetMap real world data might just need different optimization than, say, a similar algorithm finding paths for a game character on a board of square or hexagonal fields...


[Answer]: Here is a worst-case example of a complete graph where the heap.decreaseKey() operation executes on every edge:
Let the vertices be $V = \{1,2,\dotsc,n\}$. The edge set $E$ is such that for every vertex $i$ and $j$ such that $i, there is an edge of unit weight if $j = i+1$; and there is an edge of weight $2(n-i)$ if $j > i+1$.
Run the heap-based Dijkstra's algorithm on this graph with source vertex $1$.
It will decrease the distance of the vertex $j$ every time it traverses the edge $(i,j)$. Moreover, it will take $\Theta(\log |V|)$ time for each call to heap.decreaseKey() operation as per the aggregate analysis. Therefore, the time complexity will be $\Theta(|V|+|E|) \log |V|$. Compare its performance with unsorted array based approach. You will see the difference.
Note that here, the shortest path tree is $1$ -> $2$ -> $3$ -> $\dotsc$ -> $n$.


------------------------------------------------------------

ID: cs.stackexchange.com:151759
Title: Lower bound union of a unsorted array with sorted array
Body: I read this link and I have similar question.
Suppose given two Arrays $A$ that is sorted array with length $n$ and $B$ unsorted array with length $n$. We want to find union of two arrays (i.e. we try to compute $A\cup B$) with comparison computation model. Can we claim that the lower bound of this problem is $\Omega(n\log n)$?


[Answer]: I think this is possible in $\mathcal{O}(n)$ average case, using hashtables.
You just need to create a hashtable of your elements, and before adding a new one to the result, you can check in $\mathcal{O}(1)$ average if it is already present, and add it to the hashtable if not.
I suppose here that a comparison between two elements is done in $\mathcal{O}(1)$ time.


[Answer]: But I think this is possible in $O(n(log(n)))$. Since, you can set C=A (suppose that we have no duplicate in A) and then for addition of each member from B to C it is sufficient to search it in sorted array A and if it was not in A then add the member to C.


------------------------------------------------------------

ID: cs.stackexchange.com:139914
Title: Sort an array with a constant number of unsorted elements
Body: I am trying to sort an array that I know to have a constant number of unsorted elements (elements that are not in their right place).
In the solution I looked at they say you can do this in $O(n)$ time:
you can find any unsorted element in $O(n)$ and then move it to the right place in $O(n)$ also. I am not seeing how you can do these things in $O(n)$.
I mean, I thought of an algorithm that iterates from 1 to $n$ ($n$ is length of the array), checks if the element at the index is in it's sorted place and if the answer is no then you can use partition for bringing it to the right place (partition works in $O(n)$).
Because I have a constant number of elements that are not in their right place I will use partition constant $\times n$ times which means the algorithm is $O(n)$ as asked. Problem is that checking for every index if the element in the index is in his right place is $O(n)$ and doing it for every index is $O(n)$ so it sums up to $O(n^2)$ so maybe the solution is not right. (Every other solution I wrote also has this problem)
I will be glad for any help.


[Answer]: Denote the array by $A_1,\ldots,A_n$. If $A_i > A_{i+1}$ then at least one of the two elements is out of place. We scan the array, and each time we find such a pair, we remove both elements. In $O(n)$ time, we obtain a sorted subarray $B$ together with $O(1)$ many unsorted elements, forming a set $C$. We sort $C$ in $O(1)$ and merge it with $B$ in $O(n)$.


[Answer]: An important observation is that an un-sorted array $A$ must contain at least one index $i$, such that $A[i+1] (assuming we are sorting in ascending order).
In particular, if you assume that the first $k$ elements are ordered (but the array is not fully ordered), then in the last $n-k$ elements you will have such a behavior, and in particular there will be a smallest index in which this behavior exists.
This allows us a way to find all "wrong" places in $O(n)$: start from scanning elements from the start, and scan until you find such an element. Then, fix the element's position in the array, and continue scanning again (its important to fix the element's position!)
Notice that the "fixing" process is assumed to take $O(1)$, since we are (currently) only interested in the scanning process.

Another way to sort elements (not using the quicksort and partitioning idea) is to use bubble sort. The total number of "bubble transfers" must be at most $c\cdot n$ where $c$ is the number of unsorted elements. Take a quick look at bubblesort to confirm that any other element will not take more than $O(1)$ time overall.


------------------------------------------------------------

ID: cs.stackexchange.com:145288
Title: Searching through an unsorted array with a better than O(n) complexity?
Body: This question kind of puzzled me, it was presented as a homework assignment for 2nd year undergrads and one of them approached me in case I could give him some pointers, but the problem is I couldn't even think of the answer myself.
They start with an unsorted array of integers and they're to find an element (there could be any number of them, guaranteed to be at least one) whose value is less than that of its neighbors, recursively.
Due to the nature of the problem the array can't be sorted, yet they asked them for a better than linear time solution. How could such a thing be?
He told me they explained binary search in class, but it doesn't apply, just like any other search  algorithm based on sorted arrays; still, apparently he tried to do the assignment traversing the array (after all, numbers meeting that criteria could be anywhere, it's not like you can discard part of it), but his teachers added a couple of unit tests to ensure that the search algorithm is both recursive and not O(n).
They gave them two examples:

[1, 2, 3, 4] -> Only the 1 would meet that criteria.
[1, 3, 4, 1, 1] -> All three 1s would meet that criteria.

And they also gave them hints:

It can be solved similarly to how binary search works, via divide and conquer, having the function receive the array, and two integers that mark the beginning and end of a subarrays.
If the subarray size is 1, that's that location is good to go.
If its size is 2, the biggest of them is the one.
If its size is >= 3, you're to check the middle one and its neighbors, if it's smaller than the neighbors you're done, otherwise you'd choose the interval based on the comparison.

The emphasis is mine, but, what the hell?
I mean, if an array is size 2, let's say, [1, 2], the one being smallest should be the chosen index, not the biggest; leaving that aside though, given the description of the problem, how is that comparison going to allow you to discard some part of the array? Which partitions would you take when the whole thing is unsorted?
My days of working with these kinds of algorithms are long gone, but I couldn't figure out what they wanted them to do. Is there something I'm missing? How could this be of logarithmic complexity?


[Answer]: You are right.
For example, if the array is:
      |  0  if i=52
a[i]= |  1  if 152

, it is impossible to use the binary search. You cannot recognize the half containing the single local minimum by a[] at its end and middle points. They all have the same meanings for both halves.
So, there is an error in the formulation of the task.
(BTW, it would be much better to put an exact formulation of the task in the question)


[Answer]: Your example $1,3,4,1,1$, in which you state that all $1$s are less than their neighbors, suggests that by "less than" you actually mean "at most". Any array contains an element whose value is at most that of its neighbors — just take any minimal value of the array.
As you mention, if the array has length $1$ or $2$ then the problem is easy (when the array is $x,y$, you choose the minimum of $x,y$).
Given an array $A$ of length $n \geq 3$, let us split it as follows: $B, x, y, z, C$, where $B,C$ are subarrays of lengths $\lfloor \frac{n-3}{2} \rfloor, \lceil \frac{n-3}{2} \rfloor$. We consider three cases:

If $y \leq x$ and $y \leq z$ then we choose $x$.
If $y > x$ then we recurse on $B,x$, an array of length $\lfloor \frac{n-1}{2} \rfloor$.
If $y > z$ then we recurse on $z,C$, an array of length $\lceil \frac{n-1}{2} \rceil$.

The procedure terminates in $\log_2 n$ steps, and so runs in time $O(\log n)$.

If you are actually looking for an element which is strictly less than its neighbors, you cannot do better than linear time. Indeed, consider any algorithm that makes at most $n-2$ queries. We run the algorithm, and whenever the algorithm accesses an array element, we always answer $1$. We claim that at the end, the array cannot output any index. Indeed, suppose that it inputs $i$. Since the algorithm makes at most $n-2$ queries, there is some index $j \neq i$ it hasn't queried. The queries made by the algorithm are consistent with an array in which $A[j] = 0$ and $A[k] = 1$ for all $k \neq j$, in which the only correct answer is $j \neq i$. Therefore at least $n-1$ queries are necessary.


------------------------------------------------------------

ID: cs.stackexchange.com:112819
Title: Time complexity of finding predecessor for a dictionary implemented as a sorted array
Body: I'm currently reading "The Algorithm Design Manual" by Steven Skiena. On page 73, he discusses the time complexity of implementing $ Predecessor(D, k) $ and $ Successor(D, k) $ and suggests that it takes O(1) time.

If the data structure looks something like

[(k0, x), (k1, x), ...]


where the keys k0 to kn are sorted, given k, I thought the successor(D, k) would first have to search the sorted array for k ( $ O(log n) $ ) and then get the successor ( $ O(1) $ ) since the array is sorted, and hence the overall time complexity should be $ O(log n) $ instead of $ O(1) $ as mentioned in the book. This should also apply for predecessor(D, k).

For an unsorted array, the time complexity for predecessor and successor remain as $ O(n) $ since searching the unsorted array also takes $ O(n) $.

Did I misunderstand something?


[Answer]: It's true that on page 73, the author defines the abstract functions $Predecessor(D, k)$ and $Successor(D, k)$, which find the predecessor and successor of a given key. 

But the chart on page 74 lists the interfaces as $Predecessor(L, x)$ and $Successor(L, x)$, which find the predecessor and successor of a given entry index. Clearly this is O(1) for a sorted random access datastructure.

This particular interface is, in general, more useful and more efficient than the key-based interface previously presented, since it more accurately represents the cost of iterating through the dictionaries elements. Most such interfaces do, in fact, take some kind of cursor ("iterator" in C++ parlance) as an argument and return another cursor as a result.


[Answer]: Usually dictionaries are not sorted. There are no provisions for accessing items in sorted order, and finding the predecessor and successor of a key take O (n) and are not really useful. 

What you do however is iterating through a dictionary, that is you have a loop that will visit each key, or each value, or each key-value pair in the dictionary exactly once. Such an iterator would keep track of a position of an item in the dictionary and would have to be able to find the previous or next position in a way that allows visiting each item exactly once, and that is preferably fast. 

If you implement a dictionary as a sorted array, going to the previous or next position is trivially O(1), and it just happens to return the items in sorted order. A dictionary implemented as an unsorted array, with n/2 instead of log n access on average to find an existing item, can also trivially iterate in O(1) (and for small n it will be faster because n/2 is still not large compared to log n, and the code is faster).

If you implement a dictionary using a sorted tree, or a hash table, then iterating may be more difficult; but a sorted tree could be stored in an array, and a hash table will often use an array that is not very much bigger than n, so you would expect at least amortized O(1) to iterate (or O(n) to iterate through all elements).


------------------------------------------------------------

ID: cs.stackexchange.com:63700
Title: Can partial sorting help with lookup cost in arrays?
Body: Looking something up in an unsorted list is a task with time complexity $O(n)$. However, if the list is sorted, the time complexity is $O(\log(n))$. That means it is sometimes worthwhile to sort an array. However, that is a trade-off as a sorting algorithm has a time complexity of $O(n\log(n))$.

As far as I know, you can not sort an array in less than $O(n\log(n))$  time. I am however wondering if there is any algorithm that can partially sort the array in less time than that? I am pretty sure you can not look up a value in such a partially sorted array in $O(\log(n))$ time, but can you do better than $O(n)$?

In short, is it possible to process an unsorted array with an algorithm faster than $O(n\log(n))$ such that a lookup algorithm can do a search faster than $O(n)$, though not as fast as $O(\log(n))$?


[Answer]: If you run "balanced Quicksort" (using the exact median at every step) up to depth $k$ (at the cost of $O(n k)$), you get a partition of the original array into $2^k$ sorted parts of $n/2^k$ unsorted elements each. Given an element, we can locate the correct part in time $O(\log(2^k)) = O(k)$ using binary search, and then look it up using an additional $O(n/2^k)$, for a total complexity of $O(n/2^k + k)$.

If $k = o(\log n)$ then the partial sorting takes time $o(n\log n)$. If $k = \omega(1)$ then the lookup algorithm takes time $o(n)$. Thus if $1 \ll k \ll \log n$ we get $o(n\log n)$ preprocessing time and $o(n)$ lookup time. For example, if $k = \log\log n$ then preprocessing takes $O(n\log\log n)$ and lookup takes $O(n/\log n)$.


------------------------------------------------------------

ID: cs.stackexchange.com:54283
Title: Sorting algorithm that moves element to a 2-dimensional array
Body: I had an idea for a sorting algorithm wich is very fast, but can (potentially) use a lot of memory. I'm not a Computer Science student/graduate, only a self-taught programmer so I don't know how to evaluate it's viability. Also, I would like to know if it has already been documented and under what name.

Algorithm:


Get an unsorted array
Iterate through it, and find the highest and lowest value stored in it
Determine "range" (highest - lowest)
Make a 2 dimensional array presorted,
in which the first dimension's size is "range" + 1
For each element in array "unsorted"


Add the current element into presorted[current_element_value - lowest]

Make array "sorted", and add each element of presorted's second dimension, ignoring the empty first dimensions.
return the sorted array


An example using said algorithm would be the following:

unsorted[] = {5, 8, 2, 4, 6, 8, 2, 0, 4, 5, 6, 3, 3, 2, 1}

After iterating through it once, we get:
lowest: 0
highest: 8

range = highest - lowest = 8

Make 2 dimensional array presorted,
 consisting of range+1 arrays:

presorted[0] = {}
presorted[1] = {}
presorted[2] = {}
presorted[3] = {}
presorted[4] = {}
presorted[5] = {}
presorted[6] = {}
presorted[7] = {}
presorted[8] = {}

Next we add the elements, first element of unsorted is 5.
 5 - lowest (0) is 5

presorted[0] = {}
presorted[1] = {}
presorted[2] = {}
presorted[3] = {}
presorted[4] = {}
presorted[5] = {5}
presorted[6] = {}
presorted[7] = {}
presorted[8] = {}

After doing this which each element:

presorted[0] = {0}
presorted[1] = {1}
presorted[2] = {2, 2, 2}
presorted[3] = {3, 3}
presorted[4] = {4, 4}
presorted[5] = {5, 5}
presorted[6] = {6, 6}
presorted[7] = {}
presorted[8] = {8, 8}

Now, we simply create an empty array called "sorted",
 to which we add all the elements
 of non-empty arrays in presorted:

sorted = {0, 1, 2, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 8, 8}


This is the C++ source code for it

std::vector Sort(std::vector &unsorted)
{
   int min = unsorted[0];
   int max = unsorted[0];

   int range;

   std::vector sorted;

   for (int i = 0; i  max)
         max = unsorted[i];
      if (unsorted[i] > presorted(range+1);

   for (int i = 0; i 

Is this algorithm viable?

I think it could be used in certain scenarios (when range is not too big) and it has the advantage that you can determine if it's appropriate to use it with simply iterating once through the unsorted list.

I'll do some tests and report back.


[Answer]: What you have come up with is a sligthly less memory efficient version of counting sort. 

This algorithm works in $\mathcal{O}(n + range)$ time where $n$ is the number of elements in the vector $unsorted$ and $max$ and $min$ are as defined in your program.

Your implementation uses $\mathcal{O}(n + range)$ memory. You can make it more memory efficient by just keeping track of how many times the same number in the vector 'unsorted' occurs rather than maintaining a vector with repeated instances of the same element i.e.

vector presorted(range+1);
for(int i = 0; i 

It will now use only $\mathcal{O}(range)$ memory.

Yes you are correct, this method might potentially use up a lot of memory if the difference between the smallest and largest elements of the vector is too great.

For those who are not fluent in C++:


OP is finding the smallest and largest elements of the unsorted array $max$ and $min$ respectively. 
He is initializing an array of size $max - min$ and creating an empty linked list in each position of the array (in this code it is the vector of vector of integers $presorted$). 
Then each element of $unsorted$ is added as a new node in the linked list at $presorted[unsorted[i] - min]$. In a random access computational model this operation will take $\mathcal{O}(1)$ time and hence each element of $unsorted$ is added to its appropriate bucket in constant time.
Then he is just iterating in the array of linked lists and compressing it to the final array $sorted$ which is the final output.



------------------------------------------------------------

ID: cs.stackexchange.com:120517
Title: Sorting almost sorted array
Body: Encountered this question but I couldn't solve with the complexity they solved it:

Suppose I have an array that the first and last $\sqrt[\leftroot{-2}\uproot{2}]{n} $ elements has $\frac{n}{5}$ inverted pairs, and the middle $n - 2\sqrt[\leftroot{-2}\uproot{2}]{n}$ elemnts are sorted. What is the complexity of sorting the unsorted array?

They claim in the answer that sorting array with $I$ inversions is $O(n\log{\frac{n}{I}})$.Why?


[Answer]: If you have an array with a sequential range that covers all but k elements, and you know the range, then you sort the k elements in O (k log k), then merge two ranges that you know to be sorted in O (n). That sorts the whole array in O(n) as long as k is O (n / log  n). 

In your case k = $O (n^{1/2})$ so clearly you can sort the array in O (n). 

And you don't need to know the sorted range, because for large n we have n/2 inside the sorted range, so you can just count the elements in ascending / descending order from n/2 upwards / downwards. 


------------------------------------------------------------

ID: cs.stackexchange.com:930
Title: Adding elements to a sorted array
Body: What would be the fastest way of doing this (from an algorithmic perspective, as well as a practical matter)?

I was thinking something along the following lines.

I could add to the end of an array and then use bubblesort as it has a best case (totally sorted array at start) that is close to this, and has linear running time (in the best case).

On the other hand, if I know that I start out with a sorted array, I can use a binary search to find out the insertion point for a given element.

My hunch is that the second way is nearly optimal, but curious to see what is out there.

How can this best be done?


[Answer]: We count the number of array element reads and writes. To do bubble sort, you need $1 + 4n$ accesses (the initial write to the end, then, in the worst case, two reads and two writes to do $n$ swaps). To do the binary search, we need $2\log n + 2n + 1$ ($2\log n$ for binary search, then, in the worst case, $2n$ to shift the array elements to the right, then 1 to write the array element to its proper position).

So both methods have the same complexity for array implementations, but the binary search method requires fewer array accesses in the long run... asymptotically, half as many. There are other factors at play, naturally.

Actually, you could use better implementations and only count actual array accesses (not accesses to the element to be inserted). You could do $2n + 1$ for bubble sort, and $\log n + 2n + 1$ for binary search... so if register/cache access is cheap and array access is expensive, searching from the end and shifting along the way (smarter bubble sort for insertion) could be better, though not asymptotically so. 

A better solution might involve using a different data structure. Arrays give you O(1) accesses (random access), but insertions and deletions might cost. A hash table could have O(1) insertions & deletions, accesses would cost. Other options include BSTs and heaps, etc. It could be worth considering your application's usage needs for insertion, deletion and access, and choose a more specialized structure.

Note also that if you want to add $m$ elements to a sorted array of $n$ elements, a good idea might be to efficiently sort the $m$ items, then merge the two arrays; also, sorted arrays can be built efficiently using e.g. heaps (heap sort).


[Answer]: Because you are using an array, it costs $O(n)$ to insert an item - when you add something to the middle of an array, for example, you have to shift all of the elements after it by one so that the array remains sorted.

The fastest way to find out where to put the item is like you mentioned, a binary search, which is $O(\lg n)$, so the total complexity is going to be $O(n + \lg n)$, which is on the order of $O(n)$.

That being said, if I felt particularly snarky, I could argue that you can "add to a sorted array" in $O(1)$, simply by slapping it to the end of the array, since the description doesn't indicate that the array has to remain sorted after inserting the new element. 

Anyway, I don't see any reason to pull bubble sort out for this problem.


[Answer]: If you have any reason for not using heap, consider using Insertion Sort instead of Bubble Sort. It's better when you have a few unsorted elements.


[Answer]: Patrick87 explained this all very well. But one additional optimization you could make would be to use something like a circular buffer: you can move items right of the position of the inserted element to the right, as usual. But you can also move items to the left of the correct position to the left. To do this, you need to treat the array as circular, i.e. the last item is right before the first one and it also requires you to keep the index where the items currently start.

If you do this, it could mean you make about half as many array accesses (assuming uniform distribution of indexes you insert to). In the case of doing binary search to find the position, it's trivial to choose whether to shift to the left or to the right. In the case of bubble sort, you need to “guess” correctly before starting. But doing that is simple: just compare the inserted item with the median of the array, which can be done in single array access.


[Answer]: I have used the Insertion sort algorithm effectively for this issue. At one time we had a performance issue with a hash table object, I wrote a new object that used binary search instead that increased performance significantly. To keep the list sorted it would keep track of the number of items added since the last sort (i.e. number of unsorted items,) when the list needed to be sorted due to a search request, it performed an insertion sort or a quick sort depending on the percentage of items unsorted. Use of the insertion sort was key in improving the performance.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:355170
Title: What is wrong with this algorithmic solution of mine that checks whether a given function returns a sorted array when an array is given as input?
Body: An interviewer asked me this question:


  Given a function f(ar[]) where ar[] is an array of integers, this
  functions claims to sort ar[] and returns it's sorted version ars[].
  Determine if this function works correctly.


I approached this question as:


  
  First check if the returned array ars[] is actually sorted in either non increasing or non decreasing order. This one is easy to check, ars[] should
  either follow the sequence ar[i + 1] >= ar[i] (for an array sorted in
  non decreasing order) or ar[i + 1] 
  Then check if sizes of both the input array ar[] as well as the output array ars[] are same.
  Finally check if every element of ar[] is also present in ars[]. Since we have already examined at step 1 that ars[] is sorted and at
  step 2 that sizes of ar[] and ars[] are same we can use Binary
  Search algorithm to perform this action. The worst case time
  complexity for this should be O(n * log(n)).
  
  
  If all the above 3 checks succeeds then the function is working fine
  else it is not.  The overall time complexity of this algorithm should O(n * log(n))


But to my surprise the interviewer said that this solution is not correct and it's time complexity can be improved. I can not understand what actually is wrong with my solution, did I miss any corner case or the entire approach is wrong? Also what can be better approach to this(in terms of time complexity)?

PS: The interviewer mentioned no additional information or any additional constraint for this problem.   


[Answer]: While you say the interviewer provided no other information, was it possible to ask questions? As there is only one argument to the function, I would be under the assumption that the function would always return the array sorted it one directory, most likely ascending. i.e.

fn([4,1,5]) -> [1,4,5]


But obviously that's something the interviewer should confirm for you as it affects the answer. (Also it would be very strange for the function to return asc OR desc, without any way to specify).

As such, my layman's approach would be to pass an unsorted array to the function, and then compare that against a second array I already knew was sorted -

expected = [1,4,5]
sorted = fn([4,1,5])
// compare expected and sorted...


As far as basic testing goes, the process of comparing every element against each other, and also comparing every single element to the source array does seem convoluted - compared to just comparing against a known correct list.


[Answer]: Your method is only testing that the original and sorted arrays contain the same values, not that they contain the same number of each value; e.g. 1112 would pass for 1221

In step 3 you could, for example, mark that a particular value in the sorted array has already been matched (removing from the array would be too time consuming) but then the search would no longer be a binary search as you would hit already used values. 

[Obviously this isn't a problem if the values are unique but that isn't stated]


[Answer]: For speed improvements you can test with known arrays with either presorted answers to check against or simply run through the returned array ra checking ra[n] = for all n in the range 0..len(ra-1) complexity of which is very low. You can determine if every element is present in each array by counting the instances of each value and then comparing the counts.

Your testing should also include corner cases such as ar=[1] and ar=[] and for an interview I would at least mention testing for invalid inputs such as arrays of non-integer values, non-arrays, etc., I know that the behaviour in such cases is undefined in the case you were given but a part of a testers job is to highlight specification omissions and ambiguities such as what is the error handling. If you only test for what is specifically in the specification you will not make a good tester and this sort of issue is one of the things that the interviewer will be looking for.


[Answer]: As already mentioned by @Dipstick, step 3 can fail if there are duplicates in the input array. To resolve this and improve the time complexity, one can use a dictionary with the array elements as keys and their number of occurrence as values. Such a dictionary can be created from the sorted and the unsorted array as well in O(n), and you need to test if the resulting dictionaries are identical, which can be done in O(n), too. 

One can combine this using only one dictionary counting the total number of occurrences in the unsorted array minus the number of occurrences in the sorted array. In pseudo code (assuming a default of 0 for values in the dictionary when the key is used the first time): 

 for(e in ar)
     noOfOccurence[e]+=1; 
 for(e in ars)
     noOfOccurence[e]-=1;

 for(e in noOfOccurence.Keys)
     if(noOfOccurence[e] != 0)
         return false;

 return true;



[Answer]:   > But to my surprise the interviewer said that this solution is not correct


If i were a job interviewer i would expect to get the answer: write a simple unittest for the sort method
with a sample unsorted input and the expected output (see @USD Matt answer)

  > time complexity can be improved.


Your solution looks OK on the first glance and more complete/complicated than the simple example.

If i were searching for an experienced developer i would expect that the applicant knows unittests and does not more than neccessary.

I assume that the complex/complete automated testing is overkill and not necessary for testdriven development. A simple example should be enough.


[Answer]: If you keep track of the min and max a array of size max - min + 1 will work if the range is not large. 

Subtract min from each value so it starts at 0 

for(e in input)
     ar[e - min] +=1; 
for(e in sorter)
     ar[e - min] -=1;

for(e in ar)
     if(e != 0)
         return false;

return true;


You could check for duplicates as you test for sort
Yes binary would be O(n logn) but is might be faster than dictionary  


[Answer]: Take a step back and look at this from a simplistic standpoint. You are making your first step be the most time and compute intensive test. I'd start with a series of sanity checks which catch the most glaring issues and avoid having to (re)sort the array and compare element-by-element. 

Put your test to ensure the number of items in the result equals the number of items fed in because if the number of items don't match you can fail the test and not go any further. 

Next test that the first element is less than the last element. 

If that passes, spot check some elements in between, for example the first and last, and the items at the 1/4th mark, 1/2 mark, and 3/4th mark - and make sure first 

Assuming those tests pass, then I'd dig into iterating all the elements to test the integrity of the sort. But I wouldn't resort the input to compare, but simply iterate the results in order, comparing this item with the previous one and making sure this item is bigger than the previous one. Bail out with a failed test as soon as you hit one that is smaller than its predecessor. 

The runtime of the test will be the same as the time it takes to sort the list but only if the sort is good. If the sort is not good, the runtime will be some degree shorter than it would've taken to re-sort and test. How much smaller depends on if the failed value is on the front end of the data or the back end of the data group.

Typically, there is something about the data being sorted, and especially where that data is coming from which can point you to the cases where sorting errors are most likely to crop up. If you have the luxury of having insight into these things, then craft sanity checks to catch those cases and put that before the visitation of every item to verify sorting. 


------------------------------------------------------------

ID: cs.stackexchange.com:113343
Title: Why is heap insert O(logN) instead O(n) when you use an array?
Body: I am studying about the arrays vs heap for make a priority queue

For check the heap implementation I am reviewing this code:  Heap

, but I have the following question.

Heap is based on array, and for creating an array you need O(n), for inserting into a heap you need O(logN), so if you have a unordered list of task and you want to create a heap, you need O(NLogN).

If you use an array, you need O(n), to create the array, and O(NlogN) to sort the array, so you need O(NLogN).

So if you need implement some similar to this:

function priorityQueue(listOfTask l)


There isn't a diference betwen use an Array or an Heap right? So, why I should use a heap instead an array for solve this function?

Thanks


[Answer]: Keeping a heap is more efficient than keeping a sorted array, when you need to keep adding items to the priority queue. In case you don't need to add to it, you don't need a queue in the first place, just an array sorted by priority.

Insertion to heap-based priority queue is O(logN), while insertion to sorted array is O(N) (binary search for position is O(logN), but inserting there is O(N) ).

As you can see here, almost all data structures are about trade-offs. Converting an array to heap (and keeping it heap) you gain something (O(logN) priority queue action) but also lose something (ability to iterate contents in order, binary search). In contrast, if you used an unsorted array, you'd gain O(1) insertion (because you can just append), but almost everything else would be O(N), which is excellent if you have handful of items, but becomes bad if you have dozens, impossible if you have thousands.


[Answer]: Given your link, you seem to be interested in data structures supporting the following operations:


Create(m): create a new instance with room for m elements.
Size(): return the number of elements currently stored in the instance.
Insert(k): insert an element with priority k.
ExtractMax(): return the maximal priority currently stored, and remove it.


Since Size() is easy to implement in constant time as part of the other operations, I will not discuss it below.

Here are three sample implementations:

Unsorted array:


Create() simply allocates memory.
Insert() simply inserts the element at position i+1, where i is the current number of elements.
ExtractMax() goes over the entire array, finding the maximum; and then "compacts" the array by moving all elements to the right of the maximum one entry to the left.


Sorted array: (your implementation)


Create() simply allocates memory.
Insert() first scans the array to find the proper location for the element, then moves all elements to the right of the intended location one entry to the right, and finally stores the element.
ExtractMax() returns and removes the ith element, where i is the number of elements currently in the instance.


Binary heap:


Create() simply allocates memory.
Insert() and ExtractMax() are described on Wikipedia.


Binary heaps are implemented using "partially sorted" arrays. That is, the order of elements in the array is not arbitrary, but it is also not completely determined. Rather, we are only guaranteed that A[i] ≥ A[2i],A[2i+1]. This freedom allows us to trade-off the running time of the two operations Insert() and ExtractMax().

In all of these implementations, Create() is roughly the same, so to compare the various implementations it suffices to consider the two operations Insert() and ExtractMax():

$$
\begin{array}{c|c|c}
\text{Implementation} & \text{Insert()} & \text{ExtractMax()} \\\hline
\text{Unsorted array} & O(1) & O(n) \\
\text{Sorted array} & O(n) & O(1) \\
\text{Binary heap} & O(\log n) & O(\log n)
\end{array}
$$

Here $n$ is the number of elements currently in the array.

If you perform many Insert() and ExtractMax() operations, a binary heap is likely to be more efficient.

Another operation which you mentioned is


Initialize(A): add to an empty instance the elements in A


You can add support to this operation to all different implementations mentioned above:


Unsorted array: simply copy A to the array. Running time: $O(|A|)$.
Sorted array: copy A and sort the resulting array. Running time: $O(|A|\log|A|)$.
Binary heap: run Insert() for each element of A. Running time: $O(|A|\log|A|)$.


Considering this operation doesn't strengthen the case of your implementation.


[Answer]: A priority queue does something entirely different than sorting an array. 

The important operations for a priority queue are: 1. Add an item to the queue. 2. Tell us the smallest item in the queue and remove it from the queue. Both these operations run in O(log n).

Now use a sorted array. Operation 2 is fast if we sorted in descending order. But operation 1 isn’t: Adding a random value to a sorted array and keeping the array sorted requires moving n/2 array elements on average. 

You could sort an array in O(n log n) by adding all items to a priority queue, then removing the smallest item n times. Fast sorting algorithms process the array as a whole, they don’t try to keep part of the array sorted (straight insertion or binary insertion do; they ar not fast). Quicksort moves items very roughly into the right place at each step. Note that a priority queue only performs a rough ordering of the items as well. That’s what makes both Quicksort and a priority queue fast: They don’t ask for the data to be sorted at all times. 


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:234871
Title: Correct way to undo the past few git commits including a merge?
Body: My team is working with git for the first time and has really messed up our git remote due to our inexperience. I'm trying to help educate them on git to solve the problem in the long term, but in the meantime, the commits that have been made are totally wrong. We haven't made very much progress at all, and I don't see the value in retaining these commits when they could cause confusion/additional problems down the line. 

The most recent commit was a merge between the master branch and a second branch that one of the team members unintentionally created. If I try to revert, git states that I must be using the -m flag, but I'm not sure whether I should be reverting or trying to rebase in this case, or how I would do it given the structure of the past couple of commits.

What is the best way to resolve this issue and how should I go about doing it?


[Answer]: I believe you're looking for the git reset command. If you run git log it will show you a commit history for your repo. Then run git reset --hard HEAD-number of commits you'd like to go back.

Documentation here: http://git-scm.com/docs/git-reset


[Answer]: To undo the merge, you can do

git checkout master
git revert -m 1 [SHA of the merge commit]
git push


This will create and push a new commit which reverts all changes that were merged in.

It's also possible to alter master's history so that it appears as if the merge never happened, but I wouldn't recommend it. Your teammates would have trouble when they try to pull master and the history is different. Also it's easy to revert a revert commit if you reverted the wrong changes, but if you alter mater's history, it's harder to undo that.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:168331
Title: git changing head not reflected on co-dev's branch
Body: Basically, we undid history.  I know this is bad, and I am already committed to avoiding this at all costs in the future, but what is done is done.  

Anyway, I issued a git push origin :master to undo some bad commits.  I then deleted a buggered branch called release(which had also received some bad commits) from remote and then branched a new release off master.  I pushed this to remote.  So basically, remote master & release are clones and just how I want them.

The issue is if I clone the repo anew(or work in my current repo) everything looks great....but when my co-devs delete their release branch and create a new one based off the new remote release I created, they still see all the old junk I tried to remove.

I feel this has to do with some local .git files mistaking the new branch release for the old release.

Any thoughts?  Thanks.


[Answer]: So the branch is still synced to the local site. It needs to be reset to the commit in the remote. I think, ideally, what you would want to do...

$ git checkout release
$ git reset --hard origin/release
$ git checkout master
$ git reset --hard origin/master


Or... if they have everything locally pushed, they can just delete the local repo and clone again. 

In the future, instead of rewriting history, it's normally easer to just revert those commits. You'll still see the history but the code won't be in HEAD of that branch. 


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:408571
Title: Rewind commits to new branch in git
Body: I have three branch on my git tree. master contains validated source version, develop contains
staging versions, then I have some feature branches.

For now, I have the current strucutre;
The numbers represents some individual commit hashes for simplicity.

master              develop     feature
  o----------o----------o----------o
  1          2          3          4


Commit 2 and 3 come from a feature I merged on the develop branch.
I then started working on a new feature branch.

I need to rewrite what was done in commit 2 and 3.
For this, I need to branch my current work on new-feature from master and
re-build the merged branch I had containing commits 2 and 3.

Here is what my git repo tree should look like.

[What I want]
master/develop
      o 1
      |                rewrite-feature
      +----------o-----------o
      |          2           3
      |
      |       feature
      \----------o
                 4


master and develop are pushed. feature is a local branch.

I am the only one working on this project, so I can mess up with the origin
if needed without impacting anyone.

I'm really not sure of what I should do, specialy regarding to the pushed master
and develop branches.

I think I should rebase the feature branch on commit 1.
But I don't understand how to deal with those commits 2 and 3 without messing
with the public repository.


[Answer]: Is it really important to rewrite the history? I mean, I myself like clean history, but in the end what really matters is the state of the repo not its history.

If it's not that important, just do the fix like any others: create a new feature, fix, commit, push, merge in develop, merge in master. Then rebase the other features on develop.

I don't know if you'll have the graph you want, but if it's really important to fix the history or you just want to:

Rename develop and the feature branch as something else as backup.


create a new branch "Master2", rebase interactively and amend the commits.
Force push on Master2 on Master
Create develop from master.
Create featureA. Rebase featureA on oldFeature.
Check that everything is OK
Delete old branhces



[Answer]: Do not rewrite public history.

The easiest way would be to revert develop to master as a new commit. A simple git checkout master -- . will stage changes reverting commits 2 and 3. Commit those changes to develop and note the commit Id of master that you are reverting too in the commit message. This gives you a history similar to:

master                          feature
  o----------o----------o----------o
  1          2          3\         4
                          \
                           \    develop
                            \------o
                                   5
                                   ^
                                   |
        Reverts commits 2 and 3 ---+


If the feature branch is a private tree (it has not been pushed) then just rebase the feature branch on to develop. 

If the feature branch has been pushed, then just do a merge from develop into feature, and don't worry about the commit history. Clean history is nice, but not a requirement. Once branches have been pushed, merge commits are bound to happen. Don't sweat this too much.


[Answer]: There are two aspects:

Do not rewrite public branches: any branch that belongs to the team (rather than yourself personally) must not be rewritten, except in the direst of emergencies.
In your situation that would be master and probably also develop.

Decide whether feature branches are history or proposals: you can work either way, but it's better to be consistent. In the "clean history" approach, feature branches are proposals and can be rebased and modified up until the point where they're made part of a public branch (main, develop, master, etc). In the "keep history" approach, feature branches are part of history and are merged into the public branch.


So, in your situation, there are two possible approaches:

Declare that merging feature1 to develop was an error and undo it; this is especially suitable if it's in the immediate past and nobody else has seen develop yet. It's not really suitable if you've done substantial work on the feature branch, or if the code has gone anywhere else.
master/develop                  feature2
  o----------o----------o----------o
  1          2          3\         4
                          \
                           \    feature1-fixes
                            \------o
                                   5

There are very limited circumstances in which this will apply.

If others have seen develop at commit 3, or if it's just generally part of history now, make a new branch off develop and make bugfixes on that.
master              develop     feature2
  o----------o----------o----------o
  1          2          3\         4
                          \
                           \    feature1-fixes
                            \------o
                                   5

At that point you're just working on two features in parallel — the new feature2 and the fixes to the already-existing feature1; you can merge/rebase them to develop in either order when they're ready.




[Answer]: I realise this is an old question, but I feel the existing answers all miss parts of the described scenario. From what I understand, the desired state is:

Code on the develop branch which does not include the changes in commits 2 and 3.
A branch containing the changes from commits 2 and 3, based on the desired develop.
A branch containing the changes from commit 4, based on the desired develop.

There are two ways to "undo" changes in git, both of which have their problems:

You can reset the branch, forcibly pointing it to an earlier commit. This closely matches the intuitive requirement: master and develop point at the same commit. But this is a case of "rewriting history", which is dangerous if anyone else has seen the original history and based other changes on it.
You can revert the commits, undoing the changes as a new commit. This maintains the original history of the branch, and gets the code into the desired state. But this can also cause problems: when you want to merge the changes (commits 2 and 3) back in again, git will see them as already applied; you have to make new commits with the same change (e.g. with git rebase) before you can apply them. It can also lead to confusing merge conflicts, with the same changes being applied and reverted at different times.

If you decide that resetting develop is safe in your particular circumstances, then re-creating the feature branch is trivial: just checkout commit 3 and run git checkout -b some-branch-name. If you decide to use git revert, you will need to re-create the commits with git rebase develop; the resulting commits should look identical, but will have new commit hashes, so won't show as "already applied" when you are ready to merge.
If you reset develop, you must also rebase the other feature branch to recreate commit 4 without commits 2 and 3 in its history. If you revert, you could instead merge develop into that branch, so that the new revert commit is also in its history. Rebasing leads to the least confusing commit graph, though.

Graphically, the two options are...
Reset develop to commit 1; point one branch at the existing commit 3, and rebase commit 4 giving a new commit 4a:
master, develop
      o 1
      |                rewrite-feature1
      +----------o-----------o
      |          2           3
      |
      |       feature2
      \----------o
                 4a

Revert commits 2 and 3 on develop, as a new commit 5; rebase both branches onto this, giving new commits 2b, 3b, and 4b:
master                           develop
  o----------o----------o----------o
  1          2          3          5
                                   |                 rewrite-feature1
                                   +----------o-----------o
                                   |          2b          3b
                                   |
                                   |       feature2
                                   \----------o
                                              4b

With a potential third option where you don't rebase the second feature branch, instead merging in the revert commit from develop (shown as commit 6):
master                           develop
  o----------o----------o----------o
  1          2          3          5
                        |          |                rewrite-feature1
                        |          +----------o-----------o
                        |           \         2b          3b
                        |            \
                        |             \    
                        \------o-------o feature2
                               4       6



------------------------------------------------------------

ID: softwareengineering.stackexchange.com:356887
Title: How can I minimize git pain when everybody is working on master?
Body: Our documentation team of about ten people recently moved from SVN to Git.  In SVN, everybody worked on master -- a model I've always hated, but I wasn't able to bring about that change.  As part of the move to Git we've agreed to fix that, but we can't do it just yet (waiting on build changes that will allow builds from arbitrary branches).  Meanwhile, everybody is working on master.  Yes I know this is terrible, believe me.

We're seeing a lot more hiccups now than when we were using SVN, some of which are caused by Git's two-stage model (local and remote).  Sometimes people commit but fail to push, or they pull and get conflicts with their pending local changes.  Yesterday somebody clobbered recent changes -- somehow -- with a merge gone wrong, which I think was the merge that Git does when you pull and have outstanding changes.  (He has not been able to tell me exactly what he did, and because he's using a GUI I can't just inspect his shell history.)

As the most-proficient Git user (read: I've used it before, though not for anything super-complicated), I'm the person setting policy, teaching the tools, and cleaning up messes.  What changes can I make to how we are using the tools to make a shared, active master less error-prone until we can switch to doing development on branches?  

The team is using Tortoise Git on Windows.  We're using Tortoise Git because we used Tortoise SVN before.  (I personally use the command line under Cygwin for some operations, but the team has made it clear they need a GUI and we're going with this one.)  Answers should work with this tool, not propose replacements.

Tortoise Git has "Commit & Push" available as a single operation and I've told them to always do that.  However, it's not atomic -- it can happen that the commit (which after all is local) works just fine but the push doesn't (say, due to a conflict, or a network issue).  When that happens they get an ambiguous error; I've told them to check the BitBucket commit log if they have any doubts about a recent commit and, if they don't see it, to push.  (And to resolve the conflict if that's the problem, or ask for help if they don't know what to do.)

The team already has the good habit of "pull early and often".  However, it appears that pull can cause conflicts, which I think is new?  If not new, much more frequent than in SVN.  I've heard that I can change how Git does pulls (rebase instead of merge), but I don't have a good understanding of the trade-offs there (or how to do it in our environment).

The server is BitBucket (not Github).  I have full administrative control over our repository but none on the server more generally.  None of that is changeable.

The source files are XML.  There are also graphics files, which everybody knows you can't merge, but we also almost never have collisions there.  The merge conflicts come from the XML files, not the graphics.

What changes can I make to our use of Git to make sharing master go more smoothly for the team until we can move to using feature branches with reviewed, test-validated pull requests?


[Answer]: Sometimes, what you're doing has to change.

The biggest issue is that everyone is working on master. This is not typical for code development, and could be the wrong model in your case as well. If you can change that, by asking/requiring that changes be done on separate branches, you'll be in much better shape. With branches, you can gain the following:


Enforce that no pushes directly to master are allowed.
Enforce through Bitbucket that pull requests are created and have at least one approval prior to merging. This ensures someone is looking at the changes, and also makes the merge itself less painful, as the UI will show conflicts against the remote version of the code, not whatever the user has on the desktop. This prevents the commit-succeeded-but-push-failed scenario.
Execute "builds" against your repo prior to merging. I realize it's a doc repo, but maybe there's spell-checking, legalese scraping or even automated translation (export STRING_DEF things to a csv file) that could be built off this build. Or maybe not, depends on your work.
Allow folks to work on multiple different things concurrently more easily. Yes this can be done with stashes as well, but it's a bit messier and something tells me you're not using those either.


If you can't use branching, you might consider writing a merge-and-push script that could automate some of the pain points away. Maybe it would check that the user is not behind on master, do a fetch and pull, and then attempt the merge (possibly with --no-commit --no-ff), and so on.


[Answer]: There are three main things to remember when you are working out of the same branch as someone else:


Never use --force unless you really know what you are doing.
Either commit or stash your work in progress before every pull.
It usually goes easier if you pull right before a push.


Aside from that, I will point out that with distributed version control it doesn't matter if your "official" repo uses branches or not.  That has no bearing whatsoever on what individual users do in their local repos.  I used to use git to get local branches when my company used a completely different central VCS. If they create local branches for their features and make merging mistakes to their local master, it's a lot easier to fix without going into the reflog or some other magic.  


[Answer]: One possible mechanism, that a lot of open source teams have adopted, is to use the forking model - https://www.atlassian.com/git/tutorials/comparing-workflows (be sure to enunciate clearly when discussing a forking git workflow).

In this each developer or sub-team has their own fork of the repository that they check out from BitBucket does provide a mechanism for this, setting an "upstream" origin in addition to the default remote - they will have to remember to "fetch upstream" and "merge remote/upstream/master" on a regular basis.

It will possibly resolve your build mechanism problems as the build tools would possibly be pointed to the master on a different project, i.e. the fork.

You could then remove from most people the ability to push directly to the master project and make that a smaller team of people with review & approve roles. See https://www.atlassian.com/git/tutorials/making-a-pull-request

The place to read up on ensuring that just about any desirable checks are done before pushes is in the git book section on hooks - https://git-scm.com/book/gr/v2/Customizing-Git-Git-Hooks - you can use pre-commit and pre-push hooks to do things like running some tests on the proposed commit to ensure that the work is valid, etc. - the only problem with client side hooks is that developers can disable them or fail to enable them.

Both upstream fetch/merge & hooks are available in TortoiseGit.


[Answer]: If everyone is working on master, there's nothing that you can do. Things will inevitably get messed up. 

You should use master for completed products that get sent to a customer. You should use development for ongoing development, and you should not allow anyone to push to development. The standard is that everyone branches from dev, makes their changes, pushes them from local to their branch on the server, and issues a push request. Then someone reviews the change and merges it into development. 

To avoid conflicts, everyone merges development into their own branch before pushing, and solves conflicts at that stage (so it only affects one developer locally). If merging into development would cause conflicts, then it isn't merged - the developer merges development into their branch again, and pushes to the server again, and then it is reviewed again. 

You can use sourcetree for example to make this work without any pain. 


[Answer]: Is it possible to take a day for everyone to learn git? 

Computer using professionals should really be expected to learn a new tool and although possible to make many mistakes in any VCS they should be using the tool as it is designed to be used.

The best way to introduce this is to get every one to work on their own branch when they make a change (as short as possible) and rebase then merge back into master when they are done. This isn't too far off the current way of working and introduces a simple workflow that they can get used to until they feel confident enough to do more complicated operations. 

I don't use windows but if Tortoise is basically hiding git from them and pretending that it is SVN then maybe Tortoise is the wrong tool.


[Answer]: So far SourceTree was the best IDE to learn the concepts, because it shows all the relevant dialogs and options you have on each stage, the default options are usually fine, don't mess around with rebase, etc. Just follow the normal flow:


Pull from master, just to be sure you are up to date
Modify your files
Commit your changes (that is only locally)
Pull again from master (this will cause conflicts to appear)
Edit all files until the conflicts are resolved, meaning the file is in the propper state you want to commit (no >>> master messages in the raw file)
Commit the merge changes
Push


If everyone follows this recipe, they should be fine.

Each time someone does a bigger or central change, inform the other users to commit locally and pull from master, so they don't get too many conflicts later on and the first person is still around to resolve the conflicts together with them.

Invest a lot of time in getting everyone to understand the flow, otherwise they might get around a while and then feel comfortable with it while actually screwing the master branch, for example "use my file instead of remote" to resolve a conflict will just kick out all changes made by other people.

Git is a hard to learn system, especially if you grew up with Svn, be patient and give them time to learn it properly, with new users you can sometimes spend a day cleaning up some mess, that is normal. ;)


[Answer]: So we have a team that switched from TFS to git and retained the old ways of thinking. The general rules of operation are more or less the same.

Yes, this means everybody works on master. This isn't that bad; and a team used to TFS or SVN will find this most natural.

General procedures to make this as painless as possible:


do git stash && git pull --rebase && git stash pop every morning
commit early and often (don't need to push immediately; we can at least start taking this advantage of git early)
for pushing do the following loop:

git add
 git commit
 git pull --rebase
 fix any merges
 compile
 git push
 loop until you don't get the can't fast forward error message.



[Answer]: Emphasize that you can redo merges
It may be obvious to you but former SVN users might not be aware they can try to solve a merge multiple times. This might cut down the number of help flags your receive.
In SVN when working off of trunk you'd have changes uncommitted sitting around. Then you'd do an svn update. At which point your changes would mix with other peoples changes forever. There was no way to undo it (afaik), so you had no choice but to just manually check everything and hope the repo was in a good state. When really you'd be much more comfortable just redoing the merge.
People would have the same mentality even when we moved to git. Leading to a lot of unintentional errors.
Luckily with git there is a way back, specifically because you can make local commits. (I describe later on how this is expressed in the commandline)
Though how this is done will vary based on tooling. I find redoing a pull isn't something exposed in many GUIs as a single button but is probably possible. I like you use cygwin. My co-workers use sourcetree. Since you use BitBucket it would make sense to use that as your GUI since it is managed by the same company : Atlassian. I suspect there's some tighter integration.
Regarding pulls
I think you are right that the merge in pull is whats messing people up. A pull is actually git fetch which retrieves the changes from the server, followed by git merge origin/* which merges the remote changes into your local branch. (https://git-scm.com/docs/git-pull)
The upshot is all standard merge commands work with pull. If that merge has conflicts you can abort by git merge --abort. Which should take you back to before your merge. Then you can try again with either git pull or git merge origin/.
If you can somehow learn how to do the above using your co-workers' GUI tool of choice I think that'll solve most of your problems. Sorry I can't be more specific.
* I understand that origin is not always the case here.
Use git reflog to diagnose problems
I, like you, have to diagnose problems mostly created by misuse of GUI tools. I find that git reflog can sometimes be helpful as that is a fairly consistent trail of actions on the repository. Though it is hard to read at times.
An alternative
Since your situation is temporary, you could just go back to SVN until you have the process in place to roll out. I'd be hesitant to do this as many places would go on saying 'We tried git once but it just didnt work...' and never really pick it back up.
Some other common transitional problems

People would often delete and reclone their repo, being convinced their repo was in an unusable state. Usually this was caused by losing track of the local and remote difference. Both GUI tools and the CLI fail at showing this well. In the CLI I find git log --decorate the easiest way to overview the differences. But if things get too hairy on master (for example) you can git reset --hard origin/master



[Answer]: Well, recently I adapted the following workflow to never f*ck up the master branch:

1) Everyone uses their own branch, which is intially a copy from the master branch.

Let's name the master branch "master", and my own branch "my_master".

I just made my branch from the master, so it's exactly the same. I start working on a new feature on my own branch, and when it's done I do the following.

Currenly on my branch, just finished coding

git add . && git commit -m "Message" && git push


Go back to the master branch

git checkout master


Pull if it is not up to date

git pull


Go back to my own branch

git checkout my_master


Merge the latest master to my own branch

git merge master


Fix conflicts & merges

Test everything again

When everything is merged & fixed on my own branch, push it

git push


Go back to the master branch

git checkout master


Merge with my branch

git merge my_master


Impossible to have conflicts as they are resolved on your own branch with previous merge

Push master

git push


If everybody follows this, the master branch will be clean.


[Answer]: I went through the exact same SVN -> git experience at my company and from my experience, the only remedy is time. Let people get used to the tools, let them make mistakes, show them how to fix them. Your velocity will suffer for a while, and people will lose work, and everyone will be a bit tetchy, but that is the nature of changing something as fundamental as your VCS.

That said, I agree with everyone who is of the opinion that TortoiseGit is a hindrance, rather than a help, so early in the transition period. TortoiseGit is... not a great GUI at the best of times, and by obscuring how git actually works in the name of simplicity, it's also preventing your coworkers from gaining an understanding of core git concepts such as the two-phase commit.

We made the (rather drastic) decision to force devs to use the command-line (git bash or posh-git) for a week, and that worked wonders for comprehension of how git actually operates and how it differs from SVN. It may sound drastic, but I'd suggest you try it simply because it creates that understanding of the git model - and once they have that down, your coworkers can start using whatever GUI facades over git they like.

Final note: there will be some of your coworkers who grok how git works almost immediately, and there will be some who never will. The latter group, you just have to teach the mystical incantations to make their code get from their local machine to the server so that everyone can see it.


[Answer]: This is going to sound counterintuitive, but hear me out:

Encourage them to start experimenting with git

One of the interesting things about git is that it's surprisingly easy to make any local operation completely safe. When I first started using git, one of the things I found myself doing was zipping up the entire directory as a back up in case I screwed something up. I later discovered that this is an enormous kludge and is almost never actually necessary to protect your work, but it has the virtue of being very safe and very simple, even if you don't know what in the heck you're doing and how the command you want to try will turn out. The only thing you have to avoid when you're doing this is push. If you don't push anything, this is a 100% safe way to try out anything you want.

Fear of trying stuff is one of the biggest hindrances to learning git. It gives you so much control over everything that it's kind of daunting. The reality is that you can stick to a few very safe operations for most of your daily use, but finding which commands those are takes some exploring.

By giving them a sense of safety, they'll be far more willing to try to figure out how to do things on their own. And they'll be far more empowered to find a personal work flow on their local machine that works for them. And if not everyone does the same thing locally, that's fine, as long as they adhere to standards with what they push. If it takes zipping up the entire repo before doing an operation to make them feel that way, it's fine; they can pick up on better ways of doing things as they go and as they try stuff. Anything to get yourself to start trying stuff and seeing what it does.

This doesn't mean training is worthless. On the contrary, training can help introduce you to features and patterns and norms. But it isn't a replacement for sitting down and actually doing stuff in your daily work. Neither git nor SVN are things that you can just go to a class and then you know everything about. You have to use them to solve your problems to get familiar with them and which features are well suited for which problems.

Stop discouraging them from learning the ins and outs of git

I mentioned not pushing anything, which actually goes against one of the things you've been teaching them: to always "Commit & Push". I believe you should stop telling them to do this and tell them to start doing the opposite. Git has basically 5 "places" where your changes can be:


On disk, uncommitted
Staged but not committed
In a local commit
In a local stash
Remote repositories (Only commits and tags are ever pushed and pulled between different repositories)


Instead of encouraging them to pull and push everything in a single step, encourage them to leverage these 5 different places. Encourage them to:


Fetch changes before they commit anything.
Make a decision how to handle the fetched changes. Options are:


Commit their local changes, then rebase them on top of the fetched changes.
Commit their local changes and then do a merge with the fetched changes.
Stash their changes, merge, and then unstash and resolve any conflicts.

There's other stuff, but I won't get into it here. Note that a pull is literally just a fetch and a merge. It's not like them; it is them. (Passing --rebase changes pull from fetch+merge to fetch+rebase.)

Stage their changes and then review them.
Commit their staged changes and then review the commit.
Push separately.


This will encourage them to check their work before it's made publicly available to everyone, which means they'll catch their mistakes sooner. They'll see the commit and think, "Wait, that's not what I wanted," and unlike in SVN, they can go back and try again before they push.

Once they get used to the idea of understanding where their changes are, then they can start deciding when to skip steps and combine certain operations (when to pull because you already know you want fetch+merge or when to click that Commit & Push option).

This is actually one of the enormous benefits of git over SVN, and git is designed with this usage pattern in mind. SVN, by contrast, assumes a central repository, so it's unsurprising if the tooling for git isn't as optimized for the same workflow. In SVN, if your commit is wrong, your only real recourse is a new commit to undo the mistake.

Doing this will actually naturally lead to the next strategy:

Encourage them to use local branches

Local branches actually ease a lot of the pain points of working on shared files. I can make all the changes I want in my own branch, and it will never affect anyone since I'm not pushing them. Then when the time comes, I can use all of the same merge and rebase strategies, only easier:


I can rebase my local branch, which makes merging it into master trivial.
I could use a plain merge (create a new commit) in master to bring my local branch's changes into it.
I can squash merge my entire local branch into a single commit on master if I think my branch is too much of a mess to salvage.


Using local branches is also a good start to figuring out a systematic branching strategy. It helps your users understand their own branching needs better, so you can choose a strategy based on needs and the team's current understanding/skill level and not just drop in Gitflow because everyone has heard of it.

Summary

In brief, git is not SVN and cannot be treated like it. You need to:


Eliminate the fear by encouraging safe experimentation.
Help them understand how git is different so they can see how that changes their normal workflow.
Help them understand the features available to help them solve their problems more easily.


This will all help you gradually adopt better git usage, until you reach the point where you can start implementing a set of standards.

Specific features

In the immediate term, the following ideas might help.

Rebase

You mentioned rebase and that you don't really understand it in your question. So here's my advice: try out what I just described. Make some changes locally while someone else pushes some changes. Commit your changes locally. Zip up your repository directory as a back up. Fetch the other person's changes. Now try running a rebase command and see what happens to your commits! You can read endless blog posts or receive training about rebase and how you should or shouldn't use it, but none of that is a replacement for seeing it live in action. So try it out.

merge.ff=only

This one is going to be a matter of personal taste, but I'm going to recommend it at least temporarily since you've mentioned you already have trouble with conflict handling. I recommend setting merge.ff to only:

git config --global merge.ff only


"ff" stands for "fast forward." A fast forward merge is when git doesn't need to combine changes from different commits. It just moves the branch's pointer up to a new commit along a straight line in the graph.

What this does in practice is prevent git from ever automatically trying to create merge commits. So if I commit something locally and then pull someone else's changes, instead of trying to create a merge commit (and potentially forcing the user to deal with conflicts), the merge will just fail. In effect, git will have only performed a fetch. When you have no local commits, the merge proceeds normally.

This gives users users a chance to review the different commits before attempting to merge them and forces them to make a decision about how to best handle combining them. I can rebase, go ahead with the merge (using git merge --no-ff to bypass the configuration), or I can even just put off merging my changes for now and handle it later. I think this small speed bump will help your team avoid making the wrong decisions about merges. You can let your team turn it off once they get better at handling merges.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:222639
Title: What's the best way to undo a Git merge that wipes files out of the repo?
Body: So imagine the following the happens (and that we're all using SourceTree):


We're all working off origin/develop. 
I go on holiday for a week.
My coworker has been working locally for the past several days without merging origin/develop back in to his local develop branch.
He tries to do a push, gets told he has to merge first, and then does a pull.
He gets a conflict, stopping the automatic commit-after-a-merge from proceeding.
Assuming that Git is like SVN, my coworker discards the "new" files in his working copy and then commits the merge - wiping those "new" files from the head of origin/develop.
A weeks worth of dev work goes on on top of that revision.
I come back from holidays and find out that several days of my work is missing.


We're all very new to Git (this is our first project using it), but what I did to fix it was:


Rename "develop" to "develop_old".
Merge develop_old into a new branch "develop_new".
Reset the develop_new branch to the last commit before the bad merge.
Cherry pick each commit since then, one by one, resolving conflicts by hand.
Push develop_old and develop_new up to the origin.


At this point, develop_new is, I'm hoping, a "good" copy of all of our changes with the subsequent weeks worth of work reapplied. I'm also assuming that "reverse commit" will do odd things on a merge, especially since the next few weeks worth of work is based on it - and since that merge contains a lot of things we do want along with stuff we don't.

I'm hoping that this never happens again, but if it does happen again, I'd like to know of an easier / better way of fixing things. Is there a better way of undoing a "bad" merge, when a lot of work has gone on in the repo based on that merge?


[Answer]: If I understood correctly, this is your situation:

    ,-c--c--c--c--M--a--a--X ← develop
o--o--y--y--y--y-´


After some common history (o), you committed and pushed your work (y). Your coworker (c) did work on his local repository and did a bad merge (M).
Afterwards there might be some additional commits (a) on top of M.

git reset --hard develop M^2
git branch coworker M^1


Now your graph looks exactly like before the bad merge:

    ,-c--c--c--c ← coworker
o--o--y--y--y--y ← develop


Do a good merge (G):

git checkout develop
git merge coworker


Resulting in:

    ,-c--c--c--c-、
o--o--y--y--y--y--G ← develop


Now transplant the additional commits:

git reset --hard X
git rebase --onto G M develop


This gives the final result:

    ,-c--c--c--c-、
o--o--y--y--y--y--G--a--a--X ← develop


Be aware that this might result in more merge conflicts. Also you just changed history, i.e. all your coworkers should clone/reset/rebase to the new history.

PS: of course you should replace G, M and X in your commands by the corresponding commit id.


[Answer]: It's good that you're thinking about how to fix the repository, but if your coworker only deleted new files and didn't overwrite a lot of updates, then a much simpler approach would be to simply restore the files that were deleted.

I'd probably start by trying to just cherry-pick (onto the current head) the original commits in which you added the code that has now gone missing. If for whatever reason that doesn't work, just check out the old revision with the files you added, and re-add them in a new commit.

What you describe is actually a subset of a well-known Git anti-pattern, which for the life of me I cannot remember the name of - but the gist of it is, making any "manual edits" during a Git merge (i.e. edits that aren't actually resolving any merge conflicts but are making some totally unrelated change) is considered an extremely bad practice because they are essentially masked by the merge itself, and are easily overlooked in code reviews or debugging sessions.

So definitely explain to your coworker that this was an epic fail, but consider slapping on a band-aid before prepping for major surgery.


[Answer]: We had the same thing happen. To make matters worse, we work with documentation files and not software, so compiles don't fail and the problem goes undetected for weeks until someone lays eyeballs on something and recalls that something should be there but isn't.
Here was our starting graph when the problem was noticed:
t1--t2--c1--c2--c3
      \           \
       t3--t4--t5..cM--t6--t7--t8  ("master")

c1..c3 is two weeks of the coworker's local work. cM is the bad merge that effectively undid our team's work of t3..t5 during that same two weeks. t6..t8 represents another two weeks of work on top of the bad merge.
I'm still learning Git, but I approached this a bit differently. I created and checked out a new temporary "reapply" branch at the head of "master" at t8:
git checkout -b reapply

and the graph looks like this:
t1--t2--c1--c2--c3
      \           \
       t3--t4--t5..cM--t6--t7--t8  ("master")
                                 \
                               ("reapply")

I cherry-picked the undone commits t3..t5 into the "reapply" branch:
git cherry-pick t3^..t5 --allow-empty-message

(I'm not worried about commit messages because I squash everything from "reapply" into "master" at the end.)
If the cherry-pick runs into a conflict, I use git status to report the conflicting files, resolve the conflicts manually, then do
git add 
git commit -m '' --allow-empty-message
git cherry-pick --continue --allow-empty-message

to continue. When cherry-picking completes, I check that the missing commits have been reapplied past the end of "master" in "reapply" with
git log --decorate --oneline --graph master^..reapply

and the graph should look like this:
t1--t2--x1--x2--x3
      \           \
       t3--t4--t5..xM--t6--t7--t8  ("master")
         \   \   \               \
          \   \   \               r1--r2--r3  ("reapply")
           \   \   \             /   /   /
            ----\---\-----------/   /   /
                 ----\--------------   /
                      -----------------

Now, I move back to "master", squash-merge the reapplied changes r1..r3 from "reapply" into a single commit with a suitable comment, and delete the "reapply" branch:
git checkout master
git merge --squash reapply
git commit -m 'reapply all changes lost by commit "xM"'
git branch -D reapply
git push

Now the graph looks like this:
t1--t2--x1--x2--x3
      \           \
       t3--t4--t5..xM--t6--t7--t8--t9  ("master")
                                   ^
                                   |
                              ("reapply" squashed
                             into a single commit)

and everyone in the team gets the work back next time they pull.
Would this have worked if our coworker discarded some but not all changes from t3..t5? In other words, is cherry-picking smart enough not to reapply a change that's already there? I'm not sure.
I don't know if this is the best solution. Other replies mention merging and rebasing in ways that I don't understand. But this was step-by-step, and I could understand what was happening, and it got us through this situation and I'm thankful for it.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:189796
Title: Implementing Context-based Undo/Redo Functionality
Body: I'm currently implementing an undo/redo mechanism using the Command Pattern. Everything works so far. My problem now is to implement the undo/redo functionality in a way that it is bound to a given context. 

Assume the following situation:
You have win1 and win2. In win1 you execute action1; then you switch to win2 and you execute action2 and action3. You "undo-stack" would look as follows


action3
action2
action1


Now what happens if win1 gets closed/removed and you start performing undos. At action1 the program would probably crash as the context is has been executed against previously (i.e. win1) doesn't exist any more. As such, when win1 gets closed, all the corresponding actions should be removed from the "undo-stack".

My question is on whether there exist already implementations/best practices for implementing such scenario properly? Or do you have any ideas?


[Answer]: What happens if win1 gets closed/removed? To answer this question, ask you another one: is the action reversible?

1. Case where the closing/removal action is reversible:

The closing/removal event is recorded in the undo stack, and when the user is performing undos, it un-does the closing/removal.

For example, when a user removes, through your application, a file win1, the app should not remove the file, but only mark it as a candidate for removal; then, a potential undo will un-mark the file.

2. Case of irreversible actions:

If you're talking about irreversible actions, given that the reversibility is not depending of your application, then deal with it, for example by disabling the corresponding undo elements.

For example, you're writing a tool which manages server farms, and at a given moment, the server win1 is materially unplugged from the network. In this case, your application can't replug the server, since it requires human intervention.

The application should then adapt its behavior in a more intuitive way: cancel or undos anterior to the irreversible one, skip the irreversible action, or something radically different and innovative. It's up to your interaction designer to come with the most intuitive way.

This is not much different from a case such as when your application tries to access a database, but the database is offline: you can't do anything about it, and the way you behave depends on the precise context.

Example:

Let's take an example from your comment: the removal of an Excel sheet. The successive actions are:


Change the cell A1 in sheet 1,
Change the cell A1 in sheet 2,
Remove the sheet 1.


Two possible behaviors would be:


The reversibility of sheet removal. Strangely, Excel team have chosen to not make it reversible. While this is probably motivated by some technical aspects I ignore, this behavior is totally wrong UX-wise. As a user, when by mistake I destroy a sheet, I expect to be able to get it back by pressing Ctrl+Z.

Not being able to do that makes it risky to use Excel, since I can never know what could be reverted, and what could result in a loss of work. That's not nice at all.
The impossibility to remove the sheet with the loss of "Change the cell A1 in sheet 1" step in the undo stack after the removal. Technically, it's not so difficult. Imagine the following structure:

class undoEntry
{
    string text; // Text which is displayed to the user in the undo history.
    sheet? correspondingSheet; // The sheet where the action happened, or null.
    undoAction action; // The action to undo.
}


With correspondingSheet property, you can then walk though the undo stack when a sheet is removed, and delete the entries you shouldn't display any longer.



[Answer]: Context is important from a programming/technical standpoint, as well as, usability. Your Excel comment is a good example. Several documents and then worksheets could be in the same instance of Excel and your undo would go across them (assuming you made recent actions on different ones), but this wouldn't happen in another instance of Excel or from Excel to Word. 

Users may be able to provide the context where they see the line drawn with this functionality. Microsoft could have built this functionality in the context of all open Office applications. I wouldn't like it as a user eventhough it is technically possible.


[Answer]: Apologies for necromancy but I've actually encountered teams scratching their heads over this a lot. Usually I find the solution very simple which is just use more than one stack: done, ship it, collect money.

That was a tad crude but that's usually sufficient and it makes sense from a UX perspective. Take this example of me using StackExchange right now. Say I was simultaneously answering a separate SE question and multi-tasking in a separate tab in my browser.

I don't want to ctrl-Z undo and start invisibly undoing changes I made in my other tabs (I'd hate to switch back to the other tab and realize I accidentally reverted all my work while working in this one), or have ctrl-Z make my browser switch tabs to on me without me explicitly saying so to make those changes visible. My current tab is my "current universe", and it has its own local data which is invalidated when I close the tab. So when I undo, I only want to undo changes in my current tab and not the other tabs. And the simple answer to allowing that is to have a separate undo stack for each tab (or even more granular context; in this case I suspect the text control itself has its own context and local undo stack) in the browser, not one undo stack to rule them all with entries that get invalidated. That also keeps the memory management tight and clean and so on without these invalidation concerns.

You'll have to forgive me if I'm a little bit passionate about this one since the team I worked with before started getting tempted to do all these fancy things to detect invalidation of undo entries and when I pointed out the UX concerns above, they were contemplating having multiple "stack pointers" into the stack which can be moved separately based on which tab/window we were in. I was like, "Nooooo! Just use more than one undo stack! Geeeeeeez!!!" :-D I think I got some gray hairs just specifically from that one.

Having a separate undo stack for each window should be enough in your case given what you said here:


  
    At action1 the program would probably crash as the context is has been executed against previously (i.e. win1) doesn't exist any more. As such, when win1 gets closed, all the corresponding actions should be removed from the "undo-stack".
  


If that data is invalidated when win1 is closed, that implies the data belongs to the window and doesn't make sense outside of it, just as with my browser tab analogy. So that should go into a different undo/command stack as I see it -- don't jumble them all up in one stack.

If these windows share state and there are changes to the underlying shared business logic side of the data along with data local and specific only to a particular window being modified simultaneously with one user input action, then that gets a lot more tricky and we have to rethink a lot more. But given what I usually encounter and the specific problem you are facing now, I suspect just using multiple undo stacks should be enough, and the undo stack local to win1 would be tossed away when you close that window.


------------------------------------------------------------

ID: cs.stackexchange.com:930
Title: Adding elements to a sorted array
Body: What would be the fastest way of doing this (from an algorithmic perspective, as well as a practical matter)?

I was thinking something along the following lines.

I could add to the end of an array and then use bubblesort as it has a best case (totally sorted array at start) that is close to this, and has linear running time (in the best case).

On the other hand, if I know that I start out with a sorted array, I can use a binary search to find out the insertion point for a given element.

My hunch is that the second way is nearly optimal, but curious to see what is out there.

How can this best be done?


[Answer]: We count the number of array element reads and writes. To do bubble sort, you need $1 + 4n$ accesses (the initial write to the end, then, in the worst case, two reads and two writes to do $n$ swaps). To do the binary search, we need $2\log n + 2n + 1$ ($2\log n$ for binary search, then, in the worst case, $2n$ to shift the array elements to the right, then 1 to write the array element to its proper position).

So both methods have the same complexity for array implementations, but the binary search method requires fewer array accesses in the long run... asymptotically, half as many. There are other factors at play, naturally.

Actually, you could use better implementations and only count actual array accesses (not accesses to the element to be inserted). You could do $2n + 1$ for bubble sort, and $\log n + 2n + 1$ for binary search... so if register/cache access is cheap and array access is expensive, searching from the end and shifting along the way (smarter bubble sort for insertion) could be better, though not asymptotically so. 

A better solution might involve using a different data structure. Arrays give you O(1) accesses (random access), but insertions and deletions might cost. A hash table could have O(1) insertions & deletions, accesses would cost. Other options include BSTs and heaps, etc. It could be worth considering your application's usage needs for insertion, deletion and access, and choose a more specialized structure.

Note also that if you want to add $m$ elements to a sorted array of $n$ elements, a good idea might be to efficiently sort the $m$ items, then merge the two arrays; also, sorted arrays can be built efficiently using e.g. heaps (heap sort).


[Answer]: Because you are using an array, it costs $O(n)$ to insert an item - when you add something to the middle of an array, for example, you have to shift all of the elements after it by one so that the array remains sorted.

The fastest way to find out where to put the item is like you mentioned, a binary search, which is $O(\lg n)$, so the total complexity is going to be $O(n + \lg n)$, which is on the order of $O(n)$.

That being said, if I felt particularly snarky, I could argue that you can "add to a sorted array" in $O(1)$, simply by slapping it to the end of the array, since the description doesn't indicate that the array has to remain sorted after inserting the new element. 

Anyway, I don't see any reason to pull bubble sort out for this problem.


[Answer]: If you have any reason for not using heap, consider using Insertion Sort instead of Bubble Sort. It's better when you have a few unsorted elements.


[Answer]: Patrick87 explained this all very well. But one additional optimization you could make would be to use something like a circular buffer: you can move items right of the position of the inserted element to the right, as usual. But you can also move items to the left of the correct position to the left. To do this, you need to treat the array as circular, i.e. the last item is right before the first one and it also requires you to keep the index where the items currently start.

If you do this, it could mean you make about half as many array accesses (assuming uniform distribution of indexes you insert to). In the case of doing binary search to find the position, it's trivial to choose whether to shift to the left or to the right. In the case of bubble sort, you need to “guess” correctly before starting. But doing that is simple: just compare the inserted item with the median of the array, which can be done in single array access.


[Answer]: I have used the Insertion sort algorithm effectively for this issue. At one time we had a performance issue with a hash table object, I wrote a new object that used binary search instead that increased performance significantly. To keep the list sorted it would keep track of the number of items added since the last sort (i.e. number of unsorted items,) when the list needed to be sorted due to a search request, it performed an insertion sort or a quick sort depending on the percentage of items unsorted. Use of the insertion sort was key in improving the performance.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:355170
Title: What is wrong with this algorithmic solution of mine that checks whether a given function returns a sorted array when an array is given as input?
Body: An interviewer asked me this question:


  Given a function f(ar[]) where ar[] is an array of integers, this
  functions claims to sort ar[] and returns it's sorted version ars[].
  Determine if this function works correctly.


I approached this question as:


  
  First check if the returned array ars[] is actually sorted in either non increasing or non decreasing order. This one is easy to check, ars[] should
  either follow the sequence ar[i + 1] >= ar[i] (for an array sorted in
  non decreasing order) or ar[i + 1] 
  Then check if sizes of both the input array ar[] as well as the output array ars[] are same.
  Finally check if every element of ar[] is also present in ars[]. Since we have already examined at step 1 that ars[] is sorted and at
  step 2 that sizes of ar[] and ars[] are same we can use Binary
  Search algorithm to perform this action. The worst case time
  complexity for this should be O(n * log(n)).
  
  
  If all the above 3 checks succeeds then the function is working fine
  else it is not.  The overall time complexity of this algorithm should O(n * log(n))


But to my surprise the interviewer said that this solution is not correct and it's time complexity can be improved. I can not understand what actually is wrong with my solution, did I miss any corner case or the entire approach is wrong? Also what can be better approach to this(in terms of time complexity)?

PS: The interviewer mentioned no additional information or any additional constraint for this problem.   


[Answer]: While you say the interviewer provided no other information, was it possible to ask questions? As there is only one argument to the function, I would be under the assumption that the function would always return the array sorted it one directory, most likely ascending. i.e.

fn([4,1,5]) -> [1,4,5]


But obviously that's something the interviewer should confirm for you as it affects the answer. (Also it would be very strange for the function to return asc OR desc, without any way to specify).

As such, my layman's approach would be to pass an unsorted array to the function, and then compare that against a second array I already knew was sorted -

expected = [1,4,5]
sorted = fn([4,1,5])
// compare expected and sorted...


As far as basic testing goes, the process of comparing every element against each other, and also comparing every single element to the source array does seem convoluted - compared to just comparing against a known correct list.


[Answer]: Your method is only testing that the original and sorted arrays contain the same values, not that they contain the same number of each value; e.g. 1112 would pass for 1221

In step 3 you could, for example, mark that a particular value in the sorted array has already been matched (removing from the array would be too time consuming) but then the search would no longer be a binary search as you would hit already used values. 

[Obviously this isn't a problem if the values are unique but that isn't stated]


[Answer]: For speed improvements you can test with known arrays with either presorted answers to check against or simply run through the returned array ra checking ra[n] = for all n in the range 0..len(ra-1) complexity of which is very low. You can determine if every element is present in each array by counting the instances of each value and then comparing the counts.

Your testing should also include corner cases such as ar=[1] and ar=[] and for an interview I would at least mention testing for invalid inputs such as arrays of non-integer values, non-arrays, etc., I know that the behaviour in such cases is undefined in the case you were given but a part of a testers job is to highlight specification omissions and ambiguities such as what is the error handling. If you only test for what is specifically in the specification you will not make a good tester and this sort of issue is one of the things that the interviewer will be looking for.


[Answer]: As already mentioned by @Dipstick, step 3 can fail if there are duplicates in the input array. To resolve this and improve the time complexity, one can use a dictionary with the array elements as keys and their number of occurrence as values. Such a dictionary can be created from the sorted and the unsorted array as well in O(n), and you need to test if the resulting dictionaries are identical, which can be done in O(n), too. 

One can combine this using only one dictionary counting the total number of occurrences in the unsorted array minus the number of occurrences in the sorted array. In pseudo code (assuming a default of 0 for values in the dictionary when the key is used the first time): 

 for(e in ar)
     noOfOccurence[e]+=1; 
 for(e in ars)
     noOfOccurence[e]-=1;

 for(e in noOfOccurence.Keys)
     if(noOfOccurence[e] != 0)
         return false;

 return true;



[Answer]:   > But to my surprise the interviewer said that this solution is not correct


If i were a job interviewer i would expect to get the answer: write a simple unittest for the sort method
with a sample unsorted input and the expected output (see @USD Matt answer)

  > time complexity can be improved.


Your solution looks OK on the first glance and more complete/complicated than the simple example.

If i were searching for an experienced developer i would expect that the applicant knows unittests and does not more than neccessary.

I assume that the complex/complete automated testing is overkill and not necessary for testdriven development. A simple example should be enough.


[Answer]: If you keep track of the min and max a array of size max - min + 1 will work if the range is not large. 

Subtract min from each value so it starts at 0 

for(e in input)
     ar[e - min] +=1; 
for(e in sorter)
     ar[e - min] -=1;

for(e in ar)
     if(e != 0)
         return false;

return true;


You could check for duplicates as you test for sort
Yes binary would be O(n logn) but is might be faster than dictionary  


[Answer]: Take a step back and look at this from a simplistic standpoint. You are making your first step be the most time and compute intensive test. I'd start with a series of sanity checks which catch the most glaring issues and avoid having to (re)sort the array and compare element-by-element. 

Put your test to ensure the number of items in the result equals the number of items fed in because if the number of items don't match you can fail the test and not go any further. 

Next test that the first element is less than the last element. 

If that passes, spot check some elements in between, for example the first and last, and the items at the 1/4th mark, 1/2 mark, and 3/4th mark - and make sure first 

Assuming those tests pass, then I'd dig into iterating all the elements to test the integrity of the sort. But I wouldn't resort the input to compare, but simply iterate the results in order, comparing this item with the previous one and making sure this item is bigger than the previous one. Bail out with a failed test as soon as you hit one that is smaller than its predecessor. 

The runtime of the test will be the same as the time it takes to sort the list but only if the sort is good. If the sort is not good, the runtime will be some degree shorter than it would've taken to re-sort and test. How much smaller depends on if the failed value is on the front end of the data or the back end of the data group.

Typically, there is something about the data being sorted, and especially where that data is coming from which can point you to the cases where sorting errors are most likely to crop up. If you have the luxury of having insight into these things, then craft sanity checks to catch those cases and put that before the visitation of every item to verify sorting. 


------------------------------------------------------------

ID: cs.stackexchange.com:145288
Title: Searching through an unsorted array with a better than O(n) complexity?
Body: This question kind of puzzled me, it was presented as a homework assignment for 2nd year undergrads and one of them approached me in case I could give him some pointers, but the problem is I couldn't even think of the answer myself.
They start with an unsorted array of integers and they're to find an element (there could be any number of them, guaranteed to be at least one) whose value is less than that of its neighbors, recursively.
Due to the nature of the problem the array can't be sorted, yet they asked them for a better than linear time solution. How could such a thing be?
He told me they explained binary search in class, but it doesn't apply, just like any other search  algorithm based on sorted arrays; still, apparently he tried to do the assignment traversing the array (after all, numbers meeting that criteria could be anywhere, it's not like you can discard part of it), but his teachers added a couple of unit tests to ensure that the search algorithm is both recursive and not O(n).
They gave them two examples:

[1, 2, 3, 4] -> Only the 1 would meet that criteria.
[1, 3, 4, 1, 1] -> All three 1s would meet that criteria.

And they also gave them hints:

It can be solved similarly to how binary search works, via divide and conquer, having the function receive the array, and two integers that mark the beginning and end of a subarrays.
If the subarray size is 1, that's that location is good to go.
If its size is 2, the biggest of them is the one.
If its size is >= 3, you're to check the middle one and its neighbors, if it's smaller than the neighbors you're done, otherwise you'd choose the interval based on the comparison.

The emphasis is mine, but, what the hell?
I mean, if an array is size 2, let's say, [1, 2], the one being smallest should be the chosen index, not the biggest; leaving that aside though, given the description of the problem, how is that comparison going to allow you to discard some part of the array? Which partitions would you take when the whole thing is unsorted?
My days of working with these kinds of algorithms are long gone, but I couldn't figure out what they wanted them to do. Is there something I'm missing? How could this be of logarithmic complexity?


[Answer]: You are right.
For example, if the array is:
      |  0  if i=52
a[i]= |  1  if 152

, it is impossible to use the binary search. You cannot recognize the half containing the single local minimum by a[] at its end and middle points. They all have the same meanings for both halves.
So, there is an error in the formulation of the task.
(BTW, it would be much better to put an exact formulation of the task in the question)


[Answer]: Your example $1,3,4,1,1$, in which you state that all $1$s are less than their neighbors, suggests that by "less than" you actually mean "at most". Any array contains an element whose value is at most that of its neighbors — just take any minimal value of the array.
As you mention, if the array has length $1$ or $2$ then the problem is easy (when the array is $x,y$, you choose the minimum of $x,y$).
Given an array $A$ of length $n \geq 3$, let us split it as follows: $B, x, y, z, C$, where $B,C$ are subarrays of lengths $\lfloor \frac{n-3}{2} \rfloor, \lceil \frac{n-3}{2} \rfloor$. We consider three cases:

If $y \leq x$ and $y \leq z$ then we choose $x$.
If $y > x$ then we recurse on $B,x$, an array of length $\lfloor \frac{n-1}{2} \rfloor$.
If $y > z$ then we recurse on $z,C$, an array of length $\lceil \frac{n-1}{2} \rceil$.

The procedure terminates in $\log_2 n$ steps, and so runs in time $O(\log n)$.

If you are actually looking for an element which is strictly less than its neighbors, you cannot do better than linear time. Indeed, consider any algorithm that makes at most $n-2$ queries. We run the algorithm, and whenever the algorithm accesses an array element, we always answer $1$. We claim that at the end, the array cannot output any index. Indeed, suppose that it inputs $i$. Since the algorithm makes at most $n-2$ queries, there is some index $j \neq i$ it hasn't queried. The queries made by the algorithm are consistent with an array in which $A[j] = 0$ and $A[k] = 1$ for all $k \neq j$, in which the only correct answer is $j \neq i$. Therefore at least $n-1$ queries are necessary.


------------------------------------------------------------

ID: cs.stackexchange.com:113343
Title: Why is heap insert O(logN) instead O(n) when you use an array?
Body: I am studying about the arrays vs heap for make a priority queue

For check the heap implementation I am reviewing this code:  Heap

, but I have the following question.

Heap is based on array, and for creating an array you need O(n), for inserting into a heap you need O(logN), so if you have a unordered list of task and you want to create a heap, you need O(NLogN).

If you use an array, you need O(n), to create the array, and O(NlogN) to sort the array, so you need O(NLogN).

So if you need implement some similar to this:

function priorityQueue(listOfTask l)


There isn't a diference betwen use an Array or an Heap right? So, why I should use a heap instead an array for solve this function?

Thanks


[Answer]: Keeping a heap is more efficient than keeping a sorted array, when you need to keep adding items to the priority queue. In case you don't need to add to it, you don't need a queue in the first place, just an array sorted by priority.

Insertion to heap-based priority queue is O(logN), while insertion to sorted array is O(N) (binary search for position is O(logN), but inserting there is O(N) ).

As you can see here, almost all data structures are about trade-offs. Converting an array to heap (and keeping it heap) you gain something (O(logN) priority queue action) but also lose something (ability to iterate contents in order, binary search). In contrast, if you used an unsorted array, you'd gain O(1) insertion (because you can just append), but almost everything else would be O(N), which is excellent if you have handful of items, but becomes bad if you have dozens, impossible if you have thousands.


[Answer]: Given your link, you seem to be interested in data structures supporting the following operations:


Create(m): create a new instance with room for m elements.
Size(): return the number of elements currently stored in the instance.
Insert(k): insert an element with priority k.
ExtractMax(): return the maximal priority currently stored, and remove it.


Since Size() is easy to implement in constant time as part of the other operations, I will not discuss it below.

Here are three sample implementations:

Unsorted array:


Create() simply allocates memory.
Insert() simply inserts the element at position i+1, where i is the current number of elements.
ExtractMax() goes over the entire array, finding the maximum; and then "compacts" the array by moving all elements to the right of the maximum one entry to the left.


Sorted array: (your implementation)


Create() simply allocates memory.
Insert() first scans the array to find the proper location for the element, then moves all elements to the right of the intended location one entry to the right, and finally stores the element.
ExtractMax() returns and removes the ith element, where i is the number of elements currently in the instance.


Binary heap:


Create() simply allocates memory.
Insert() and ExtractMax() are described on Wikipedia.


Binary heaps are implemented using "partially sorted" arrays. That is, the order of elements in the array is not arbitrary, but it is also not completely determined. Rather, we are only guaranteed that A[i] ≥ A[2i],A[2i+1]. This freedom allows us to trade-off the running time of the two operations Insert() and ExtractMax().

In all of these implementations, Create() is roughly the same, so to compare the various implementations it suffices to consider the two operations Insert() and ExtractMax():

$$
\begin{array}{c|c|c}
\text{Implementation} & \text{Insert()} & \text{ExtractMax()} \\\hline
\text{Unsorted array} & O(1) & O(n) \\
\text{Sorted array} & O(n) & O(1) \\
\text{Binary heap} & O(\log n) & O(\log n)
\end{array}
$$

Here $n$ is the number of elements currently in the array.

If you perform many Insert() and ExtractMax() operations, a binary heap is likely to be more efficient.

Another operation which you mentioned is


Initialize(A): add to an empty instance the elements in A


You can add support to this operation to all different implementations mentioned above:


Unsorted array: simply copy A to the array. Running time: $O(|A|)$.
Sorted array: copy A and sort the resulting array. Running time: $O(|A|\log|A|)$.
Binary heap: run Insert() for each element of A. Running time: $O(|A|\log|A|)$.


Considering this operation doesn't strengthen the case of your implementation.


[Answer]: A priority queue does something entirely different than sorting an array. 

The important operations for a priority queue are: 1. Add an item to the queue. 2. Tell us the smallest item in the queue and remove it from the queue. Both these operations run in O(log n).

Now use a sorted array. Operation 2 is fast if we sorted in descending order. But operation 1 isn’t: Adding a random value to a sorted array and keeping the array sorted requires moving n/2 array elements on average. 

You could sort an array in O(n log n) by adding all items to a priority queue, then removing the smallest item n times. Fast sorting algorithms process the array as a whole, they don’t try to keep part of the array sorted (straight insertion or binary insertion do; they ar not fast). Quicksort moves items very roughly into the right place at each step. Note that a priority queue only performs a rough ordering of the items as well. That’s what makes both Quicksort and a priority queue fast: They don’t ask for the data to be sorted at all times. 


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:329682
Title: How can I use local Git with VSS
Body: My employer still uses VisualSourceSafe6. Everything is checked-in into the main project directory in VSS. They are not using branching in VSS. Check-in's are supposed to be ready for release to QA and should not break the build (not enforced).

I want to use git as my local repository so I can track changes per feature (branch) and have history of my changes before I commit them to VSS. I need this because some of the changes might span more than 3-4 days before they can be checked-in to VSS. 

NOTE: my understanding for some git concepts is very new and might be offbase.

After reading these similar questions:


Using Git with Visual Source Safe 6.0 
How Best Can I Use Git When My Employer Uses VSS? 
Combine DVCS with Visual Source Safe


Following is what I was thinking of doing:

C:\VSS\ProjectA 
where I perform VSS specific actions get-latest-version, chechout, and checkin etc. The solution/project file will have bindings to VSS since other developers will still use VSS and some might use git with VSS like me.

C:\MY\ProjectA
where I will be doing my development work and using git workflows.

To setup the folders I did:


Get latest version from VSS into C:\VSS\ProjectA
C:\VSS\ProjectA>git init and C:\VSS\ProjectA>git add ., this becomes my master branch
git clone C:\VSS\ProjectA C:\MY\ProjectA, this becomes my development clone


Q: If I have VSS bindings in my solution/project file, VisualStudio will try and connect to VSS even from my git cloned folder C:\MY\ProjectA. So I need to remove the VSS bindings after cloning, correct?

When working on a feature I need to:


get latest version from VSS into C:\VSS\ProjectA
Q: do I need to commit the changed file to the master git repository?
C:\MY\ProjectA>git fetch and C:\MY\ProjectA>git checkout, Q: correct?
do whatever branching, commits, merge, rebase etc in my development repository
get diff of my development repository and master repository and checkout those files in VSS
C:\MY\ProjectA>git push, Q: correct?
C:\VSS\ProjectA>git checkout, Q: correct?
C:\VSS\ProjectA>git commit, Q: correct?
checkin changed files to VSS


UPDATE it seems git for windows (VisualStudio2015) does not yet support pushing to non-bare local repositories, is there any workaround?


[Answer]: My way with VSS 2005 for your reference.

On project Day 1,


Check out the whole project Recursively to a directory named projVss, e.g.
Undo the checkout to release it to others.
Copy the directory projVss to another named projMy
Do git init both projVss and projMy


On any day N,


Work on projMy. Do any versioning you like and leave projVss alone.


To submit your work,


Commit your work and Cleanup any uncommitted files in projMy
Check out the whole project Recursively to projVss. Commit changes that checked in by others to projVss without necessary review. 
Copy and overwrite the working files from projVss to projMy. Discard the changes from your files of old version and merge others' codes. 
Generate a patch by

git diff lastSubmitVerHash head --name-only | zip ../projPatch.zip -@
Apply the patch to projVss by extracting it and overwriting
Commit the patch in projVss. Review the merges.
As VSS does not add files automatically when checkin, generate a patch of newly added files by

git diff lastSubmitVerHash head --name-only -diff-filter=A | zip ../projPatchNew2Vss.zip -@
Similarly, as VSS does not remove files automatically when checkin, list out files to be deleted by

git diff lastSubmitVerHash head --name-only -diff-filter=D
Check in recursively in VSS.
Extract the files in projPatchNew2Vss.zip to VSS
Delete files according to 14, manually, that's what I do, since it should not be many.


On day N+1, repeat 5 - 16, you should be able to do 8 - 14 quickly to enable a short check-out(locking) time.

The problem

You still need to pay attention to VSS Checkout warnings, for the file(s) you need to update. If it is being checked out by others, you still need to ask your teammate to release it.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:74764
Title: How often should I/do you make commits?
Body: I am a recent (as of yesterday) college grad - BS in Computer Science. I've been a huge fan of version control ever since I got mad at an assignment I was working on and just started from scratch, wiping out a few hours worth of actually good work. Whoops!

Since then, I've used Bazaar and Git (and SVN when I had to for a Software Engineering class), though mostly Git to do version control on all of my projects. Usually I'm pretty faithful about how often I commit, but on occasion I'll go for a while (several functions, etc.) before writing a new commit. 

So my question for you is, how often should you make commits? I'm sure there's no hard and fast rule, but what guideline do you (try to) follow?


[Answer]: Whenever I have something that works (meaning, doesn't break anything for anyone else) I'll do a check-in. I write most of my stuff test first, so that means every time I have a new test that passes, I check-in.

In practice that means a few times per hour. At least a couple of times every hour, with 5 being quite a bit.


[Answer]: Automatically commit after every successful build.

I assume that if you want to revert to an earlier version, and you can remember the approximate wall-clock time of the point you want to revert to, you can just look up at that revision.

Of course if you use TDD then there are better integrated solutions.


[Answer]: At least once a day, usually three or four as I tend to commit anytime I take a break.  I work on my own, so I don't have to worry about messing someone else up, and I can always revert a file if I think I've gone down the wrong path.

I had a bad experience once when I wasn't making commits regularly and my hard drive crashed and I lost a few hours work.  Since my repository is on another machine and is backed up regularly, there's no reason I should have lost anything.


[Answer]: On DVCS, I commit basically anytime I'm starting on a change that would be really annoying if I couldn't undo, anytime I need to share with others, or anytime it would be really useful to see what I changed since that point.  That ranges from several times an hour to a couple times per day.  Also, I very rarely commit when my build doesn't at least compile and run without immediate segfaults, although that can be useful too if you want to be able to isolate and share just the fix for that particular bug.

Note with centralized version control, which you are very likely to be required to use on the job, you generally don't commit until you're pretty sure you won't get a storm of angry emails for doing so.  Depending on how independent the changes you're making are, and how much your company subscribes to the continuous integration dogma, that's more along the lines of once a day to maybe once a month on a big new feature.

That frequency difference is one of the main reasons I use git on top of my company's official centralized VCS.


[Answer]: Every night when I go home I have a script that will automatically go through all my branches and do a commit (note all developers don't get these changes.  Its just my personal branch).  This way I don't have to think about it.  

Also I have a script that will automatically run an update in the morning.  I use this one so that if I work from my laptop at night my changes are automatically there in the morning.

As for rolling back changes that I make during the day my editor has a history of all changes and is much faster than doing a commit on every save/compile.


[Answer]: Thanks to git branches and how they are easily managed, I commit really often. I would say, whenever you add something that you feel is important and that might add up to building something later, you should commit.

Also, I often find myself doing some refactoring and adding new tests before the new code, so for this case I commit for every time I run the test, watch it fail, make it pass again and refactor.

Also, after any bug fix no matter how tiny or insignificant it might seem.

I sometimes go down the "commit it as soon as it compiles" road as well.


[Answer]: It depends a little on what the development environment is.

If you're the only one contributing to a code base, then a deferred commit won't be that crucial. However, if you're in a team of several developers, and everyone thinking "oh well, I'll wait with the commit a little" then you'll often end up handling a lot of conflicts and losing a lot of time.

The general rule (for both scenarios) would be: Commit as often as possible. If you think "it's not ready yet" (because it'll break the build or simply isn't done yet) then create a branch and commit to that branch but make sure you do commit.


[Answer]: If your working in a team environment with a build server, you'll only want to commit when you've got something that builds. It's very annoying for the rest of the team otherwise :)


[Answer]: I work alone. My development environment is my notebook. The commit environment is a server on the web.

If the client uses the web server to keep track of progress. So nearly every day I do an upload along with an e-mail saying what's new. 

If the commit environment is a production environment then I have to be more careful about every change being tested before any commit. But still, often seems better, as nobody has doubts about whether the new feature is planned or done already; they get what they ask for ASAP.


[Answer]: You shouldn't commit based on a time basis, but on a feature basis. Whenever you add a new feature that's worth committing, commit. You added a working method? Commit. You fixed a typo? Commit. You fixed a file's wrong indentation? Commit. There's nothing wrong committing small changes, as soon as the commit is relevant.
What is wrong is committing a huge number of changes, with no relations between each others. It makes it very hard to detect the commit source of a given regression.
Sometimes, I make twenty commits in an hour, and sometimes I commit once a day, depending of the amount of code that was modified.
Making small commits allows you to use very powerful tools like git-bisect.
Don't forget the rule n°1 of the committer: never break the trunk. If you need to make several commits that are likely to break the trunk, create a branch instead.


[Answer]: Watched this Joel On Software - FogBugz/Kiln presentation recently and think it relates well to your question.

While in some version control 'commit' is synonymous with 'publish' (or 'give pain to others') in distributed VCS they are often seperated.  

So when to commit may depend on the VCS you use.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:307571
Title: Git Bisect found the buggy commit. Now what?
Body: I've seen blog posts explaining how to do Git bisect, but how to you fix the bug, commit again and maintain the rest of the commit history?

An actual problem I faced in my project was like this (the image is just an approximation):



I had marked the top-most commit as bad and the bottom-most commit as good. Some of the commits below Buggy commit A were good commits, because somewhere in-between, I had commented out the code that made Bad commit B a bad commit. So naturally, git bisect led me first to Buggy commit B.

I corrected the bug and when I tried to commit, I wanted the changed lines of code to be applied to the same Buggy commit B commit. But I found out that: 


Git would always create a new commit for this  
I got the detached head error:

HEAD detached at b855e36 Changes not staged for commit:
modified:  main.cpp



Now what do I do?

Create a new branch (let's name it fixedit) and commit this corrected code? If I do, then what happens to all the other commits above it? Should I do a rebase (never done it before) of the commit above Buggy commit B and put the entire line of commits onto the new fixedit branch?

I would prefer that I'd just be allowed to modify the existing commit and the entire tree would remain as it is, but then I'd have to go to all the commits above Buggy commit B and fix the buggy line in all those commits. That doesn't make sense. So how does one fix bugs and yet retain commit history properly?  

UPDATE:

So I do a git bisect reset and fix the code on the topmost commit (because I know which line of Buggy commit B introduced the bug)?

Like I mentioned earlier, the code that caused Buggy commit B was commented out in one of the commits just before Buggy commit A.


So when in my topmost commit I see that the line is commented out, I realize that the bug is somewhere else.
During the first bisect since one of the commits between A and B was marked good, I'd choose that as the good commit for my second Git bisect attempt, and the topmost newest commit as the bad commit and then continue with Git bisect. That'd lead me to Buggy commit A.
Then I'd do a git bisect reset and correct the lines of code in the topmost code and then commit.


Ok; that makes sense. Thanks :-)


[Answer]: You don't fix that commit. It's a fact that the bug was introduced, and that's okay. You found it now and know how to fix it, great! Like with any other bug fix, you create a new commit on top of the current state of the project. This retains the history and fixes the bug. Rewriting the history so that the bug never existed is pointless, even dangerous:


It doesn't have any benefit. I mean, you could pretend the bug never existed, but if you had to bisect then the bug probably existed for a while and was encountered by users. Rewriting git history doesn't undo that. It will just make it harder to understand what happened.
If the code at and around the location of the bug fix has changed in other ways since the bug was introduced (which seems to be the case), you can't just fix the bug in commit B and rebase all later commits on that. You would have to manually resolve conflicts, which takes time and risks introducing new bugs.
The usual concerns regarding rewriting history apply in force. Rebasing so many publicized commits will cause trouble for everyone who has work based on these commits (e.g. if they have a branch based on a commit after B, they won't be able to merge this branch easily). More generally, it breaks all references to specific commits (that came after B), since all hashes change.



------------------------------------------------------------

ID: softwareengineering.stackexchange.com:253919
Title: How to cleanly add after-the-fact commits from the same feature into git tree
Body: I am one of two developers on a system.  I make most of the commits at this time period.  My current git workflow is as such:


there is master branch only (no develop/release)
I make a new branch when I want to do a feature, do lots of commits, and then when I'm done, I merge that branch back into master, and usually push it to remote.


...except, I am usually not done.  I often come back to alter one thing or another and every time I think it is done, but it can be 3-4 commits before I am really done and move onto something else.

Problem

The problem I have now is that .. my feature branch tree is merged and pushed into master and remote master, and then I realize that I am not really done with that feature, as in I have finishing touches I want to add, where finishing touches may be cosmetic only, or may be significant, but they still belong to that one feature I just worked on.

What I do now 

Currently, when I have extra after-the-fact commits like this, I solve this problem by rolling back my merge, and re-merging my feature branch into master with my new commits, and I do that so that git tree looks clean.  One clean feature branch branched out of master and merged back into it.  I then push --force my changes to origin, since my origin doesn't see much traffic at the moment, so I can almost count that things will be safe, or I can even talk to other dev if I have to coordinate.  But I know it is not a good way to do this in general, as it rewrites what others may have already pulled, causing potential issues.  And it did happen even with my dev, where git had to do an extra weird merge when our trees diverged.

Other ways to solve this which I deem to be not so great


Next best way is to just make those extra commits to the master branch directly, be it fast-forward merge, or not.  It doesn't make the tree look as pretty as in my current way I'm solving this, but then it's not rewriting history.
Yet another way is to wait.  Maybe wait 24 hours and not push things to origin.  That way I can rewrite things as I see fit.  The con of this approach is time wasted waiting, when people may be waiting for a fix now.
Yet another way is to make a "new" feature branch every time I realize I need to fix something extra.  I may end up with things like feature-branch feature-branch-html-fix, feature-branch-checkbox-fix, and so on, kind of polluting the git tree somewhat.  


Is there a way to manage what I am trying to do without the drawbacks I described?  I'm going for clean-looking history here, but maybe I need to drop this goal, if technically it is not a possibility.

Sample After-the-Fact Commit Messages

Main commits:  alter module behavior to give user more information


Here I thought I was done, check out,  & merge to master, then oops, forgot this (OFT):

OFT:  (enhancement) added better error messages


Okay, now I am done!

Later, user comes back and asks for an enhancement/feature.  Turns out a certain input number for an order can have a revision associated with it.  So it's really two numbers, not one.

OFT:  (feature-add):  fix up order input to accept revisions


Now I'm done ...

OFT:  (bug-fix) there was a bug with the two-number input method, fix SQL for that


okay now I'm done ... 

OFT:  (enhancement) oh wait, improved error messages again, for clarity!



[Answer]: When you merge into master, you are essentially declaring "I'm done".  It's like dropping your mail at the post office, or signing a lease.  You can't change your mind after merging (of course, in reality you can, thanks to the wonders of technology; it's just bad etiquette when collaborating with others).

If you feel that you need to add more to it, then what you're adding is either:


A non-essential improvement, which means you can treat it as a new branch and work from there.
Or an important bug fix, which should happen very rarely – this is why one should always run the tests before merging into master.  In this case, be sure to let everyone know that master is broken and resolve the situation quickly.


If you feel that your feature still needs more revising, then don't merge into master!

If for some reason other developers want a sneak peek of what you're doing, you could just push the feature branch instead, or make a dedicated temporary branch with all the caveats of being "temporary".

(Of course, there's nothing sacred about master per se, it's just that by convention most people expect it to be a stable base from which everyone can pull.)

Note that branches do not necessarily correlate with your features.  Git is for tracking history and thus commits don't necessarily happen in a logical and neat way sometimes.


------------------------------------------------------------

ID: cs.stackexchange.com:154298
Title: How do you specify Big-Theta of an algorithm when the Big-O and Big-Omega are different?
Body: I understand that if f(n) ∈ O(g(n)) and f(n) ∈ Ω(g(n)), we can conclude that f(n) ∈ Θ(g(n)).
But what if we have an algorithm where the Big-O and Big-Ω are different? For example f(n) ∈ O(n^2) and f(n) ∈ Ω(n). Then what would be its Big-Theta notation?
To me, it seems that both f(n) ∈ Θ(n^2) and f(n) ∈ Θ(n) would be technically correct claims. I suspect I might have misunderstood something and I would appreciate if anyone could clarify this.


[Answer]: There are two possibilities:

either the Big-$\Theta$ lies somewhere between the Big-$O$ and Big-$\Omega$ but we can't tell from the given;

or there is no Big-$\Theta$ expression at all (but we can't tell from the given).


E.g.
$T_n=O(n^2), T_n=\Omega(n)$ and $T_n=\Theta(n^2)$ are all true,
where $T_n$ denotes the $n^{\text{th}}$ triangular number, and
$n^2\cos^2(n)+n\sin^2(n)=O(n^2),n^2\cos^2(n)+n\sin^2(n)=\Omega(n)$, but there is no $\Theta$.


------------------------------------------------------------

ID: cs.stackexchange.com:129595
Title: What is the difference between Big O, Omega, and Theta?
Body: I know that this question is asked a lot of time but I don't understand or I think, I got lost when I was reading Introduction to algorithms
They said, "It is not contradictory, however, to say that the worst-case running time of insertion sort is Omega(n^2), since there exists an input that causes the algorithm to take Omega(n^2) time.".
I read many articles that Omega is the best-case. As I understand correct me if I'm wrong, that the best-case is Omega notation and why they said, "since there exists an input that causes the algorithm to take Omega(n^2) time.", I don't understand why they called it worst-case and why I can call it Omega(n^2), isn't Big O for worst-case.
Also I don't understand they said, "Theta is notation is a stronger notion than Big O".
Why is that? and When should I call an algorithm that it's Big O of whatever, Theta or Omega?
Because I'm confused and I don't know which one is for or how to use them.


[Answer]: Check the definitions, e.g. in Hildebrand's Introduction to asymptotics. In a nutshell, for the usual computer science use for running times (all relevant functions positive), it is said that:

$f(n) = O(g(n))$ if there are $n_0$ and $c > 0$ so that for all $n \ge n_0$ it is $f(n) \le c g(n)$
$f(n) = \Omega(g(n))$ if there are $n_0$ and $c > 0$ so that for all $n \ge n_0$ it is $f(n) \ge c g(n)$
$f(n) = \Theta(g(n))$ if both $f(n) = O(g(n))$ and $f(n) = \Theta(g(n))$.

Note that the last implies two possibly different $n_0$ values, and different values for $c$ (one each for $O$ and $\Omega$).
Informally, $O(\cdot)$ gives an upper bound, $\Omega(\cdot)$ gives a lower bound, while $\Theta(\cdot)$ gives a sharp bound.
Some examples:
$\begin{align*}
   n
     &= O(2^n) \\
   (3/2)^n
     &= \Omega(n^3) \\
   n^2 (\sin n + \cos n)
     &= \Theta(n^2)
\end{align*}$}
There are functions that don't have a simple expression, like:
$\begin{align*}
   f(n)
     &= \begin{cases}
           n^2 & n \text{ odd} \\
           n^5 & n \text{ even}
        \end{cases}
\end{align*}$
Here clearly $f(n) = \Omega(n^2)$ and $f(n) = O(n^5)$, both best possible among functions $n^\alpha$; there is no simple $g$ so that $f(n) = \Theta(g(n))$.
Lower and upper bounds don't need to be "best possible" in any sense. Often people take great care to get best bounds, but they aren't implied in the notation at all.
To use $\Omega$ to mean best case and $O$ for worst case is misleading at best. For example, the best case for bubblesort on an array of $n$ elements is $\Theta(n)$ (bounded below and above by a linear function in the number of elements; when sorting an already sorted array it does one pass over the data), it's worst case is $\Theta(n^2)$ (if the data are in reverse order). We could say it is $\Omega(n^{1/2})$ and $O(n^3)$ as well, both valid for best and worst cases.


------------------------------------------------------------

ID: cs.stackexchange.com:87305
Title: What is the difference between a tight Big $O$ bound, a tight Big $\Omega$ bound and a Big $\Theta$ bound?
Body: I occasionally see these terms used and I'm not really sure what is meant by all of them. Is it possible for an asymptotic bound that is not Big $\Theta$ bound to be "tight"? What does it mean for bounds that are not Big $\Theta$ to be tight?

Is there a difference between a tight Big $O$ bound and a Big $\Theta$ bound?


[Answer]: $O$ and $\Theta$ refer to different properties of some function. In a sense, we know more of a function when we can describe it with $\Theta$ than with $O$. 

However, usage of $O$ or $\Theta$ doesn't imply tightness of a bound. You have some function $f(n)$ that is a bound which you could either describe with $f(n)\in O(g(n))$ or $f(n)\in \Theta(g(n))$, but whether the bound is tight depends on the actual bound $f$.

So, as a tight bound means simply that there is no 'better' bound for all cases, a tight asymptotic bound (either in $O$ or $\Theta$, or even $\Omega$), simply means that there is no better asymptotic description of a bound.

An example of a tight bound with only $O$ is the time complexity of finding an element in an unordered list of size $n$. This complexity is bounded by $O(n)$ and this is tight. The complexity is not $\Theta(n)$, as there are cases when we find the element without scanning the entire list. (The asymptotic complexity of the worst case is $\Theta(n)$, though)

For more reading on bounds with Landau ('big $O$') notation, see the reference question How does one know which notation of time complexity analysis to use? and its answers.


------------------------------------------------------------

ID: cs.stackexchange.com:91228
Title: What is the relationship/difference between best/worse/expected case and big O/omega/theta?
Body: In the big O section of Cracking the Coding Interview 6th edition, I read the following.

"Best, worse, and expected case describe the big O for expected inputs and scenarios."

"Big O, big omega, big theta describe the upper, lower and tighter bounds for the run-time."

I understand what best, worse,  and the expected case is describing.

I know what bounds are but still don't understand the decoupling of the two concepts. Why would an algorithm have different bounds if the inputs/scenarios are the same?

Thanks in advance!


[Answer]: 
  Why would an algorithm have different bounds if the inputs/scenarios are the same?


A deterministic algorithm has a single running time on each input. However, this is not what best case, worst case and average case are about. Let us consider a deterministic sorting algorithm. Its running time on an array of length $n$ depends on the array (and on the algorithm). 
The best case running time is the minimal running time of the algorithm on an array of length $n$. The worst case running time is the maximal running time of the algorithm on an array of length $n$. The average case running time with respect to a distribution on arrays of length $n$ is the average running time of the algorithm on an array chosen at random from the given distribution.

For a randomized algorithm, it makes sense to consider the best case, worst case and average case for a single input, but this is not what these terms usually refer to.


  "Best, worse, and expected case describe the big O for expected inputs and scenarios."


This is simply wrong. I have explained above the meaning of best case, worst case and average case running time for the particular case of sorting algorithms, and the general case is similar.

How is running time measured? One way to measure running time is via an experiment – choose an actual machine, code the algorithm in a particular way, and measure the running time. However, this approach is problematic – which machine should we run the code on? How should we implement the algorithm? Another problem that the exact formula for the running time might be very complicated, and so not very helpful.

The way out is to state the running time up to a constant factor. This makes sense since under reasonable assumptions, the exact machine used and the exact implementation only affect the running time by a constant factor. Moreover, this allows us to hide the complexities of the exact running time by sweeping them under the rug of asymptotic notation.
(As an aside, we measure the running time on an abstract machine known as the RAM machine, which has unbounded memory. This allows us to make sense of arbitrary input sizes.)

Consequently, we always quote the running time in asymptotic notation. It is common to use big O notation, but this is in fact only an upper bound on the running time. Merge sort, for example, runs in time $O(n^5)$. It also runs in time $O(n\log n)$, and this upper bound is tight, in the sense that the worst case running time is $\Theta(n\log n)$.
When quoting the running time as $O(f(n))$, what one usually means is that the worst case running time is $\Theta(f(n))$. The reason we use big O rather than big $\Theta$ is that on some inputs the algorithm may run faster, and so for arbitrary inputs it may not be correct that the running time is $\Theta(f(n))$, but it is always correct that the running time is upper-bounded by $O(f(n))$.


  "Big O, big omega, big theta describe the upper, lower and tighter bounds for the run-time."


Big O, big $\Omega$ and big $\Theta$ are general-purpose asymptotic notation. They can be used to express asymptotic estimates on the rate of growth of arbitrary functions. Even in computer science, it is common to use asymptotic notation not only to measure running time, but also to measure memory consumption; and in other fields asymptotic notation is used for functions not related to algorithms at all.


------------------------------------------------------------

ID: cs.stackexchange.com:144941
Title: Disambiguating Big-O and Theta for Expressing Time Complexity
Body: Can someone please give me an example of two algorithms, one where "Big-O" is the most appropriate expression of how time complexity grows with input size, and one where this would be Θ?
Can you please be very clear about whether the statements refer to best, average or worst case, and about implicit assumptions about the input or anything else, beyond the fact that the general approach is to consider how the number of basic operations in a hypothetical machine grows with input size.


[Answer]: Lets start by explaining the difference between big-O and $\Theta$. Basically, if we think of big-O as "bounding from above", we can think of $\Theta$ as "bounding both from below and from above". Formally, $f=\Theta(g)\iff f=O(g)\land f=\Omega(g)$. This $f=\Omega(g)$ means that $g$ bounds $f$ from below, and is equivalent to saying that $g=O(f)$ ($g$ bounds $f$ from below if $f$ bounds $g$ from above).
So basically, $f=O(g)$ means that $f$ is asymptomatically at most like $g$, but $f=\Theta(g)$ means that $f$ is asymptomatically equivalent to $g$.
In the context of algorithms, the notions are a bit abused. For example, one may write "this algorithm works in $O(n)$", when he/she really meant "this algorithm runs in $\Theta(n)$".
Most of the times, this doesn't make a big difference. But sometimes, especially with algorithms that are hard to analyze - this distinction is important. Since those algorithms are hard to analyze, its perfectly possible that the proven run-time bounds are not tight, which means they are not $\Theta$ but are just $O$.
On the other hand, when someone explicitly writes $\Theta$, it probably means either one of two things (depending on the context, check it carefully!):

The algorithm's proven bound for the run-time is tight
This algorithm was proven to achieve a run-time equal to the known lower bound pf the problem. For example, with sorting - there is a nice proof that no comparison-based algorithm can sort in less than $n\log(n)$ time - this is a lower bound. Now, mergesort runs in $O(n\log(n))$, and hence it is optimal since it achieves the lower bound. So one may say that mergesort works in $\Theta(n\log(n))$.



[Answer]: Asymptotic notation arose in number theory, and is now commonly used in combinatorics. It has nothing to do with algorithms per se. We use asymptotic notation to describe the resource consumption of algorithms, mainly time and space. It comes in five main flavors:

$f(n) = O(g(n))$ if $f(n) \leq Cg(n)$ for some constant $C>0$. In words, $f(n)$ grows at most as fast as $g(n)$.
$f(n) = \Omega(g(n))$ if $f(n) \geq cg(n)$ for some constant $c>0$. In words, $f(n)$ grows at least as fast as $g(n)$.
$f(n) = \Theta(g(n))$ if $cg(n) \leq f(n) \leq Cg(n)$ for some constants $C,c>0$. In words, $f(n)$ and $g(n)$ grow at the same rate.
$f(n) = o(g(n))$ if $\lim_{n\to\infty} f(n)/g(n) = 0$. In words, $f(n)$ grows slower than $g(n)$.
$f(n) = \omega(g(n))$ if $\lim_{n\to\infty} f(n)/g(n) = \infty$. In words, $f(n)$ grows faster than $g(n)$.


When we say that an algorithm runs in time $O(f(n))$, we mean that its running time is bounded by $Cf(n)$, for some constant $C > 0$. Often there is an unstated assumption that the bound is tight, that is, the function $f(n)$ cannot be replaced by any asymptotically smaller function. However, strictly speaking, this is not the meaning of $O(f(n))$. For example, quicksort runs in time $O(n!)$. This is a pretty bad bound, but the statement is mathematically valid nonetheless.
What do we mean when we say that an algorithm runs in time $\Theta(f(n))$? Strictly speaking, this means that the running time of the algorithm is always between $cf(n)$ and $Cf(n)$, for some constants $C,c>0$. However, sometimes this kind of bound is too much to hope for, since the running time could depend on the particular input, or if the algorithm is randomized, on the random coin tosses. For example, quicksort always runs in $O(n^2)$, but sometimes it runs in $O(n\log n)$. We can say that its worst-case complexity is $\Theta(n^2)$, but it is not true that its runtime is $\Theta(n^2)$.

When should you use which notation? That's up to you. If you can bound the quantity in question tightly, then you might as well use big Theta. If all you have is an upper bound, you should use big O. If you know the worst-case complexity but want to describe the "every-case" complexity, the convention is to use big O, with the tacit understanding that the bound is probably tight for the worst case.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:41968
Title: Maintaining a main project line with satellite projects
Body: Some projects I work on have a main line of features, but are customizable per customer.  Up until now those customizations have been implemented as preferences, but now there are 2 problems with the system...


The settings page is getting out of
control with features.  There are
probably some improvements that
could be made to the settings UI,
but regardless, it is quite
cumbersome setting up new instances
for new customers.
Customers have started asking for
customizations which would be more
easily maintained as separate
threads instead of having tons of
customizations code.


Optimally I am envisioning some kind of source control in which features are either in the main project line and customizations per customer are maintained in a repo per customer set up.  The customizations per project would need to remain separate but if a bug is found and fixed in a particular project, I would need to percolate the fix back to the main line and into all of the other customer repos.  

The problem is I have never seen this done before, and before spending time trying to find source control that can accommodate this scenario and implement it, I figure it best to ask if anyone has something less complicated or knows of a source control product which can handle this with very little hair pulling.


[Answer]: This is definitely possible with branching. Imagine your main productline as a river. You would make branches for the individual customers these would be tributaries off the river. Primary product development would continue on the main product line and when features are ready for the other branches, you would merge them up (essentially the river flows back into the tributaries again). Sometimes, a problem is identified and fixed on a branch, if the fix is useful for the main product line it's merged back down to main and from there can be merged to the other branches.

If you're using Visual Studio and TFS there is a document that discusses various branching strategies including the one I just mentioned.

If you have a choice of version control (i.e. you haven't committed to one yet) I would go with one of the DVCS's (Distributed Version Control Systems) like Git, Mercurial, or Bazaar. Branching and merging is so much simpler with them.


[Answer]: Since you are already talking about making separate builds for each customer why not implement each feature under its own compile switch.  Then when you build have a separate makefile for each customer with the appropriate #defines included.  This would allow you the following:


There is only one place to fix bugs, this alone is worth a lot since having multiple places to check a fix is a ticking time bomb.  No matter how hard you try there will be some change that doesn't make it into all branches, if only by accident.
If another customer requires a feature that is already implemented you can just add the define to the makefile.



[Answer]: We have a couple of products with something similar.

One system is set up by having libraries with the base code and other support and then relatively small (code-wise) customer specific apps are built using those libraries.  That's for code based changes.

Another product has a single codebase and generates configuration packages for each system to be installed.  The code is installed, then the configuration package for that system is installed.  The configuration packages are built from files within the same codebase as the executable deliverable.


------------------------------------------------------------

ID: cs.stackexchange.com:91228
Title: What is the relationship/difference between best/worse/expected case and big O/omega/theta?
Body: In the big O section of Cracking the Coding Interview 6th edition, I read the following.

"Best, worse, and expected case describe the big O for expected inputs and scenarios."

"Big O, big omega, big theta describe the upper, lower and tighter bounds for the run-time."

I understand what best, worse,  and the expected case is describing.

I know what bounds are but still don't understand the decoupling of the two concepts. Why would an algorithm have different bounds if the inputs/scenarios are the same?

Thanks in advance!


[Answer]: 
  Why would an algorithm have different bounds if the inputs/scenarios are the same?


A deterministic algorithm has a single running time on each input. However, this is not what best case, worst case and average case are about. Let us consider a deterministic sorting algorithm. Its running time on an array of length $n$ depends on the array (and on the algorithm). 
The best case running time is the minimal running time of the algorithm on an array of length $n$. The worst case running time is the maximal running time of the algorithm on an array of length $n$. The average case running time with respect to a distribution on arrays of length $n$ is the average running time of the algorithm on an array chosen at random from the given distribution.

For a randomized algorithm, it makes sense to consider the best case, worst case and average case for a single input, but this is not what these terms usually refer to.


  "Best, worse, and expected case describe the big O for expected inputs and scenarios."


This is simply wrong. I have explained above the meaning of best case, worst case and average case running time for the particular case of sorting algorithms, and the general case is similar.

How is running time measured? One way to measure running time is via an experiment – choose an actual machine, code the algorithm in a particular way, and measure the running time. However, this approach is problematic – which machine should we run the code on? How should we implement the algorithm? Another problem that the exact formula for the running time might be very complicated, and so not very helpful.

The way out is to state the running time up to a constant factor. This makes sense since under reasonable assumptions, the exact machine used and the exact implementation only affect the running time by a constant factor. Moreover, this allows us to hide the complexities of the exact running time by sweeping them under the rug of asymptotic notation.
(As an aside, we measure the running time on an abstract machine known as the RAM machine, which has unbounded memory. This allows us to make sense of arbitrary input sizes.)

Consequently, we always quote the running time in asymptotic notation. It is common to use big O notation, but this is in fact only an upper bound on the running time. Merge sort, for example, runs in time $O(n^5)$. It also runs in time $O(n\log n)$, and this upper bound is tight, in the sense that the worst case running time is $\Theta(n\log n)$.
When quoting the running time as $O(f(n))$, what one usually means is that the worst case running time is $\Theta(f(n))$. The reason we use big O rather than big $\Theta$ is that on some inputs the algorithm may run faster, and so for arbitrary inputs it may not be correct that the running time is $\Theta(f(n))$, but it is always correct that the running time is upper-bounded by $O(f(n))$.


  "Big O, big omega, big theta describe the upper, lower and tighter bounds for the run-time."


Big O, big $\Omega$ and big $\Theta$ are general-purpose asymptotic notation. They can be used to express asymptotic estimates on the rate of growth of arbitrary functions. Even in computer science, it is common to use asymptotic notation not only to measure running time, but also to measure memory consumption; and in other fields asymptotic notation is used for functions not related to algorithms at all.


------------------------------------------------------------

ID: cs.stackexchange.com:86148
Title: Can a 3 Color DFS be used to identify cycles (not just detect them)?
Body: Can a 3 color DFS be used to identify all cycles in a directed graph not just detect them? 

In other words if I have a directed graph with multiple cycles, can I run a function on them such that the function returns the list of nodes that compose each simple directed cycle in the graph and not just a boolean true or false? Most answers online show pseudocode for detection and not identification (ie functions which return boolean values instead of node lists). You can assume that the graphs I'm referring to are mostly tree like in structure and aren't deeper than 5 nodes or so.

So for example, for this graph:


The list would be [[a,b,e], [f,g], [c,d], [d,h]]

With white, grey, black DFS a cycle is found when a node already colored grey is visited a second time through a different edge. What I'm struggling to wrap my head around is when this happens, how do we back track to identify all of the nodes involved in this detected cycle without increasing complexity or running DFS a second time. In the example below, if we do a DFS (exploring right edges first) starting at A, the stack will look like this [A,B,C,E,D,B] and the cycle will be detected when B is visited a second time. Given this stack how do we deduce that C D and B only are part of the cycle and not E or A?  



I am aware that there are plenty of algorithms other than DFS that can do this (such as Johnson's algorithm or Tarjan's with a twist) I just want something simple to implement.


[Answer]: The answer to this question is YES. A three color DFS algorithm can be used to both detect and identify the nodes associated with all simple cycles in a directed graph with more or less the same time complexity. 

The key is to keep a stack of parent and child pairs as you recursively explore the graph using white, grey, black (3 color) depth first search. You must also remember to pop pairs off the stack after you are done exploring a child (ie when the child's color is set to black). This will give you an easy way to trace what nodes are traversed going to and from a "starting node" in a cycle.

I will add more detail and an example implementation later!


[Answer]: The moment you find a grey node, you have the edge that closes the cycle in hand. All other edges are part of the DFS tree. That is, store the "cycle-closing" edge and obtain the rest afterwards.

That is, assuming that those are the cycles you are interested in. Other cycles (such that contain more than one non-DFS-tree edge) are more complicated, but I don't think those are easy to cover. A directed graph may have super-exponentially many cycles, so you can't expect to list them all with a simple, linear-time traversal.


[Answer]: Here is an algorithm which takes your stack content $S$ when loop was detected & return array of the nodes which are forming cycle. From your example it is $ [A,B,C,E,D,B]$

 Algorithm_Get_Loop_Nodes(S):
     cycle_nodes = []
     repeated_node = S[top]
     while S[top] != repeated_node:
         prev_node = S.pop()
         curr_node = S[top]
         if curr_node is parent of prev_node:
            cycle_nodes.add(curr_node)
     return cycle_nodes



------------------------------------------------------------

ID: cs.stackexchange.com:54283
Title: Sorting algorithm that moves element to a 2-dimensional array
Body: I had an idea for a sorting algorithm wich is very fast, but can (potentially) use a lot of memory. I'm not a Computer Science student/graduate, only a self-taught programmer so I don't know how to evaluate it's viability. Also, I would like to know if it has already been documented and under what name.

Algorithm:


Get an unsorted array
Iterate through it, and find the highest and lowest value stored in it
Determine "range" (highest - lowest)
Make a 2 dimensional array presorted,
in which the first dimension's size is "range" + 1
For each element in array "unsorted"


Add the current element into presorted[current_element_value - lowest]

Make array "sorted", and add each element of presorted's second dimension, ignoring the empty first dimensions.
return the sorted array


An example using said algorithm would be the following:

unsorted[] = {5, 8, 2, 4, 6, 8, 2, 0, 4, 5, 6, 3, 3, 2, 1}

After iterating through it once, we get:
lowest: 0
highest: 8

range = highest - lowest = 8

Make 2 dimensional array presorted,
 consisting of range+1 arrays:

presorted[0] = {}
presorted[1] = {}
presorted[2] = {}
presorted[3] = {}
presorted[4] = {}
presorted[5] = {}
presorted[6] = {}
presorted[7] = {}
presorted[8] = {}

Next we add the elements, first element of unsorted is 5.
 5 - lowest (0) is 5

presorted[0] = {}
presorted[1] = {}
presorted[2] = {}
presorted[3] = {}
presorted[4] = {}
presorted[5] = {5}
presorted[6] = {}
presorted[7] = {}
presorted[8] = {}

After doing this which each element:

presorted[0] = {0}
presorted[1] = {1}
presorted[2] = {2, 2, 2}
presorted[3] = {3, 3}
presorted[4] = {4, 4}
presorted[5] = {5, 5}
presorted[6] = {6, 6}
presorted[7] = {}
presorted[8] = {8, 8}

Now, we simply create an empty array called "sorted",
 to which we add all the elements
 of non-empty arrays in presorted:

sorted = {0, 1, 2, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 8, 8}


This is the C++ source code for it

std::vector Sort(std::vector &unsorted)
{
   int min = unsorted[0];
   int max = unsorted[0];

   int range;

   std::vector sorted;

   for (int i = 0; i  max)
         max = unsorted[i];
      if (unsorted[i] > presorted(range+1);

   for (int i = 0; i 

Is this algorithm viable?

I think it could be used in certain scenarios (when range is not too big) and it has the advantage that you can determine if it's appropriate to use it with simply iterating once through the unsorted list.

I'll do some tests and report back.


[Answer]: What you have come up with is a sligthly less memory efficient version of counting sort. 

This algorithm works in $\mathcal{O}(n + range)$ time where $n$ is the number of elements in the vector $unsorted$ and $max$ and $min$ are as defined in your program.

Your implementation uses $\mathcal{O}(n + range)$ memory. You can make it more memory efficient by just keeping track of how many times the same number in the vector 'unsorted' occurs rather than maintaining a vector with repeated instances of the same element i.e.

vector presorted(range+1);
for(int i = 0; i 

It will now use only $\mathcal{O}(range)$ memory.

Yes you are correct, this method might potentially use up a lot of memory if the difference between the smallest and largest elements of the vector is too great.

For those who are not fluent in C++:


OP is finding the smallest and largest elements of the unsorted array $max$ and $min$ respectively. 
He is initializing an array of size $max - min$ and creating an empty linked list in each position of the array (in this code it is the vector of vector of integers $presorted$). 
Then each element of $unsorted$ is added as a new node in the linked list at $presorted[unsorted[i] - min]$. In a random access computational model this operation will take $\mathcal{O}(1)$ time and hence each element of $unsorted$ is added to its appropriate bucket in constant time.
Then he is just iterating in the array of linked lists and compressing it to the final array $sorted$ which is the final output.



------------------------------------------------------------

ID: cs.stackexchange.com:120517
Title: Sorting almost sorted array
Body: Encountered this question but I couldn't solve with the complexity they solved it:

Suppose I have an array that the first and last $\sqrt[\leftroot{-2}\uproot{2}]{n} $ elements has $\frac{n}{5}$ inverted pairs, and the middle $n - 2\sqrt[\leftroot{-2}\uproot{2}]{n}$ elemnts are sorted. What is the complexity of sorting the unsorted array?

They claim in the answer that sorting array with $I$ inversions is $O(n\log{\frac{n}{I}})$.Why?


[Answer]: If you have an array with a sequential range that covers all but k elements, and you know the range, then you sort the k elements in O (k log k), then merge two ranges that you know to be sorted in O (n). That sorts the whole array in O(n) as long as k is O (n / log  n). 

In your case k = $O (n^{1/2})$ so clearly you can sort the array in O (n). 

And you don't need to know the sorted range, because for large n we have n/2 inside the sorted range, so you can just count the elements in ascending / descending order from n/2 upwards / downwards. 


------------------------------------------------------------

ID: cs.stackexchange.com:930
Title: Adding elements to a sorted array
Body: What would be the fastest way of doing this (from an algorithmic perspective, as well as a practical matter)?

I was thinking something along the following lines.

I could add to the end of an array and then use bubblesort as it has a best case (totally sorted array at start) that is close to this, and has linear running time (in the best case).

On the other hand, if I know that I start out with a sorted array, I can use a binary search to find out the insertion point for a given element.

My hunch is that the second way is nearly optimal, but curious to see what is out there.

How can this best be done?


[Answer]: We count the number of array element reads and writes. To do bubble sort, you need $1 + 4n$ accesses (the initial write to the end, then, in the worst case, two reads and two writes to do $n$ swaps). To do the binary search, we need $2\log n + 2n + 1$ ($2\log n$ for binary search, then, in the worst case, $2n$ to shift the array elements to the right, then 1 to write the array element to its proper position).

So both methods have the same complexity for array implementations, but the binary search method requires fewer array accesses in the long run... asymptotically, half as many. There are other factors at play, naturally.

Actually, you could use better implementations and only count actual array accesses (not accesses to the element to be inserted). You could do $2n + 1$ for bubble sort, and $\log n + 2n + 1$ for binary search... so if register/cache access is cheap and array access is expensive, searching from the end and shifting along the way (smarter bubble sort for insertion) could be better, though not asymptotically so. 

A better solution might involve using a different data structure. Arrays give you O(1) accesses (random access), but insertions and deletions might cost. A hash table could have O(1) insertions & deletions, accesses would cost. Other options include BSTs and heaps, etc. It could be worth considering your application's usage needs for insertion, deletion and access, and choose a more specialized structure.

Note also that if you want to add $m$ elements to a sorted array of $n$ elements, a good idea might be to efficiently sort the $m$ items, then merge the two arrays; also, sorted arrays can be built efficiently using e.g. heaps (heap sort).


[Answer]: Because you are using an array, it costs $O(n)$ to insert an item - when you add something to the middle of an array, for example, you have to shift all of the elements after it by one so that the array remains sorted.

The fastest way to find out where to put the item is like you mentioned, a binary search, which is $O(\lg n)$, so the total complexity is going to be $O(n + \lg n)$, which is on the order of $O(n)$.

That being said, if I felt particularly snarky, I could argue that you can "add to a sorted array" in $O(1)$, simply by slapping it to the end of the array, since the description doesn't indicate that the array has to remain sorted after inserting the new element. 

Anyway, I don't see any reason to pull bubble sort out for this problem.


[Answer]: If you have any reason for not using heap, consider using Insertion Sort instead of Bubble Sort. It's better when you have a few unsorted elements.


[Answer]: Patrick87 explained this all very well. But one additional optimization you could make would be to use something like a circular buffer: you can move items right of the position of the inserted element to the right, as usual. But you can also move items to the left of the correct position to the left. To do this, you need to treat the array as circular, i.e. the last item is right before the first one and it also requires you to keep the index where the items currently start.

If you do this, it could mean you make about half as many array accesses (assuming uniform distribution of indexes you insert to). In the case of doing binary search to find the position, it's trivial to choose whether to shift to the left or to the right. In the case of bubble sort, you need to “guess” correctly before starting. But doing that is simple: just compare the inserted item with the median of the array, which can be done in single array access.


[Answer]: I have used the Insertion sort algorithm effectively for this issue. At one time we had a performance issue with a hash table object, I wrote a new object that used binary search instead that increased performance significantly. To keep the list sorted it would keep track of the number of items added since the last sort (i.e. number of unsorted items,) when the list needed to be sorted due to a search request, it performed an insertion sort or a quick sort depending on the percentage of items unsorted. Use of the insertion sort was key in improving the performance.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:355170
Title: What is wrong with this algorithmic solution of mine that checks whether a given function returns a sorted array when an array is given as input?
Body: An interviewer asked me this question:


  Given a function f(ar[]) where ar[] is an array of integers, this
  functions claims to sort ar[] and returns it's sorted version ars[].
  Determine if this function works correctly.


I approached this question as:


  
  First check if the returned array ars[] is actually sorted in either non increasing or non decreasing order. This one is easy to check, ars[] should
  either follow the sequence ar[i + 1] >= ar[i] (for an array sorted in
  non decreasing order) or ar[i + 1] 
  Then check if sizes of both the input array ar[] as well as the output array ars[] are same.
  Finally check if every element of ar[] is also present in ars[]. Since we have already examined at step 1 that ars[] is sorted and at
  step 2 that sizes of ar[] and ars[] are same we can use Binary
  Search algorithm to perform this action. The worst case time
  complexity for this should be O(n * log(n)).
  
  
  If all the above 3 checks succeeds then the function is working fine
  else it is not.  The overall time complexity of this algorithm should O(n * log(n))


But to my surprise the interviewer said that this solution is not correct and it's time complexity can be improved. I can not understand what actually is wrong with my solution, did I miss any corner case or the entire approach is wrong? Also what can be better approach to this(in terms of time complexity)?

PS: The interviewer mentioned no additional information or any additional constraint for this problem.   


[Answer]: While you say the interviewer provided no other information, was it possible to ask questions? As there is only one argument to the function, I would be under the assumption that the function would always return the array sorted it one directory, most likely ascending. i.e.

fn([4,1,5]) -> [1,4,5]


But obviously that's something the interviewer should confirm for you as it affects the answer. (Also it would be very strange for the function to return asc OR desc, without any way to specify).

As such, my layman's approach would be to pass an unsorted array to the function, and then compare that against a second array I already knew was sorted -

expected = [1,4,5]
sorted = fn([4,1,5])
// compare expected and sorted...


As far as basic testing goes, the process of comparing every element against each other, and also comparing every single element to the source array does seem convoluted - compared to just comparing against a known correct list.


[Answer]: Your method is only testing that the original and sorted arrays contain the same values, not that they contain the same number of each value; e.g. 1112 would pass for 1221

In step 3 you could, for example, mark that a particular value in the sorted array has already been matched (removing from the array would be too time consuming) but then the search would no longer be a binary search as you would hit already used values. 

[Obviously this isn't a problem if the values are unique but that isn't stated]


[Answer]: For speed improvements you can test with known arrays with either presorted answers to check against or simply run through the returned array ra checking ra[n] = for all n in the range 0..len(ra-1) complexity of which is very low. You can determine if every element is present in each array by counting the instances of each value and then comparing the counts.

Your testing should also include corner cases such as ar=[1] and ar=[] and for an interview I would at least mention testing for invalid inputs such as arrays of non-integer values, non-arrays, etc., I know that the behaviour in such cases is undefined in the case you were given but a part of a testers job is to highlight specification omissions and ambiguities such as what is the error handling. If you only test for what is specifically in the specification you will not make a good tester and this sort of issue is one of the things that the interviewer will be looking for.


[Answer]: As already mentioned by @Dipstick, step 3 can fail if there are duplicates in the input array. To resolve this and improve the time complexity, one can use a dictionary with the array elements as keys and their number of occurrence as values. Such a dictionary can be created from the sorted and the unsorted array as well in O(n), and you need to test if the resulting dictionaries are identical, which can be done in O(n), too. 

One can combine this using only one dictionary counting the total number of occurrences in the unsorted array minus the number of occurrences in the sorted array. In pseudo code (assuming a default of 0 for values in the dictionary when the key is used the first time): 

 for(e in ar)
     noOfOccurence[e]+=1; 
 for(e in ars)
     noOfOccurence[e]-=1;

 for(e in noOfOccurence.Keys)
     if(noOfOccurence[e] != 0)
         return false;

 return true;



[Answer]:   > But to my surprise the interviewer said that this solution is not correct


If i were a job interviewer i would expect to get the answer: write a simple unittest for the sort method
with a sample unsorted input and the expected output (see @USD Matt answer)

  > time complexity can be improved.


Your solution looks OK on the first glance and more complete/complicated than the simple example.

If i were searching for an experienced developer i would expect that the applicant knows unittests and does not more than neccessary.

I assume that the complex/complete automated testing is overkill and not necessary for testdriven development. A simple example should be enough.


[Answer]: If you keep track of the min and max a array of size max - min + 1 will work if the range is not large. 

Subtract min from each value so it starts at 0 

for(e in input)
     ar[e - min] +=1; 
for(e in sorter)
     ar[e - min] -=1;

for(e in ar)
     if(e != 0)
         return false;

return true;


You could check for duplicates as you test for sort
Yes binary would be O(n logn) but is might be faster than dictionary  


[Answer]: Take a step back and look at this from a simplistic standpoint. You are making your first step be the most time and compute intensive test. I'd start with a series of sanity checks which catch the most glaring issues and avoid having to (re)sort the array and compare element-by-element. 

Put your test to ensure the number of items in the result equals the number of items fed in because if the number of items don't match you can fail the test and not go any further. 

Next test that the first element is less than the last element. 

If that passes, spot check some elements in between, for example the first and last, and the items at the 1/4th mark, 1/2 mark, and 3/4th mark - and make sure first 

Assuming those tests pass, then I'd dig into iterating all the elements to test the integrity of the sort. But I wouldn't resort the input to compare, but simply iterate the results in order, comparing this item with the previous one and making sure this item is bigger than the previous one. Bail out with a failed test as soon as you hit one that is smaller than its predecessor. 

The runtime of the test will be the same as the time it takes to sort the list but only if the sort is good. If the sort is not good, the runtime will be some degree shorter than it would've taken to re-sort and test. How much smaller depends on if the failed value is on the front end of the data or the back end of the data group.

Typically, there is something about the data being sorted, and especially where that data is coming from which can point you to the cases where sorting errors are most likely to crop up. If you have the luxury of having insight into these things, then craft sanity checks to catch those cases and put that before the visitation of every item to verify sorting. 


------------------------------------------------------------

ID: cs.stackexchange.com:113343
Title: Why is heap insert O(logN) instead O(n) when you use an array?
Body: I am studying about the arrays vs heap for make a priority queue

For check the heap implementation I am reviewing this code:  Heap

, but I have the following question.

Heap is based on array, and for creating an array you need O(n), for inserting into a heap you need O(logN), so if you have a unordered list of task and you want to create a heap, you need O(NLogN).

If you use an array, you need O(n), to create the array, and O(NlogN) to sort the array, so you need O(NLogN).

So if you need implement some similar to this:

function priorityQueue(listOfTask l)


There isn't a diference betwen use an Array or an Heap right? So, why I should use a heap instead an array for solve this function?

Thanks


[Answer]: Keeping a heap is more efficient than keeping a sorted array, when you need to keep adding items to the priority queue. In case you don't need to add to it, you don't need a queue in the first place, just an array sorted by priority.

Insertion to heap-based priority queue is O(logN), while insertion to sorted array is O(N) (binary search for position is O(logN), but inserting there is O(N) ).

As you can see here, almost all data structures are about trade-offs. Converting an array to heap (and keeping it heap) you gain something (O(logN) priority queue action) but also lose something (ability to iterate contents in order, binary search). In contrast, if you used an unsorted array, you'd gain O(1) insertion (because you can just append), but almost everything else would be O(N), which is excellent if you have handful of items, but becomes bad if you have dozens, impossible if you have thousands.


[Answer]: Given your link, you seem to be interested in data structures supporting the following operations:


Create(m): create a new instance with room for m elements.
Size(): return the number of elements currently stored in the instance.
Insert(k): insert an element with priority k.
ExtractMax(): return the maximal priority currently stored, and remove it.


Since Size() is easy to implement in constant time as part of the other operations, I will not discuss it below.

Here are three sample implementations:

Unsorted array:


Create() simply allocates memory.
Insert() simply inserts the element at position i+1, where i is the current number of elements.
ExtractMax() goes over the entire array, finding the maximum; and then "compacts" the array by moving all elements to the right of the maximum one entry to the left.


Sorted array: (your implementation)


Create() simply allocates memory.
Insert() first scans the array to find the proper location for the element, then moves all elements to the right of the intended location one entry to the right, and finally stores the element.
ExtractMax() returns and removes the ith element, where i is the number of elements currently in the instance.


Binary heap:


Create() simply allocates memory.
Insert() and ExtractMax() are described on Wikipedia.


Binary heaps are implemented using "partially sorted" arrays. That is, the order of elements in the array is not arbitrary, but it is also not completely determined. Rather, we are only guaranteed that A[i] ≥ A[2i],A[2i+1]. This freedom allows us to trade-off the running time of the two operations Insert() and ExtractMax().

In all of these implementations, Create() is roughly the same, so to compare the various implementations it suffices to consider the two operations Insert() and ExtractMax():

$$
\begin{array}{c|c|c}
\text{Implementation} & \text{Insert()} & \text{ExtractMax()} \\\hline
\text{Unsorted array} & O(1) & O(n) \\
\text{Sorted array} & O(n) & O(1) \\
\text{Binary heap} & O(\log n) & O(\log n)
\end{array}
$$

Here $n$ is the number of elements currently in the array.

If you perform many Insert() and ExtractMax() operations, a binary heap is likely to be more efficient.

Another operation which you mentioned is


Initialize(A): add to an empty instance the elements in A


You can add support to this operation to all different implementations mentioned above:


Unsorted array: simply copy A to the array. Running time: $O(|A|)$.
Sorted array: copy A and sort the resulting array. Running time: $O(|A|\log|A|)$.
Binary heap: run Insert() for each element of A. Running time: $O(|A|\log|A|)$.


Considering this operation doesn't strengthen the case of your implementation.


[Answer]: A priority queue does something entirely different than sorting an array. 

The important operations for a priority queue are: 1. Add an item to the queue. 2. Tell us the smallest item in the queue and remove it from the queue. Both these operations run in O(log n).

Now use a sorted array. Operation 2 is fast if we sorted in descending order. But operation 1 isn’t: Adding a random value to a sorted array and keeping the array sorted requires moving n/2 array elements on average. 

You could sort an array in O(n log n) by adding all items to a priority queue, then removing the smallest item n times. Fast sorting algorithms process the array as a whole, they don’t try to keep part of the array sorted (straight insertion or binary insertion do; they ar not fast). Quicksort moves items very roughly into the right place at each step. Note that a priority queue only performs a rough ordering of the items as well. That’s what makes both Quicksort and a priority queue fast: They don’t ask for the data to be sorted at all times. 


------------------------------------------------------------

ID: cs.stackexchange.com:63255
Title: Output cycle found by DFS
Body: We can use DFS to find a cycle in a given graph. The idea is that a cycle exists if we can find back edge in the graph. First I just want to detect if a cycle exists, if so return true else false. Here is what I have got so far:

DFS(G,s)
  for all v in V do
    color[v] 

G is the given graph and s is the starting node.

DFS-Visit(u)
  color[u] 

Ajd[u] is the list of u's neighbours. So by processing the children of u we might discover a node that is marked gray. If so u and v are connected with a back edge and we found a cycle.

Now I want to extend this algorithm to output the found cycle. Any ideas on how to do that? I thought about using a stack and simply push all u's in DFS-Visit onto the stack. When true is returned I can output the cycle by taking the nodes from the stack. What do you think?



Using a stack won't solve my problem, because the stack will contain nodes that are not part of the cycle.



I have another idea. When we found a cycle we know v=u. To print the cycle we have to go back using the parent list until we reach u:

DFS-Visit(u)

  color[u] 


[Answer]: 
  When true is returned I can output the cycle by taking the nodes from the stack.


The stack contains the nodes you still have to visit, so they can not be part of the cycle.

There are at least two options to actually obtain data from a DFS.


Create the DFS tree explicitly; this gives you full information.
If you only want one cycle, you can store a pointer to the visited node's parent (the tree edge) instead of labelling it gray.



------------------------------------------------------------

ID: cs.stackexchange.com:96918
Title: Detect non existence of a cycle in a graph using Datalog : SMTLIB Format for Z3
Body: I want to detect the non existence of a cycle in a graph using Datalog (which is  a declarative logic programming language).

The proposed solution was:

(set-option :fixedpoint.engine datalog) 
(define-sort s () Int) 

(declare-rel edge (s s)) 
(declare-rel path (s s)) 

(declare-var a s) 
(declare-var b s) 
(declare-var c s) 

(rule (=> (edge a b) (path a b)) P-1)
(rule (=> (and (path a b) (path b c)) (path a c)) P-2)

(rule (edge 1 2) E-1)
(rule (edge 2 3) E-2)
(rule (edge 3 1) E-3)

(declare-rel cycle (s))
(rule (=> (path a a) (cycle a)))
(query cycle :print-answer true)


But my question is how can i get SAT when there is no cycle in the graph and UNSAT when there exists a cycle.

One suggestion (but that does not meet my need) is:

(set-option :fixedpoint.engine datalog) 
(define-sort s () Int) 

(declare-rel edge (s s)) 
(declare-rel path (s s)) 

(declare-var a s) 
(declare-var b s) 
(declare-var c s) 

(rule (=> (edge a b) (path a b)))
(rule (=> (and (path a b) (path b c)) (path a c)))

(rule (edge 1 2) r-1)
(rule (edge 2 3) r-2)
(rule (edge 3 1) r-3)


(assert (not (path a a)))

(check-sat)
(get-model)


which returns as result:

> z3 test.txt
sat
(model
  (define-fun a () Int
    0)
  (define-fun path ((x!0 Int) (x!1 Int)) Bool
    (ite (and (= x!0 0) (= x!1 0)) false
      false))
)


I don't understand why z3 assign 0 to the variables while I only have 1, 2 and 3 as vertices?

Another suggestion was :

(set-option :fixedpoint.engine datalog) 
(define-sort s () Int) 

(declare-rel edge (s s)) 
(declare-rel path (s s)) 

(declare-var a s) 
(declare-var b s) 
(declare-var c s) 

(rule (=> (edge a b) (path a b)))
(rule (=> (and (path a b) (path b c)) (path a c)))

(rule (edge 1 2) r-1)
(rule (edge 2 3) r-2)
(rule (edge 3 1) r-3)


(assert
         (=> (path a a)
            false
            )  

 )


(check-sat)
(get-model)


Which returns the result:

> z3 test2.txt
sat
(model
  (define-fun a () Int
    0)
  (define-fun path ((x!0 Int) (x!1 Int)) Bool
    (ite (and (= x!0 0) (= x!1 0)) false
      false))
)


Any idea to resolve this problem?

Note: I need to use Datalog for reasons of complexity.


[Answer]: I don't think you can do it in plain Datalog, but you can do it in Datalog plus negation.

I suggest you define a relation $R$, so that $R(x,y)$ is true if vertex $y$ is reachable from vertex $x$ in one or more steps.  You can define $R$ recursively, namely,

$$R(x,y) = E(x,y) \lor \exists w . E(x,w) \land R(w,y),$$

where $E$ is a relation that represents the edges in the graph ($E(x,y)$ is true iff $(x,y)$ is an edge in the graph).  Now the graph has a cycle iff $\exists x . R(x,x)$ is true.  I think you should be able to translate this in Datalog.

From this you can get a Datalog instance that is satisfiable if there is a cycle in the graph, or unsatisfiable if there is no cycle.

If you want the reverse (an instance that is satisfiable iff there is no cycle), I think you need negation.

The details of how to express that in Z3 code seem off-topic here.


------------------------------------------------------------

ID: cstheory.stackexchange.com:49017
Title: Detect if a graph has a $k$ cycle in space complexity $O((\log k)^d)$ for fixed $d \geq1$
Body: For a graph $G$, I want to test if it contains a cycle of length $k$, for some $k$ much smaller than $|G|$. I am interested in particular in an algorithm with low space complexity. The cycle need not be induced, so that I want to return success also for instance in the case that G contains a $k$-clique.
I considered first a well known much simpler case, which is testing if $G$ is in fact a $k$ cycle. This can be done using $O(\log k)$ variables: Assume for simplicity for the moment that $k=4m$. Such an algorithm then boils down to the fact that we can test if two vertices have distance $t$ in space complexity $O(\log t)$. Then we test to see if there are four vertices $x_1,x_2,x_3,x_4$ such that $d(x_1,x_3)=d(x_2,x_4)=2m$ and all other distances between the $x_i$ are $m$.
Note crucially that this algorithm will fail to report true if the graph $G$ has $k$ vertices and contains a cycle, but also some additional edges.
My question now is the following: Is there a similar (or completely different) algorithm that detects when a graph $G$ of potentially large size, contains a $k$ cycle that is not necessarily induced, that has $O((\log k)^d)$ space complexity for some fixed $d \geq  1$? Clearly such an algorithm with space complexity $ k $ exists, but I would like to do better.
P.S. I don't know if it makes much of a difference, but in the applications I am looking at, $k$ will itself be of the order $(\log n)^2$ where $n = |G|$.
P.P.S Also, the graph $G$ that I am looking at will be sparse, they are Erdos-Renyi random graphs in $\mathbb{G}(n,n^{-c})$ for some $c\in (0,1)$.


[Answer]: No, this is impossible for your parameters.  With $s$ bits of space, you can only visit at most $2^s$ vertices of the graph.  Now set $s = O((\log k)^d)$ and $k=(\log n)^2$ and it is clear that you cannot visit all of the graph, as $2^s = o(n)$.  Thus for any algorithm that uses only $O((\log k)^d)$ bits of space, there exist graphs where you fail to detect a $k$-cycle (e.g., because the $k$-cycle is in the unvisited part of the graph).

If $k=n^{\Theta(1)}$, and if your definition of a cycle allows repeated vertices to appear in the cycle, then an algorithm exists.  A simple approach is to guess a starting vertex $v_0$ and store it (which requires $\log n = O(\log k)$ bits); then, non-deterministically guess a path of length $k$, counting up to $k$ as you go (which requires $\log k$ bits), checking that none of the intermediate vertices are equal to $v_0$ and the final vertex is equal to $v_0$.
If $k=n^{\Theta(1)}$, and if your definition of a cycle does not allow repeated vertices to appear in the cycle, then I don't know whether an algorithm exists or not.


------------------------------------------------------------

ID: cs.stackexchange.com:75952
Title: detecting a cycle in an undirected graph problem is in $RL$ complexity class
Body: I need come up with an algorithm for detecting a cycle in an undirected graph where the algorithm is in $RL$. That is, the algorithm detects a cycle with a probability greater-equal to $\frac{1}{2}$ but it doesn't return true for a graph with no cycle (no false-positive).

In class, we've learned that $USTCONN$ (the problem of telling if there's a path from $s$ to $t$ in an unidrected graph) is in $RL$.

I thought about utilizing it for the current problem:  


We choose $s,t$ in random s.t $t$ has at least $2$ edges and check if there's a path between them.
Then, in the same fashion, we check if there's a path between $s$ and $t$'s neighbors (If there are too many we may limit it to some constant, randomly, since we must use only logarithmic space) 
If we found two different paths from $s$ to $u_1,u_2$ where the latter are $t$'s neighbors then there must be two different paths from $s$ to $t$ - Which is a cycle.
We repeat the above sufficiently many times (TBD)


I would like to know if that is the algorithm expected from me to come up with before I put efforts in proving that this algorithm is indeed in $RL$.

Thanks!


[Answer]: In fact USTCONN is in L (as shown by Reingold), and you can reduce your problem to USTCONN using non-backtracking walks. These are walks which are not allowed to backtrack, that is, after going from $x$ to $y$ we can't go immediately back to $x$ (but we could go back to $x$ via a different route). The graph contains a cycle iff there is a non-backtracking walk from some vertex to itself. You can formulate this problem as aan undirected connectivity problem, and solve it using the logspace algorithm for USTCONN. As a result, you can detect whether the graph contains a cycle in logspace.


------------------------------------------------------------

ID: cs.stackexchange.com:75341
Title: Proving $\#CYCLE \in \#P$
Body: I'm trying to find a way to count distinct simple cycles in a graph in order to prove that $\#CYCLE \in \#P$, if I could represent a distinct cycle, then I'll have a witness.

I saw this question: 
Number of cycles in a graph? they mention the reduction from HAM #CYCLE in Counting Complexity by Arora and Barak, but I don't understand how does that help.

You have some directed graph, no one said it has a hamilton cycle, and the gadget doesn't return how many cycles there in the graph or a way to distinguish between them, it just makes all the cycles longer.

So instead, is it possible to use a topological sort to count cycles? since it can detect every cycle, we can use it to distinguish between them.


[Answer]: The class #P consists of those functions $f$ such that there is a polytime relation $R$ and a polynomial bound $p$ satisfying
$$
 f(x) = |\{ |y| 


------------------------------------------------------------

ID: cs.stackexchange.com:18342
Title: single algorithm to work on both directed and undirected graph to detect cycles?
Body: I have been trying to implement an algorithm to detect cycles (probably how many of them) in a directed and undirected graph. That is the code should apply for both directed and undirected graphs.

Using DFS or topological sort is mostly recommended in various posts. But largely, everything is addressed for undirected graph. 

This link describes one approach for cycle detection. To my understanding this works for directed graphs.

This link has the code for cycle detection in undirected graphs. but I fail to understand how it ignores the back edge. That is it must ignore any cycles with two nodes, say D to C and C to D.
which means it must remember it parent as the DFS recurses. But the code does not seem take care of that.

Any suggestions welcome..




[Answer]: Take a look at the answers for finding all cycles in graph.

You can transform undirected graph into a directed graph quite easily: for every edge (u,v) in your original graph G, put two edges (u --> v) and (v --> u) into your new directed graph T. Apply any of the algorithms that find cycles in a directed graph on T. However, notice that any undirected edge transforms into a directed cycle (u - v - u), so if your original graph was initially undirected, you can ignore cycles of length 2 at the end.


[Answer]: For a Directed Graph - we keep track of the recursion stack. For an edge (u,v), if we currently are processing u, and we see that v is in the recursion stack, then we have a Cycle.
For Undirected Graph - we look construct a parent array while we are traversing with DFS. Similar situation, for an edge (u,v) while processing u, if we see that v is Visited && not the parent of u, then we have a cycle.
I hope this helps!


------------------------------------------------------------

ID: cs.stackexchange.com:115845
Title: How to deal with parallel edges between two vertices in cycle detection using BFS in an undirected graph?
Body: I am new to Programming and learning Algorithms and was studying BFS when I read that BFS could be used for cycle detection. I tried to implement the same on an undirected graph G with Adjacency List Representation.
What I did is as follows:

• Do a simple BFS Traversal using a Queue while maintaining the parent node of nodes enqueued in the queue.
• If I come across a node u that has a neighbor v such that v is already visited but v is not the parent of u then that means there is cycle in the graph.

Pseudocode:
#adjList is the adjacency list given as a dictionary
#myQueue is a double-sided queue containing node and its parent node ([Node, parNode])
#visited is a set containing visited nodes

while(myQueue):
    currNode, parNode = myQueue.pop() #dequeue operation
    visited.add(currNode) #Marking currNode as visited
    for childNode in adjList[currNode]: #Traversing through all children of currNode
        if currNode not in visited:
            myQueue.appendleft([childNode, currNode]) #Enqueue operation
        else:
            if childNode!=parNode: #Main logic for cycle detection
                print('CYCLE DETECTED')
                break

The above approach is working except in cases when I have more than 1 edge between 2 vertices for e.g. in following case we have 2 edges between vertices 0 and 1:

Adjacency list of above graph is: adjList = {0:[1, 1, 2], 1:[0, 0], 2:[0]}. Here we can clearly see that the graph contains a cycle (In the adjacency list, it is represented by the fact that 1 appears twice in the adjacency list of 0 and 0 appears twice in the adjacency list of 1) but above algorithm is not able to detect the same because when BFS will reach vertex 1, vertex 0 is already visited but vertex 0 is also the parent of vertex 1 so this cycle will go undetected.
My question is how I can modify above algorithm to detect such cases?
Edit: I tried the same logic on directed graphs also, and I am facing similar problem i.e. case when I have a directed edge from vertex 0 to vertex 1 and another directed edge from vertex 1 to vertex 0


[Answer]: If the case arrives where you see the case of a node being already visited but it is the parent of your current node, you just have to check whether there is a double edge between them. How to do that depends on your data structure, but if your adjacency list is sorted it would just amount to searching for the edge and checking how often it is in there.

Another problem I see is that your adjacency list doesn't actually contain any information that the edge is doubled.

For directed graphs, you can just completely get rid of the "parent check", as the only time that case arrives is when you have two edges going from $u$ to $v$ and vice versa.

Additionally, be careful if the graph is not connected, your BFS will not cover all of it, so you would need to start another BFS from an unvisited vertex again. This is especially important for directed graphs, as even if the graph might be connected, your BFS will probably not cover all of it.


------------------------------------------------------------

ID: cs.stackexchange.com:86148
Title: Can a 3 Color DFS be used to identify cycles (not just detect them)?
Body: Can a 3 color DFS be used to identify all cycles in a directed graph not just detect them? 

In other words if I have a directed graph with multiple cycles, can I run a function on them such that the function returns the list of nodes that compose each simple directed cycle in the graph and not just a boolean true or false? Most answers online show pseudocode for detection and not identification (ie functions which return boolean values instead of node lists). You can assume that the graphs I'm referring to are mostly tree like in structure and aren't deeper than 5 nodes or so.

So for example, for this graph:


The list would be [[a,b,e], [f,g], [c,d], [d,h]]

With white, grey, black DFS a cycle is found when a node already colored grey is visited a second time through a different edge. What I'm struggling to wrap my head around is when this happens, how do we back track to identify all of the nodes involved in this detected cycle without increasing complexity or running DFS a second time. In the example below, if we do a DFS (exploring right edges first) starting at A, the stack will look like this [A,B,C,E,D,B] and the cycle will be detected when B is visited a second time. Given this stack how do we deduce that C D and B only are part of the cycle and not E or A?  



I am aware that there are plenty of algorithms other than DFS that can do this (such as Johnson's algorithm or Tarjan's with a twist) I just want something simple to implement.


[Answer]: The answer to this question is YES. A three color DFS algorithm can be used to both detect and identify the nodes associated with all simple cycles in a directed graph with more or less the same time complexity. 

The key is to keep a stack of parent and child pairs as you recursively explore the graph using white, grey, black (3 color) depth first search. You must also remember to pop pairs off the stack after you are done exploring a child (ie when the child's color is set to black). This will give you an easy way to trace what nodes are traversed going to and from a "starting node" in a cycle.

I will add more detail and an example implementation later!


[Answer]: The moment you find a grey node, you have the edge that closes the cycle in hand. All other edges are part of the DFS tree. That is, store the "cycle-closing" edge and obtain the rest afterwards.

That is, assuming that those are the cycles you are interested in. Other cycles (such that contain more than one non-DFS-tree edge) are more complicated, but I don't think those are easy to cover. A directed graph may have super-exponentially many cycles, so you can't expect to list them all with a simple, linear-time traversal.


[Answer]: Here is an algorithm which takes your stack content $S$ when loop was detected & return array of the nodes which are forming cycle. From your example it is $ [A,B,C,E,D,B]$

 Algorithm_Get_Loop_Nodes(S):
     cycle_nodes = []
     repeated_node = S[top]
     while S[top] != repeated_node:
         prev_node = S.pop()
         curr_node = S[top]
         if curr_node is parent of prev_node:
            cycle_nodes.add(curr_node)
     return cycle_nodes



------------------------------------------------------------

ID: cstheory.stackexchange.com:49017
Title: Detect if a graph has a $k$ cycle in space complexity $O((\log k)^d)$ for fixed $d \geq1$
Body: For a graph $G$, I want to test if it contains a cycle of length $k$, for some $k$ much smaller than $|G|$. I am interested in particular in an algorithm with low space complexity. The cycle need not be induced, so that I want to return success also for instance in the case that G contains a $k$-clique.
I considered first a well known much simpler case, which is testing if $G$ is in fact a $k$ cycle. This can be done using $O(\log k)$ variables: Assume for simplicity for the moment that $k=4m$. Such an algorithm then boils down to the fact that we can test if two vertices have distance $t$ in space complexity $O(\log t)$. Then we test to see if there are four vertices $x_1,x_2,x_3,x_4$ such that $d(x_1,x_3)=d(x_2,x_4)=2m$ and all other distances between the $x_i$ are $m$.
Note crucially that this algorithm will fail to report true if the graph $G$ has $k$ vertices and contains a cycle, but also some additional edges.
My question now is the following: Is there a similar (or completely different) algorithm that detects when a graph $G$ of potentially large size, contains a $k$ cycle that is not necessarily induced, that has $O((\log k)^d)$ space complexity for some fixed $d \geq  1$? Clearly such an algorithm with space complexity $ k $ exists, but I would like to do better.
P.S. I don't know if it makes much of a difference, but in the applications I am looking at, $k$ will itself be of the order $(\log n)^2$ where $n = |G|$.
P.P.S Also, the graph $G$ that I am looking at will be sparse, they are Erdos-Renyi random graphs in $\mathbb{G}(n,n^{-c})$ for some $c\in (0,1)$.


[Answer]: No, this is impossible for your parameters.  With $s$ bits of space, you can only visit at most $2^s$ vertices of the graph.  Now set $s = O((\log k)^d)$ and $k=(\log n)^2$ and it is clear that you cannot visit all of the graph, as $2^s = o(n)$.  Thus for any algorithm that uses only $O((\log k)^d)$ bits of space, there exist graphs where you fail to detect a $k$-cycle (e.g., because the $k$-cycle is in the unvisited part of the graph).

If $k=n^{\Theta(1)}$, and if your definition of a cycle allows repeated vertices to appear in the cycle, then an algorithm exists.  A simple approach is to guess a starting vertex $v_0$ and store it (which requires $\log n = O(\log k)$ bits); then, non-deterministically guess a path of length $k$, counting up to $k$ as you go (which requires $\log k$ bits), checking that none of the intermediate vertices are equal to $v_0$ and the final vertex is equal to $v_0$.
If $k=n^{\Theta(1)}$, and if your definition of a cycle does not allow repeated vertices to appear in the cycle, then I don't know whether an algorithm exists or not.


------------------------------------------------------------

ID: cs.stackexchange.com:35994
Title: Why does Randomized Quicksort have O(n log n) worst-case runtime cost
Body: Randomized Quick Sort is an extension of Quick Sort in which the pivot element is chosen randomly. What can be the worst case time complexity of this algorithm. According to me, it should be $O(n^2)$, as the worst case happens when randomly chosen pivot is selected in sorted or reverse sorted order. But in some texts [1] [2] its worst case time complexity is written as $O(n\log{n})$

What's correct?


[Answer]: Both of your sources refer to the "worst-case expected running time" of $O(n \log n).$ I'm guessing this refers to the expected time requirement, which differs from the absolute worst case.

Quicksort usually has an absolute worst-case time requirement of $O(n^2)$. The worst case occurs when, at every step, the partition procedure splits an $n$-length array into arrays of size $1$ and $n-1$. This "unlucky" selection of pivot elements requires $O(n)$ recursive calls, leading to a $O(n^2)$ worst-case.

Choosing the pivot randomly or randomly shuffling the array prior to sorting has the effect of rendering the worst-case very unlikely, particularly for large arrays. See Wikipedia for a proof that the expected time requirement is $O(n\log n)$. According to another source, "the probability that quicksort will use a quadratic number of compares when sorting a large array on your computer is much less than the probability that your computer will be struck by lightning."

Edit:

Per Bangye's comment, you can eliminate the worst-case pivot selection sequence by always selecting the median element as the pivot. Since finding the median takes $O(n)$ time, this gives $\Theta(n \log n)$ worst-case performance. However, since randomized quicksort is very unlikely to stumble upon the worst case, the deterministic median-finding variant of quicksort is rarely used. 


[Answer]: Note that there are two things to take expectation/average over: the input permutation and the pivots (one per partitioning). 

For some inputs and implementations of Quicksort all pivots are bad ($n$ times the same number sometimes works) so randomisation does not help. In such a case the expected time (averaging over pivot choices) can be quadratic in the worst case (a bad input). Still, the "overall" expected time (averaging over both inputs and pivot choices) is still $\Theta(n \log n)$ for reasonable implementations.

Other implementations have true worst-case runtime in $\Theta(n \log n)$, namely those that pick the exact median as pivot and deal with duplicates in a nice way.

Bottom line, check your source(s) for which implementation they use and which quantity they consider random resp. fixed in their analysis.


[Answer]: Yes you are corect, it will be $O(n^2)$.

The worst case for randomized quicksort is same elements as input. Ex: 2,2,2,2,2,2

Here the algorithm whatever it picks will be $T(n) = T(n-1) + n$ and hence $O(n^2)$.


[Answer]: You were missing that these texts talk about "worst case expected run time", not "worst case runtime". 

They are discussing a Quicksort implementation that involves a random element. Normally you have a deterministic algorithm, that is an algorithm which for a given input will always produce the exact same steps. To determine the "worst case runtime", you examine all possible inputs, and pick the one that produces the worst runtime. 

But here we have a random factor. Given some input, the algorithm will not always do the same steps because some randomness is involved. Instead of having a runtime for each fixed input, we have an "expected runtime" - we check each possible value of the random decisions and their probability, and the "expected runtime" is the weighted average of the runtime for each combination of random decisions, but still for a fixed input. 

So we calculate the "expected runtime" for each possible input, and to get the "worst case expected runtime", we find the one possible input where the expected runtime is worst. And apparently they showed that the worst case for the "expected runtime" is just O (n log n). I wouldn't be surprised if just picking the first pivot at random would change the worst case expected runtime to o (n^2) (little o instead of Big O), because only a few out of n pivots will lead to worst case behaviour. 


------------------------------------------------------------

ID: cs.stackexchange.com:29284
Title: What is the notation for bounding running time in worst case with concrete example resulting in that worst case running time
Body: I know that Big O is used to bound worst case running time. So an algorithm with running time $O(n^5)$ means its running time in worse case is less than $n^5$ asymptotically.

Similarly, one can say that for example merge sort's running time is $O(n^2)$  which is correct. But we know that there is a better bound for it: $O(n\log n)$. Technically speaking, one can say that every polytime algorithm has running time $O(2^n)$. This is correct, but not useful.

So my question is: what is the notation used for the case of worst case running time such that there exists an input in which the worst case running time happens.

In the merge sort example, one cannot construct an input example so that merge sort would take $n^2$ comparisons, but one can construct an example that requires the number of comparisons being about $n\log n$.


[Answer]: See "big 0" or Landau notation in wikipedia. What you are looking
for is the section on Bachmann-Landau notations.


$f(n)\in O(g(n))$ means $f$ is bounded above by $g$ up to a constant.
$f(n)\in \Omega(g(n))$ means $f$ is bounded below by $g$ up to a constant.
$f(n)\in \Theta(g(n))$ means $f(n)\in O(g(n)) \wedge f(n)\in \Omega(g(n))$


Further remarks
(since the scope of the question is being implicitely extended)

These definitions can be used for worst case as well for a best case
analysis of complexity, starting from an analysis of respectively the
greatest or least costs over all inputs of size $n$.

Then, for algorithm $A$, let $A_{min}(n)$ and $A_{max}(n)$ be
respectively the best case and worst case complexity of $A$.

Then, omitting intentionally "worst case" or "best case". so as to
cover all cases, you can say that:


the complexity of $A$ is $O(g(n)$ iff $A_{max}(n)\in O(g(n)$
the complexity of $A$ is $\Omega(g(n)$ iff $A_{min}(n)\in \Omega(g(n)$


Now using the $\Theta$-notation is not necessarily meaningful for this
"overall" complexity of $A$. It is meaningful iff $O(A_{max}(n))=O(A_{min}(n))$

To be complete, many people also use average case analysis.

Also, people often mean worst case when they omit qualification of the complexity.


[Answer]: In words, such a bound is called a tight bound. In the specific case of Landau notation, $\Theta$ can be used to indicate a tight bound.
You have to be somewhat careful about your wording, though:


"The runtime of quicksort is in $O(n^2)$," is a correct statement (but somewhat imprecise).
"The runtime of quicksort is in $O(n^2)$, and this bound is tight," is correct and expresses, what you want to express.
"The worst-case runtime of quicksort is in $\Theta(n^2)$," is equivalent to the previous statement.
But "The runtime of quicksort is in $\Theta(n^2)$," is not correct, since this statement implies that quicksort takes quadratic time on every input.



[Answer]: Use the notation that expresses the strongest property you know to be true.

Suppose you have an algorithm.


If you have a class of inputs on which the algorithm runs in time $O(f(n))$, this actually tells you nothing about the worst-case running time: the worst case is worse than something that's better than $f(n)$, which could be literally anything.
If you have a class of inputs on which the algorithm runs in time $\Omega(f(n))$ or $\Theta(f(n))$, then you know that the worst case running time is at least as bad as this class, so the worst case is $\Omega(f(n))$.
If you have a class of inputs which you know to be the worst case, and the algorithm runs in time $O(f(n))$, $\Omega(f(n))$ or $\Theta(f(n))$, they you know that the worst-case running time is $O(f(n))$, $\Omega(f(n))$ or $\Theta(f(n))$, respectively.



  In the merge sort example, one cannot construct an input example so that merge sort would take $n^2$ comparisons, but one can construct an example that requires the number of comparisons being about $n\log n$.


This falls into the third bullet: you have a class of inputs which you know to be the worst case, and the algorithm takes time $\Theta(n\log n)$ on those inputs. So the worst-case running time is exactly that: $\Theta(n\log n)$.


------------------------------------------------------------

ID: cs.stackexchange.com:29145
Title: Proving Quicksort has a worst case of O(n²)
Body: I am sorting the following list of numbers which is in descending order.
I am using QuickSort to sort and it is known that the worst case running time of QuickSort is $O(n^2)$

import java.io.File;
import java.io.FileNotFoundException;
import java.util.*;



public class QuickSort 
{
    static int pivotversion;
    static int datacomparison=0;
    static int datamovement=0;

    public static void main(String args[])
    {
        Vector container = new Vector();

        String userinput = "data2.txt";
        Scanner myScanner = new Scanner("foo"); // variable used to read file
        Scanner scan = new Scanner(System.in);


        System.out.println("Enter 1 to set pivot to be first element");
        System.out.println("Enter 2 to set pivot to be median of first , middle , last element of the list");
        System.out.println("Your choice : ");
        //pivotversion = scan.nextInt();


        try
        {

            File inputfile = new File("C:\\Users\\8382c\\workspace\\AdvanceAlgorithmA3_Quicksort\\src\\" + userinput);
             myScanner = new Scanner(inputfile);

        }
        catch(FileNotFoundException e)
        {
            System.out.println("File cant be found");
        }


         String line = myScanner.nextLine(); //read 1st line which contains the number of numbers to be sorted

         while(myScanner.hasNext())
         {
             container.add(myScanner.nextInt());
         }


        System.out.println(line);



        quickSort(container,0,container.size()-1);

        for (int i =0;i container, int left, int right)
    {
          int i = left, j = right;
          int tmp;

          int pivot= 0 ;
          pivot = container.get(left);

          boolean maxarraybound = false;




          i++;

          while (i  pivot)
                {
                      j--;
                      datacomparison++;
                }
                if (i  container, int left, int right) 
    {
          int index = partition(container, left, right);
          if (left 

I am trying to prove to myself that the worst-case running time of QuickSort is indeed $O(n^2)$ by summing up the total number of data comparisons and data movements in the algorithm. 

In my current situtation, I have an input of 10000 numbers.

I would expect a total sum of data comparison and data movement to be around 100 million. 

I am only getting a total sum of data comparsion and data movement of around 26 million.


  I am sure I have miss out some "data movement" and "data comparsion"
  in my algorithm, can someone point out to me where as I have no clue?



[Answer]: Asymptotic bounds don't so much tell you how long to expect an algorithm to take, or how many operations it might perform, on a fixed input. Rather, the goal is to characterize how the time taken (or operations performed) by the algorithm grows for inputs of sufficient size. What $O(n^2)$ means is that the worst-case is bounded by some function that increases quadratically; that is, if you give your algorithm a worst-case input of size $n$ and then a worst-case input of size $2n$, you should expect your algorithm to take four times as long on the larger input (since $\frac{(2n)^2}{n^2} = \frac{4n^2}{n^2} = 4$) You might try a worst-case list of around 20,000 numbers and expect to see around 104 million operations.

Why do asymptotic notations work this way? Well, mathematics aside, it's useful for understanding how programs work. Different computers can execute at different clock frequencies, so absolute time measurements wouldn't be helpful for evaluating algorithms cross-platform (unless some reference architecture was used, which is also done, but bear with me). Code compiled for different architectures may perform more or fewer elementary operations, so an exact count of operations isn't really very useful in practice. Asymptotic growth, however, which tells you how an algorithm scales, and what to expect by way of comparison when you change the input size, is a pretty reliable indicator of how programs actually scale on any given system. You could compile your Quicksort for my x86, my friend's AMD64, my dad's ARM, and my grandpa's MIPS and you'd be able to observe the expected scaling behavior. Good luck getting the same number of operations for the same algorithm and input across those platforms!


[Answer]: To answer your question directly, In the worst case, there will be n-1 comparisons in the first recursion, n-2 comparisons in the second recursion, n-3 comparisons in the third, and so and so on, so actually, with n = 10000, I would expect the number of comparisons to be closer to 50,000,000.

As for the actual code, I think you need an additional datacomparison++ after the while ( container.get(i)  loop, as well as an additional datacomparison++ after the while ( container.get(j) > pivot) loop. If you think about how while loops work, you will realize that an additional comparison is performed for the while loop to exit, which you are not counting.

I think with the above two changes, you may get something quite close to 50 million comparisons.


[Answer]: Your question demonstrates some confusion regarding what asymptotic notations such as $O(n^2)$ and expressions such as “worst case” mean. I recommend reading the definitions and some examples. If you have a textbook, work through its exercises. Browse through our questions as well. Understanding comes from experience, so whatever you do, do work out some cases for yourself.
Asymptotic behavior

In my current situtation, I have an input of 10000 numbers.
I would expect a total sum of data comparison and data movement to be around 100 million.
I am only getting a total sum of data comparsion and data movement of around 26 million.

Asymptotic notation such as big oh counts up to a multiplicative factor. When we say that a running time is e.g. $\Theta(n^2)$, it doesn't mean that the running time is $n^2$ — that would lack a unit (is it $n^2$ clock cycles? $n^2$ seconds? …). It means that there is a multiplicative constant $C$ (which you can think of as a unit of measurement) such that the running time is about $C \, n^2$. (Furthermore, this is the case only for large enough $n$, but here the data size is large enough.)
With a single point of measurement, you can't tell anything. Asymptotic notations don't give a value, they give a shape. If the running time of an algorithm is $\Theta(n^2)$, then for large enough $n$, if you plot the running time against the data size, the shape is (approximately) a parabola. This parabola could be at any scale. When you get a number of operations $T(n) \approx 26 \times 10^6$ for an input size $n = 10,000$, if it is indeed the case that $T(n) \in \Theta(n^2)$ and the input size is into the “asymptotic zone”, this tells you that the constant $C$ such that $T(n) \approx C \, n^2$ satisfies $26 \times 10^6 \approx C \times (10^4)^2$. Note that the constant $C$ has a unit: it's a number of elementary operations.
If you want to test experimentally whether the running time is indeed $\Theta(n^2)$, you need to test with multiple input data sizes. Calculate the constant $C$ for each data size; if the hypothesis of $\Theta(n)$ behavior is correct, then you should get approximately the same value for large enough inputs. In other words, plot the running time against the data size, and check whether the shape does look like the expected parabola.
Worst case vs upper bound
The notation $T(n) \in O(n^2)$ means that $T(n)$ is at most $n^2$, up to a scaling factor (multiplicative constant), for large $n$. The big oh gives an upper bound; it's possible that the function is in fact less than this upper bound (i.e. the upper bound is not optimal). For example, $n \in O(n^2)$ (because for all $n \ge 1$, $n \le 1 \times n^2$). The time complexity of quicksort is $O(n^2)$; it's also $O(n^3)$ and $o(42^n)$, but those statements are weaker and thus less useful.
The worst case of quicksort is $O(n^2)$ (and also $O(n^3)$, etc.). It is true that for most choices of pivot, the worst case is $\Theta(n^2)$. What this means, using $T(V)$ to denote the number of operations for the input $A$, is that
    there exists $N$, $C_1$ and $C_2$ such that
    for every $n \ge N$,
    there exists an input $A$ of size $n$
    such that $C_1 n^2 \le T(A) \le C_2 n^2$
            and for all inputs $A'$ of size $n$, $T(A') \le T(A)$.
If you show that quicksort is $O(n^2)$ (i.e. at most $n^2$ for large $n$, up to a scaling factor), that obviously tells you that the worst case is $O(n^2)$, but there may be a better bound. If you want to prove that the worst case is $\Theta(n^2)$, i.e. that quicksort for large enough $n$ is sometimes approximately $C \, n^2$ but never more for some $C$, it's enough to know that quicksort in general is $O(n^2)$ and that there is some input distribution (for every $n$) that's $\Omega(n^2)$, i.e. at least $C_1 n^2$ for large enough $n$.

I am sorting a list of numbers which are in descending order , should't it be a worst case scenario already???

There is no intrinsic relationship between the original order of the elements and the difficulty of sorting them. Sorting numbers that start out in descending order is not intrinsically harder than any other order. With quicksort, what input distribution constitutes the worst case depends on how the pivot is chosen. For example, if you always pick the first element as the pivot, input that is in ascending order and input that is in descending order both reach the $\Theta(n^2)$ worst case. If you always pick the middle element as the pivot, these two input distributions do not reach the worst case — an example worst case would be with the smallest element in the middle (position $\frac{1}{2} n$ rounded to an integer), the next smallest at position $\frac{1}{4} n$, the next at $\frac{3}{4} n$, the next four at $\frac{1}{8} n$, $\frac{3}{8} n$, $\frac{5}{8} n$, $\frac{7}{8} n$, and so on.


[Answer]: here is a writeup for an NYU CS lecture containing info/reasoning on why quicksort is $O(n^2)$ in worst case even with any choice of pivot algorithm, skip to section "Worst case of quicksort". others have pointed out why $O(f(n))$ notation is not really a formula for individual datapoints. as for finding $O(n^2)$ in practice, ie via empirical experiment/ exercise as you seem interested in, try writing code that repeatedly runs Quicksort on worst case inputs (previously sorted ascending or descending) of size $n..m$ and graph the results, and fit it to a function.


------------------------------------------------------------

ID: cs.stackexchange.com:102807
Title: When average , worst and best case time complexity happens in quick sort?
Body: I know recurrence relation corresponding to quick sort worst case is 

$T(n)=T(n-1)+T(0)+\Theta(n)$

and time complexity is $O(n^2)$.

This happens when we select pivot which is either largest element or smallest element in the current sub problem.

This can occur when array is sorted in either descending or ascending order and we select either largest or smallest element as pivot.

I also know that best case occurs when median is chosen as pivot, giving recurrence relation

$T(n)=T(n/2)+T(n/2)+\Theta(n)$.

and time complexity $O(n \log n)$.


  Statement 1: Any other partition should result in average case. 


I have come across problems each time choosing pivot which partition array in some fixed proportion (say $T(n)=T(n/10)+T(9n/10)+\Theta(n)$) ending in average case performance of $O(n \log n)$ (and I understand its proof). 

However I have also came across text saying:


  Statement 2: Any pivot which does not partition array in some proportion should result in worst case. If $n$th smallest or largest element is selected as pivot, it will result in worst case. 


Q1. Is statement 1 true? If no, which other pivot selection strategies could result in worst case or best case behavior?

Q2. Is statement 2 true?


[Answer]: Remember, for big-O we care about the asymptotic limit. If you choose the $k$th largest (or smallest) element, for constant $k$, then when you make $n$ big enough it's as though you chose the single largest or smallest. The $k$ elements on the other side just aren't enough to matter any more. So statement 2 is true.

Statement 1 is also true; you say you understand the proof of it, so I'm not sure how much detail to go into here. But intuitively, the amount on each side of the pivot grows proportionally to $n$. That's what makes it different from the constant $k$.


[Answer]: The time complexity for particular runs of quicksort can be anywhere between the best case and the worse case.  It can be $\Theta(n(\log n)^2)$, $\Theta(n^{\frac32})$, $\Theta(n^{\frac{19}{10}})$ or $\Theta(n^2/\log n)$. Given any $f$ such that $n\log n\le f(n)\le n^2$, we can construct runs of quicksort whose time-complexity is $\Theta(f(n))$.
Let $q$ be a quicksort algorithm. Let $c_q(A)$ be the number of comparisons used by $q$ on input array $A$. Let $m_{q,n}=\min_{\#A=n}c_q(A)$ and $M_{q,n}=\max_{\#A=n}c_q(A)$ for $n>0$. The following property holds for various versions of quicksort.
Continuum of comparisons by quicksort: Given any integer $i$ between $m_{q,n}$ and $M_{q,n}$, there is an array $A$ of size $n$ such that $c_q(A)=i$.
We can replace "comparisons" above by "swaps".


Statement 1: Any other partition should result in average case.
Q1. Is statement 1 true? If no, which other pivot selection strategies could result in worst case or best case behavior?

The "continuum of comparisons by quicksort" shows statement 1 does not make much sense. To determine the asymptotic behavior of a quicksort algorithm, we need to specify the version of quicksort with its partition scheme and which kind of input arrays might be given. It does not make much sense to say $O(n\log n)$ is the time-complexity of the average case without specifying "the average case".
Instead of statement 1, here is the clearer summary, where the average case is when the given input arrays are uniformly random, as described here.

All quicksort algorithms (that I have seen, including these variants)  takes $\Theta(n \log n)$ time in expectation in the average case.

Some quicksort algorithms choose the pivot that partition an array within some fixed proportion. Those algorithms run in $\Theta(n\log n)$-time in all cases, assuming a linear-time algorithm is used to choose the pivot. However, the constant factors hidden in the big $\Theta$-notations for these algorithms are much larger than those factors for other algorithms.




Statement 2: Any pivot which does not partition array in some proportion should result in worst case. If nth smallest or largest element is selected as pivot, it will result in worst case.
Q2. Is statement 2 true?

Although sounds reasonable, statement 2 is too ambiguous to be verified or refuted. The following is a version that could be proved.
Statement 3: The classic quicksort with Lomuto partition scheme or
Hoare partition scheme runs in $\Theta(n^2)$-time if the pivoting on every array partition the array into two parts with one part of size $O(1)$. In particular, for any constant $c$, if $c$-th smallest or $c$-th largest element is selected as pivot whenever the size is no less than $c$, it runs in $\Theta(n^2)$ time.


Exercise 1. Prove statement 3.
Exercise 2. Prove  "continuum of comparisons by quicksort" for a version of quicksort known to you.




[Answer]: One addition: Always picking the median element is not necessarily optimal. Quicksort spends time comparing items and time moving items. Picking the median element minimises the number of comparisons, but the number of moves is smaller if we don't pick exactly the median. Depending on the relative cost of comparing and moving items, picking a pivot a bit away from the median can be faster. 


[Answer]: It is true that when all partitions fulfill a constant proportion (or better), the $n\log n$ behavior occurs. This is because the size of the partitions decreases in geometric progression.
In particular, the median-of-medians strategy ensures such a ratio.
It is also true that when no partition fulfills a constant proportion, the $n^2$ behavior occurs.  This is because the size of the partitions decreases in arithmetic progression.
Statement 1 is wrong. (Unless part of the context is missing or "any other partition" has a special meaning.)
Statement 2 is right.


------------------------------------------------------------

ID: cs.stackexchange.com:145748
Title: How can you have an upper and lower bound on the worst-case complexity of an algorithm?
Body: Yesterday I started reading "An Introduction To The Analysis of Algorithms" by Sedgewick/Flajolet. For me it was not clear what he meant with "theory of algorithms" and the "scientific approach". I googled a bit and found someone who had the same question:
Sedgewick analysis of algorithm, difference between theory of algorithm and scientific approach
Until then I thought that I had a good understanding of what worst-case complexity of an algorithm is and how it relates to $O()$ but I think that I was a bit wrong.
In the upper thread someone commented:  "(BTW O(), Ω(), Θ() as used in CLRS are about upper, lower and tight bounds all on the worst-case running time, they are not about worst-case, best-case, average-case.)"
And then I was confused. Let split this up in worst-case analysis of a problem and worst-case analysis of a specific algorithm.
Worst-case analysis of a problem
When you have a problem B you want to solve, than it is absolutely understandable that you can have an upper and lower bound on the worst-case complexity since there are numerous algorithms for problem B which all can have different worst-case complexities.
In this case one could say:
The worst-case complexity of problem B lies in O(f(n)) if there is an algorithm for problem B whose worst-case runtine complexity lies in O(f(n)).
The worst-case complexity of a problem B lies in Ω(f(n)) if there can't be (yeah this has to be rigorously proved) an algorithm for the problem B whose worst case complexity lies not! in Ω(f(n)).
A good example is the proof in CLRS that the worst-case complexity of sorting(via comparing)  lies in Θ(n log(n)).
So now to the worst-case complexity of an algorithm.
Worst-case analysis of an algorithm
Lets say we have an algorithm A for some problem and want to find out something about the worst-case complexity of A. For the upper bound I just have to find out for which problem instances the algorithm A has to do the most steps or the most "work".
So here is my problem: What should the lower bound on the worst-case complexity of an algorithm A be?
There is only one way which for me could explain that. If every algorithm yields several different implementations than one could really define a lower bound on the "best" worst-case complexity on all possible implementations of this algorithm A.
If everything what I wrote here is right for the most part than I have to say, that a lot! of books deal extremely sloppy with the relation between the asymptotic analysis and the best-,average and worst-case runtime.


[Answer]: There is no real difference between the definition of upper and lower bounds.
You say

For the upper bound I just have to find out for which problem instances the algorithm A has to do the most steps or the most "work"

Well, that is the same thing with the lower bound, but instead of saying "this particular worst instance of size $n$ takes at most $f(n)$ time to compute", you will say "this particular worst instance of size $n$ takes at least $g(n)$ time to compute".
Obviously, since we are talking about the same worst instance, the computing time will be the same, so it would be more pertinent to give a tight bound instead of a lower or an upper. However, it is not always as easy to find a lower bound than an upper bound (and vice versa).
For example, considering the quicksort algorithm, the worst-case occurs when the array is already sorted. Since a partition is needed, it is easy to see that the worst-case is $\Omega(n)$. Since there is at most $n$ calls to the partition function, we can say that the worst-case is $\mathcal{O}(n^2)$. However, it needs a bit more of analysis to conclude that the worst-case is $\Theta(n^2)$.


[Answer]: Just because an algorithm has a worst case, doesn’t mean you can find it out.
You may find a set of cases that seem quite bad to you, but can all be solved in O(n^2). But you can’t prove that every instance can be solved in O(n^2). So the lower bound for worst case is O(n^2). On the other hand maybe you can prove that every instance can be solved in O(n^3). But you don’t actually have a set of instances that take that long. The worst you have is your set of O(n^2) instances. So you have an upper bound for the worst case, which is O(n^3). The actual worst case is somewhere in between.


------------------------------------------------------------

ID: datascience.stackexchange.com:60268
Title: Load large .jsons file into Pandas dataframe
Body: I'm trying to load a large jsons-file (2.5 GB) into a Pandas dataframe. Due to the large size of the file  pandas.read_json() will result in a memory error. 

Therefore I'm trying to read it in like this: 

S_DIR = r'path-to-directory'

with open(os.path.join(S_DIR, 'file.jsons')) as json_file:
    data = json_file.readlines()
    data = list(map(json.loads, data))

df = pd.DataFrame(data)


However, this just keeps running, slowing/crashing my pc. 

What would be the most efficient way to do this? 

The final aim is to have a subset (sample) of this large file.jsons dataset.

Thanks  


[Answer]: There are a few ways to do this a little more efficiently:

JSON module, then into Pandas

You could try reading the JSON file directly as a JSON object (i.e. into a Python dictionary) using the json module:

import json
import pandas as pd

data = json.load(open("your_file.json", "r"))
df = pd.DataFrame.from_dict(data, orient="index")


Using orient="index" might be necessary, depending on the shape/mappings of your JSON file.

check out this in depth tutorial on JSON files with Python.

Directly using Pandas

You said this option gives you a memory error, but there is an option that should help with it. Passing lines=True and then specify how many lines to read in one chunk by using the chunksize argument. The following will return an object that you can iterate over, and each iteration will read only 5 lines of the file:

df = pd.read_json("test.json", orient="records", lines=True, chunksize=5)


Note here that the JSON file must be in the records format, meaning each line is list like. This allows Pandas to know that is can reliably read chunksize=5 lines at a time. Here is the relevant documentation on line-delimited JSON files. In short, the file should have be written using something like: df.to_json(..., orient="records", line=True).

Not only does Pandas abstract some manual parts away for you, it offers a lot more options, such as converting dates correctly, specifying data type of each column and so on. Check out the relevant documentation.

Check out a little code example in the Pandas user guide documentation.

Another memory-saving trick - using Generators

There is a nice way to only have one file's contents in memory at any given time, using Python generators, which have lazy evaluation. Here is a starting place to learn about them.

In your example, it could look like this:

import os

# Get a list of files
files = sorted(os.listdir("your_folder"))
# Load each file individually in a generator expression
df = pd.concat(pd.read_json(file, orient="index") for f in files, ...)


The concatenation happens only once all files are read. Add any more parameters that are required where I left the .... The documentation for pd.concat are here.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:354503
Title: maintaining indexing on json files
Body: I have a directory which has approx. 100k JSON files.

I want to retrieve a list of file names, where the file satisfies a set of conditions, with a filter condition in same format as we can pass the filter object in a MongoDB find query.

Does the native file system itself have some sort of logic to maintain indexing as MongoDB does for its documents, or do there exist some tools to do the same?

The files can be added, updated, removed at any point of time, so indexing should be maintained throughout.

I am working on some NodeJS application so would prefer some npm tool that is cross platform compatible. 


[Answer]: You haven't described the "set of conditions" in any detail or with specific examples, but unless they are very simple conditions (e.g. filename.startswith('r')), mainstream filesystems lack the indexing and query features you seem to be describing. That is what databases are good at, and what they are for. Load the data and/or metadata you want to query into a database, do the queries, and process the resulting files.

If you have 100K JSON-serialized documents that you need to do (indexed) queries over, load them into a database and use the database indexing and query features. This is exactly what document-structured databases such as MongoDB were invented for.

Side note: There have been attempts to add database-like capabilities onto filesystems. The BeOS file system is a notable example. But they have not been present in mainstream-successful operating systems. One could argue this would be a good feature to have, but it imposes considerable complexity: basically a full database implementation beneath the filesystem. Bit of a chicken-egg problem there; what storage capabilities or abstractions does the database-like filesystem rely upon? Could be raw disk blocks, but many databases now prefer a file-like infrastructure. That implies a lower-level, non-database-like filesystem. If you're doing multiple filesystem stages, why not rely on more classical abstraction layers: raw disk, logical disk, filesystem, database? To support the notion of filesystem-that's-also-database-queryable, you have to assume that managing data as files is going to be a major use case. And you have to assume that handling general queries in the file system is going to be valuable enough to compensate the extra complexity. Historically and in the evolution of filesystems and large dataset handling, those assumptions have not borne out especially well. 

Today's highest-scale filesystems indeed use indexing techniques such as B-trees, but indexing on a simple set of attributes (e.g. file names) in support of reasonable performance at scale rather than generalized querying. Typical practice would be managing queries inside a database that is layered atop a less-capable filesystem. You might store the resulting datasets/payloads in the database (i.e. decomposed into native database tables or documents), in the database as blobs (e.g. blobs in relational DBMS, GridFS in MongoDB), or externally (e.g. as files).


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:355166
Title: Load JSON immediately for SPA instead of AJAX? (Python=>AngularJS)
Body: I have access to a Response object in my python backend for returning HTML pages, etc. I can declare headers, content-type, etc.

Instead of re-engineering the Python to use something like Django/Flask (which would allow me to parse JSON data on the backend and stick in right on the page), could I do something like add an arbitrary response header key to hold my initial JSON that needs to load on the page, then parse that JSON with my AngularJS? Or is this bad practice /a lazy & dirty hack?

The HTML Page I return contains all the necessary  tags to call JS that makes the app run and do Ajax requests as the user interacts with the app, but I really would like to pass on JSON during the initial load without rebuilding how our Python serves HTML files / templates.


[Answer]: I'd rather embed the json in the page content than in a header. 

Best practice for embedding arbitrary JSON in the DOM? on stackoverflow has some suggestions, such as using a  tag or a data- attribute.

Just make sure to encode/escape it appropriately, so that an unexpected string value can't break the parsing of your html (leads to xss vulnerabilities).


[Answer]: Response headers are not available from javascript so the proposed solution would not work for you. 

I don't know your setup but less than a complete framework should be able to handle this task. You can insert an empty script tag with an id in your html file. Then you can parse the file using a html parser, replace the content in the script with 

var myData=JSON...


Before you serve the file.


------------------------------------------------------------

ID: datascience.stackexchange.com:23674
Title: Converting Json file to Dataframe Python
Body: I have a json file which has multiple events, each event starts with EventVersion Key. The data looks similar to the following synthesized data.

{"Records":[{"eventVersion":"1.04","userIdentity":{"type":"R","principalId":"P:i","arn":"arn:aws:sts::5","accountId":"50","accessKeyId":"AW","sessionContext":{"attributes":{"mfaAuthenticated":"f","creationDate":"2013-09"},"sessionIssuer":{"type":"R","principalId":"WA","arn":"arn:aws:iam::6","accountId":"70","userName":"user1"}}},"eventTime":"2027-6","eventSource":"a.com","eventName":"DS","awsRegion":"UZ","sourceIPAddress":"2.1.3","userAgent":"li","requestParameters":null,"responseElements":null,"requestID":"OO","eventID":"09","eventType":"ABC","apiVersion":"2010-4","recipientAccountId":"78"},{"eventVersion":"1.04","userIdentity":{"type":"R","principalId":"P:i","arn":"arn:aws:sts::5","accountId":"50","accessKeyId":"AW","sessionContext":{"attributes":{"mfaAuthenticated":"f","creationDate":"2013-09"},"sessionIssuer":{"type":"R","principalId":"WA","arn":"arn:aws:iam::6","accountId":"70","userName":"user1"}}},"eventTime":"2027-6","eventSource":"a.com","eventName":"DS","awsRegion":"UZ","sourceIPAddress":"2.1.3","userAgent":"li","requestParameters":null,"responseElements":null,"requestID":"OO","eventID":"09","eventType":"ABC","apiVersion":"2010-4","recipientAccountId":"78"}]}


I'm using the following code in Python to convert this to Pandas Dataframe such that Keys are columns and values of each event is a row. 

with open('/Users/snehahonnappa/Documents/NLP_AWSlogs/Model/Data/505728423372_CloudTrail_ap-northeast-1_20160913T1700Z_yKA3wB5Nx6juR6Kg.json') as json_data:
    sample_object = json.load(json_data)

df = pd.io.json.json_normalize(sample_object)
df.columns = df.columns.map(lambda x: x.split(".")[-1])

print df.shape


When I print shape of the dataframe its 1X1. I'm expecting (Number of unique keys X Number of records) 

Snippet of how I'm expecting the dataframe to be

eventVersion               userIdentity                          eventTime
             type   principalId P   arn     accountID userName  
1.04         R      P           i   arn:aws 50        user1      2027-6
1.06         Q      O           i   arn:aws 67        u2         2027-7 


Appreciate any help. 

Update :

I'm writing the json file into a csv and then trying to convert this to dataframe on which my models can be applied on. Following is my code.

import json
import csv
import sys

data_parsed = json.loads(open('/tmp/A.json').read())
log_data = data_parsed['Records']

# open a CSV file for writing
data = open('/tmp/log.csv', 'w')

# create the csv writer object
csvwriter = csv.writer(data)
count = 0

for i in log_data:
      if count == 0:
             header = i.keys()
             csvwriter.writerow(header)
             count += 1
      csvwriter.writerow(i.values())

data.close()


This is writing the keys as headers and values of each record as a separate row which is as expected. However the nested json objects are being written as one value.

Following is a snippet of my csv file which was obtained by executing the above code.

eventVersion eventID eventTime requestParameters                eventType
1.04         0       2016-20                                    AwsApiCall
1.04         8       2016-20    {u'tagKeys': [u'User Name']}    AwsApiCall
1.05         4       2016-30    {u'filterSet': {u'items': [{u'name': u'resource-type', u'valueSet': {u'items': [{u'value': u'*'}]}}, {u'name': u'tag:User Name', u'valueSet': {u'items': [{u'value': u'*'}]}}]}}    
                                                                AwsApiCall


Any suggestions to tackle this?


[Answer]: I once ran into a situation like this where i wanted a complex dataframe due to the original source having a complex data structure.

I solved my issue by simplifying my structure into multiple separate dataframes instead of one big complex multi structure dataframe.

With simple separate dataframes i was better positioned to apply complex algorithmic operations.

Looking at your specific data, you could get rid of userIdentity which results in a simple 2d dataframe. This should position you to do any complex operation.

I understand this doesn't answer your specific dataframe structure requirement. 

But i hope this answers the spirit of your objective.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:432153
Title: Constantly writing a JSON file
Body: I need to write continuously into a JSON file in C++. The format and structure are defined as requirement. The data is produced in the process itself and needs to be stored multiple times per second. It's actually a protocol of realtime data.
Structure is basically:
[{Object01}, {Object02}, ...]
With every tenth of a sec, I have to add another dataset object. The total duration is between 30 to 90 minutes.
I'll go for rapidJSON, to ensure the proper form, etc.
The question is: How do I organize the write stream?

I hold all the data in one Document object and write it into the file at the end of the process.
I need to read in the whole file in every loop and write it back.
There is a proper way that only new objects getting appended to the array in the file.

The I would go for the last point, but I have actually no idea how to do that.


[Answer]: The last time I had a requirement like that, the process looked like

When starting a new file, write the characters [\n to it to open a JSON array.
When appending to an existing file, seek to the end. Then move 3 characters back (over the \n]\n sequence expected at the end) and write ,\n
Serialize a single object to JSON
Write ]\n or \n]\n to the file (depending on if the serializer ends the string with a newline or not)

This means that your JSON library only processes a single object at a time and you are manually maintaining the file as a valid JSON array.


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:441734
Title: Dynamic Variables from JSON Parameter File
Body: I want to assign Python variables imported from a JSON file. This question had an interesting answer using a classmethod, but I couldn't get it to work and I'm not allowed to comment...
So, let's consider a really simple example: I want to evaluate z = x^2+y^2 but I want to be able to define x and y in a JSON file. My json file (params.json) might look like:
{
    "x":3,
    "y":2
}

Then I could load an load the file and generate dynamic variables:
with open("params.json", "r") as read_file:
    params = json.load(read_file)

for k, v in params.items():
    vars()[k] = v
    
z = x^2+y^2

This works, but it seems dangerous to dynamically generate variables. Is there a standard/smarter way to do this?


[Answer]: This depends a lot on your security goals, and on what kind of user interface you want to offer to the author of these expressions.
Loading variables into the local scope does work, since Python is a very dynamic language. There's a risk though that the variables might re-define existing objects, thus breaking your code – what if there's a variable called len, for example?
Therefore, it's usually safer to avoid running the user input in a Python context. Instead:

define a simple programming language for these expressions
write an interpreter that executes the expressions

Python does have tools to help here. We can parse strings as Python code via the ast module. This returns a data structure that represents the syntax, and doesn't execute anything (though the parser isn't necessarily safe against malicious inputs). We can take the data structure, walk it, and execute it according to the rules we define – such as by resolving variables only from a dictionary. Example code for Python 3.10:
import ast

def interpret(code: str, variables: dict) -> dict:
  module: ast.Module = ast.parse(code, mode='exec')
  for statement in module.body:
    _interpret_statement(statement, variables)
  return variables

def _interpret_statement(statement: ast.stmt, variables: dict) -> None:
  match statement:
    case ast.Assign(targets=[ast.Name(id=name)], value=value):
      variables[name] = _interpret_expr(value, variables)
      return

    case other:
      raise InterpreterError("Syntax not supported", other)

def _interpret_expr(expr: ast.expr, variables: dict) -> Any:
  match expr:
    case ast.BinOp(left=left_ast, op=op, right=right_ast):
      left = _interpret_expr(left_ast, variables)
      right = _interpret_expr(right_ast, variables)
      return _interpret_binop(left, op, right)

    case ast.Name(id=name):
      return variables[name]

    case ast.Constant(value=(int(value) | float(value))):
      return value

    case other:
      raise InterpreterError("Syntax not supported", other)
    

def _interpret_binop(left: Any, op: ast.operator, right: Any) -> Any:
  match op:
    case ast.Add(): return left + right
    case ast.Sub(): return left - right
    case ast.Mult(): return left * right
    case ast.Div(): return left / right
    case ast.Pow(): return left**right
    case other:
      raise InterpreterError(
        "Operator not supported",
        ast.BinOp(ast.Name("_"), other, ast.Name("_")))

class InterpreterError(Exception):
  def __init__(self, msg: str, code: Optional[ast.AST] = None) -> None:
    super().__init__(msg, code)
    self._msg = msg
    self._code = code

  def __str__(self):
    if self._code:
      return f"{self._msg}: {ast.unparse(self._code)}"
    return self._msg

This can then be used to interpret commands, returning a dictionary with all the variables:
>>> interpret("z = x**2+y**2", {"x": 3, "y": 2})
{'x': 3, 'y': 2, 'z': 13}

While this allows you to interpret the Python code however you want (you control the semantics), you are still limited to Python's syntax. For example, you should use the ** operator for exponentiation, not Python's ^ xor-operator.
If you want your own syntax, then you'll probably have to write your own parser. There are a variety of parsing algorithms and parser generators, but I'm partial to hand-written “recursive descent”. This generally involves writing recursive functions of the form parse(Position) -> Optional[tuple[Position, Value]] that gradually consume the input. I have written an example parser and interpreter using that strategy, and have previously contrasted different parsing approaches in an answer about implementing query languages in a Python program.


------------------------------------------------------------

ID: datascience.stackexchange.com:62096
Title: How to access an embedding table that is too large to fully load into memory?
Body: I'm currently trying to find a way of loading/deserializing a .json file containing Flair word embeddings that is too large to fit in my RAM at once (>60GB .json with 32GB of RAM). My current code for loading the embedding is below.

def get_embedding_table(config):
    words_id2vec = json.load(open(config.words_id2vector_filename, 'r'))
    words_vectors = [0] * len(words_id2vec)
    for id, vec in words_id2vec.items():
        words_vectors[int(id)] = vec

    words_vectors.append(list(np.random.uniform(0, 1, config.embedding_dim)))
    words_embedding_table = tf.Variable(name='words_emb_table', initial_value=words_vectors, dtype=tf.float32)


The rest of the code that I am trying to reproduce with a different word embedding can be found here.

I wonder if it is somehow possible to access the embedding table without deserialization of the entire .json file, for example: by sequentially reading it, somehow splitting it, or reading it directly from my disk. I would greatly appreciate your input!


[Answer]: There are a couple of options:


Incrementally parse JSON with something like ijson. Munge and append each part to tf.Variable.
Reduce the precision of the numbers dtype=tf.float32 to 
dtype=tf.bfloat16



------------------------------------------------------------

ID: softwareengineering.stackexchange.com:361714
Title: Webpack and Lazy Load for large-scale Web Application
Body: Background
I am trying to develop with Webpack and JavaScript. Webpack would bundle all source code into one single file. When application becomes large, the file would be very large and cause negative impact to the performance.
Webpack
Webpack provided feature named code splitting which would split the bundle output into different according the predefined entry point. It would help increase the performance if the application become too large. However, It required to restructure the application and deeply understand the application, then the application spitted into different modules which would be the entry points for build. Also, it would requires code change when another required to call the function by Lazy Loading.
SystemJs
SystemJs is a dynamic Es Module Loader which would lazy load the required module in Runtime.
Question
Should I use webpack for large-sale application?
Possible Solution
JavaScript Source Code Bundling is necessary for large application but the application may be spitted into sub-modules and may be lazy load into application if the sub module is loaded.
Update
stage 3 proposal specific the dynamic import syntax which would used to solve my problem (lazy load for specific part).
import('./test.js').then(function(Test){
  var test = new Test.default();
  test.doSomething()
})

Which is similar to
import Test from './test';
var test = new Test();
test.doSomething();

Webpack support
It required to config the public path, chunkFilename and add syntax-dynamic-import for enable new syntax (import()).
const path = require('path');

module.exports = {
    entry: {
        app:    './src/app.js'
    },
    output: {
        publicPath: 'bin/',
        path: path.resolve(__dirname, 'bin'),
        filename: '[name].bundle.js',
        chunkFilename: '[name].bundle.js'
    },
    devtool: 'source-map',
    module: {
        loaders: [
            {
                test: /\.js$/,
                loaders: [
                    {
                        loader: 'babel-loader',
                        options: {
                            presets: ['es2015'],
                            plugins: ["syntax-dynamic-import"]
                        }
                    }
                ]               
            }
        ]
    }
}; 



[Answer]: All mature JavaScript loading and bundling tools, from Webpack to RequireJS to SystemJS, provide techniques for doing this.

In Webpack it's enabled by a feature known as code splitting.

In SystemJS it's enabled by features such as bundle arithmetic, dep caching, and bundle injection and is provided by JSPM or direct usage of the SystemJS  Builder.

I forget the terminology for RequireJS but the functionality exists there as well and is provided by r.js.

So just research.

But, to answer your direct question, all of these tools, including Webpack, are suitable for developing large applications. In fact they excel at it. My personal preference is not for Webpack but rather for JSPM.

As Ben Cheng notes, Webpack's code splitting approach requires you to structure your application in terms of different entry points, which you can morally think of as sub packages or sub apps. That said, this structuring technique plays a role in splitting up bundles in all of these tools.

Breaking down your application into these more independent units can be a difficult task.

Disclaimer: I am a member of the JSPM team


------------------------------------------------------------

ID: softwareengineering.stackexchange.com:206781
Title: JSON object and storage of nosql
Body: I have read Would a NoSQL DB be more efficient than a relational DB for storing JSON objects? and am building a small test project in Asp.Net. I have a webapi up in Azure. It returns a List and Company is my object which has several properties and child list and a lat/long value.
//id, name etc.
public List Certifications { get; set; }
public float Latitude { get; set; }
public float Longitude { get; set; }
public GeoCoordinate Cordinate // etc. GeoCoordinate is from System.Device reference

I return this List of companies and use the JSON output.
Now internally, loading this list I load the complete list of companies out of a json file. and if there is no file, a file will be created. This is all good. But the Latitude and Longtitude is empty on the initial basis. So I fill it using googles reverse geocode. That works, but has a request limit. So I'd like to load the list and if lat/long is empty, retrieve the values from google's service and store it. But I am looking for a solution not to store the complete json list to a file again. And I am not looking for a relational database solution, because that is something that I have done enough. Now I have read about mongoDB. But it is a bit hard to set up on Azure. I have had Redis on Azure. What easy and fast solution do you recommend for me to store my list of objects? Do you even recommend it to store it as JSON? or something else? like XML? and use xpath to update values?
So I am looking for an architecture/design to update all lat/longs untill google gives the quota limit error and give it a go next try I access the list of companies.
ps. I do not want to store a list of Certification's. I am curious if I can keep it as property of company and store the complete company project.


[Answer]: I'm gonna show how I would solve this using the Starcounter database to store the data and using it's internal web server to fetch the data as JSON models. The application model is automatically the database if [Database] attribute is set.

The database

[Database]
public class Company{
   public String Name; 
   public String RegistrationNumber;
   ... and so on
   public IEnumerable Certificates{
         get{
            return Db.SQL("SELECT c FROM Certificate where c.Position=?",this);
         }
   }

   private Coordinate Coordinate;
   public double Latitude{
        get{
            if(Coordinate == null){
                 AssureCoordinates();
            }
            return Coordinate.Latitude;
         }
   }
   public double Longitute
        get{
            if(Coordinate == null){
                 AssureCoordinates();
            }
            return Coordinate.Longitute;
         }
   }

   private void AssureCoordinates(){
          ... fetch from whatever source and set to Coordinate
   }


And then in the integrated web server I would define a JSON model for the Company:

{
     Name:"ACME Ltd",
     RegistrationNumber:"555-5555",
     Longitude:"123.4",
     Latitude:"123.4",
     Certificates:[
        {
            ... certifcates properties
        }],
     $:{DataType:"Company"},
     $Certificates:{DataType:"Certificate"
 }


This JSON model will be automatically bound to the persistent data in the database upon request.

And lastly register the REST verb+URI to respond to:

Handle.GET("/company/?", (String registrationNumber) =>
{
    CompanyModel cModel = new CompanyModel();
    Company comp = ... find company in DB using SQL
    cModel.Data = comp;
return cModel;
});


Now a JSON model of the requested company would be returned and all data is filled automatically from the database. The Coordinate will be fetched if not set upon first call to Long/Lat. You could of course also return a list of these Companies as well.

This setup will make your regular database act like a JSON source, and you can skip the regular web server as well.

Hope this helped you on how to solve this problem using a NoSQL database!


------------------------------------------------------------

ID: cs.stackexchange.com:75341
Title: Proving $\#CYCLE \in \#P$
Body: I'm trying to find a way to count distinct simple cycles in a graph in order to prove that $\#CYCLE \in \#P$, if I could represent a distinct cycle, then I'll have a witness.

I saw this question: 
Number of cycles in a graph? they mention the reduction from HAM #CYCLE in Counting Complexity by Arora and Barak, but I don't understand how does that help.

You have some directed graph, no one said it has a hamilton cycle, and the gadget doesn't return how many cycles there in the graph or a way to distinguish between them, it just makes all the cycles longer.

So instead, is it possible to use a topological sort to count cycles? since it can detect every cycle, we can use it to distinguish between them.


[Answer]: The class #P consists of those functions $f$ such that there is a polytime relation $R$ and a polynomial bound $p$ satisfying
$$
 f(x) = |\{ |y| 


------------------------------------------------------------

ID: cs.stackexchange.com:18342
Title: single algorithm to work on both directed and undirected graph to detect cycles?
Body: I have been trying to implement an algorithm to detect cycles (probably how many of them) in a directed and undirected graph. That is the code should apply for both directed and undirected graphs.

Using DFS or topological sort is mostly recommended in various posts. But largely, everything is addressed for undirected graph. 

This link describes one approach for cycle detection. To my understanding this works for directed graphs.

This link has the code for cycle detection in undirected graphs. but I fail to understand how it ignores the back edge. That is it must ignore any cycles with two nodes, say D to C and C to D.
which means it must remember it parent as the DFS recurses. But the code does not seem take care of that.

Any suggestions welcome..




[Answer]: Take a look at the answers for finding all cycles in graph.

You can transform undirected graph into a directed graph quite easily: for every edge (u,v) in your original graph G, put two edges (u --> v) and (v --> u) into your new directed graph T. Apply any of the algorithms that find cycles in a directed graph on T. However, notice that any undirected edge transforms into a directed cycle (u - v - u), so if your original graph was initially undirected, you can ignore cycles of length 2 at the end.


[Answer]: For a Directed Graph - we keep track of the recursion stack. For an edge (u,v), if we currently are processing u, and we see that v is in the recursion stack, then we have a Cycle.
For Undirected Graph - we look construct a parent array while we are traversing with DFS. Similar situation, for an edge (u,v) while processing u, if we see that v is Visited && not the parent of u, then we have a cycle.
I hope this helps!


------------------------------------------------------------

ID: cs.stackexchange.com:115845
Title: How to deal with parallel edges between two vertices in cycle detection using BFS in an undirected graph?
Body: I am new to Programming and learning Algorithms and was studying BFS when I read that BFS could be used for cycle detection. I tried to implement the same on an undirected graph G with Adjacency List Representation.
What I did is as follows:

• Do a simple BFS Traversal using a Queue while maintaining the parent node of nodes enqueued in the queue.
• If I come across a node u that has a neighbor v such that v is already visited but v is not the parent of u then that means there is cycle in the graph.

Pseudocode:
#adjList is the adjacency list given as a dictionary
#myQueue is a double-sided queue containing node and its parent node ([Node, parNode])
#visited is a set containing visited nodes

while(myQueue):
    currNode, parNode = myQueue.pop() #dequeue operation
    visited.add(currNode) #Marking currNode as visited
    for childNode in adjList[currNode]: #Traversing through all children of currNode
        if currNode not in visited:
            myQueue.appendleft([childNode, currNode]) #Enqueue operation
        else:
            if childNode!=parNode: #Main logic for cycle detection
                print('CYCLE DETECTED')
                break

The above approach is working except in cases when I have more than 1 edge between 2 vertices for e.g. in following case we have 2 edges between vertices 0 and 1:

Adjacency list of above graph is: adjList = {0:[1, 1, 2], 1:[0, 0], 2:[0]}. Here we can clearly see that the graph contains a cycle (In the adjacency list, it is represented by the fact that 1 appears twice in the adjacency list of 0 and 0 appears twice in the adjacency list of 1) but above algorithm is not able to detect the same because when BFS will reach vertex 1, vertex 0 is already visited but vertex 0 is also the parent of vertex 1 so this cycle will go undetected.
My question is how I can modify above algorithm to detect such cases?
Edit: I tried the same logic on directed graphs also, and I am facing similar problem i.e. case when I have a directed edge from vertex 0 to vertex 1 and another directed edge from vertex 1 to vertex 0


[Answer]: If the case arrives where you see the case of a node being already visited but it is the parent of your current node, you just have to check whether there is a double edge between them. How to do that depends on your data structure, but if your adjacency list is sorted it would just amount to searching for the edge and checking how often it is in there.

Another problem I see is that your adjacency list doesn't actually contain any information that the edge is doubled.

For directed graphs, you can just completely get rid of the "parent check", as the only time that case arrives is when you have two edges going from $u$ to $v$ and vice versa.

Additionally, be careful if the graph is not connected, your BFS will not cover all of it, so you would need to start another BFS from an unvisited vertex again. This is especially important for directed graphs, as even if the graph might be connected, your BFS will probably not cover all of it.


------------------------------------------------------------

